{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM6cFhjHqlM5G/TY8ZLgFw1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CamiloVga/Curso-IA-Para-Ciencia-de-Datos/blob/main/Script_Sesi%C3%B3n_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# IA para la Ciencia de Datos\n",
        "## Universidad de los Andes\n",
        "\n",
        "**Profesor:** Camilo Vega - AI/ML Engineer  \n",
        "**LinkedIn:** https://www.linkedin.com/in/camilo-vega-169084b1/\n",
        "\n",
        "---\n",
        "\n",
        "## Gu√≠a: OCR y Extracci√≥n de Datos - An√°lisis Automatizado de Documentos\n",
        "\n",
        "Este notebook presenta **2 implementaciones pr√°cticas de OCR**:\n",
        "\n",
        "1. **Tesseract + RAG Database** - An√°lisis de facturas/recibos con extracci√≥n estructurada\n",
        "2. **Mistral OCR + RAG Documents** - Procesamiento de peri√≥dicos con an√°lisis multimodal\n",
        "\n",
        "### Requisitos\n",
        "- **APIs:** Groq API token, HuggingFace access\n",
        "- **GPU:** Opcional para modelos locales\n",
        "- **Datasets:** Se descargan autom√°ticamente desde HuggingFace\n",
        "\n",
        "## Configuraci√≥n APIs\n",
        "- **Groq API:**\n",
        "  1. [Crear token](https://console.groq.com/keys)\n",
        "  2. En Colab: üîë Secrets ‚Üí Agregar `GROQ_KEY` ‚Üí Pegar tu token\n",
        "\n"
      ],
      "metadata": {
        "id": "RxoZ4Vc9n2v3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OCR Treseract +LLM"
      ],
      "metadata": {
        "id": "wSmtIvXGLit6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nNlkBjQ9krrC"
      },
      "outputs": [],
      "source": [
        "# OCR con Tesseract + Visi√≥n Groq - Extracci√≥n de Datos\n",
        "\n",
        "\"\"\"\n",
        "ARQUITECTURA DEL SISTEMA OCR + VISION LLM\n",
        "\n",
        "1. Carga de datos: Ingesta flexible desde HuggingFace datasets o carpetas locales\n",
        "2. An√°lisis de esquema: Primer documento define estructura JSON autom√°ticamente\n",
        "3. Extracci√≥n h√≠brida: OCR (Tesseract) + Vision LLM (Groq) en paralelo\n",
        "4. Mapeo inteligente: LLM estructura datos seg√∫n esquema aprendido\n",
        "5. Procesamiento batch: Aplica pipeline a todos los documentos\n",
        "6. Salida JSON: Resultados estructurados con metadatos y validaci√≥n\n",
        "\"\"\"\n",
        "\n",
        "# Instalaciones necesarias\n",
        "!pip install tesseract pytesseract Pillow datasets groq -q\n",
        "!apt update && apt install tesseract-ocr -y -q\n",
        "\n",
        "import json\n",
        "import pytesseract\n",
        "import base64\n",
        "import io\n",
        "import os\n",
        "import glob\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "from datasets import load_dataset\n",
        "from groq import Groq\n",
        "from google.colab import userdata\n",
        "\n",
        "# HIPERPAR√ÅMETROS CONFIGURABLES\n",
        "USE_LOCAL_FOLDER = False  # True: usar carpeta local, False: usar dataset\n",
        "CARPETA_FACTURAS = \"/content/facturas\"  # Carpeta con facturas propias\n",
        "DATASET_NAME = \"v2run/invoices-donut-data-v1\"  # Dataset de facturas HF\n",
        "NUM_DOCUMENTOS = 20  # N√∫mero de documentos a procesar en fase 2\n",
        "FORMATOS_IMAGEN = [\"*.png\", \"*.jpg\", \"*.jpeg\", \"*.tiff\", \"*.bmp\"]\n",
        "\n",
        "# Configuraci√≥n\n",
        "client = Groq(api_key=userdata.get('GROQ_KEY'))\n",
        "\n",
        "def get_image_files(carpeta, num_max):\n",
        "    \"\"\"Obtiene lista de archivos de imagen de la carpeta\"\"\"\n",
        "    archivos = []\n",
        "    for formato in FORMATOS_IMAGEN:\n",
        "        archivos.extend(glob.glob(os.path.join(carpeta, formato)))\n",
        "        archivos.extend(glob.glob(os.path.join(carpeta, formato.upper())))\n",
        "    return sorted(archivos)[:num_max + 1]  # +1 para el ejemplo\n",
        "\n",
        "def load_local_images(carpeta, num_docs):\n",
        "    \"\"\"Carga im√°genes desde carpeta local\"\"\"\n",
        "    archivos = get_image_files(carpeta, num_docs)\n",
        "\n",
        "    if not archivos:\n",
        "        raise Exception(f\"No se encontraron im√°genes en: {carpeta}\")\n",
        "\n",
        "    images_data = []\n",
        "    for archivo in archivos:\n",
        "        img = Image.open(archivo)\n",
        "        images_data.append({\n",
        "            'image': img,\n",
        "            'filename': Path(archivo).name\n",
        "        })\n",
        "\n",
        "    return images_data\n",
        "\n",
        "def image_to_base64(image):\n",
        "    \"\"\"Convierte imagen PIL a base64\"\"\"\n",
        "    buffer = io.BytesIO()\n",
        "    image.save(buffer, format='PNG')\n",
        "    img_str = base64.b64encode(buffer.getvalue()).decode()\n",
        "    return f\"data:image/png;base64,{img_str}\"\n",
        "\n",
        "def analyze_example_structure(ejemplo_imagen):\n",
        "    \"\"\"Analiza la imagen de ejemplo para definir la estructura JSON\"\"\"\n",
        "    try:\n",
        "        image_base64 = image_to_base64(ejemplo_imagen)\n",
        "\n",
        "        print(\"Analizando imagen de ejemplo para definir estructura...\")\n",
        "\n",
        "        completion = client.chat.completions.create(\n",
        "            model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
        "            messages=[\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": [\n",
        "                        {\n",
        "                            \"type\": \"text\",\n",
        "                            \"text\": \"\"\"Analiza esta imagen de factura/documento y define una estructura JSON que capture TODOS los campos visibles.\n",
        "\n",
        "Crea un formato JSON completo que incluya:\n",
        "- N√∫meros de identificaci√≥n\n",
        "- Fechas\n",
        "- Informaci√≥n de cliente/proveedor\n",
        "- Productos/servicios con precios\n",
        "- Totales y subtotales\n",
        "\n",
        "Responde SOLO con la estructura JSON:\"\"\"\n",
        "                        },\n",
        "                        {\n",
        "                            \"type\": \"image_url\",\n",
        "                            \"image_url\": {\"url\": image_base64}\n",
        "                        }\n",
        "                    ]\n",
        "                }\n",
        "            ],\n",
        "            temperature=0.1,\n",
        "            max_tokens=600\n",
        "        )\n",
        "\n",
        "        result = completion.choices[0].message.content.strip()\n",
        "        result = result.replace('```json', '').replace('```', '').strip()\n",
        "\n",
        "        estructura = json.loads(result)\n",
        "        print(\"Estructura definida:\")\n",
        "        print(json.dumps(estructura, indent=2, ensure_ascii=False))\n",
        "\n",
        "        return estructura\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error analizando ejemplo: {e}\")\n",
        "        return {\n",
        "            \"invoice_number\": \"\",\n",
        "            \"date\": \"\",\n",
        "            \"customer_name\": \"\",\n",
        "            \"total_amount\": 0\n",
        "        }\n",
        "\n",
        "def extract_text_tesseract(image):\n",
        "    \"\"\"Extrae texto con Tesseract\"\"\"\n",
        "    config = '--oem 3 --psm 6'\n",
        "    return pytesseract.image_to_string(image, config=config).strip()\n",
        "\n",
        "def analyze_with_vision(image, estructura_aprendida):\n",
        "    \"\"\"Analiza documentos con el modelo de visi√≥n correcto\"\"\"\n",
        "    try:\n",
        "        image_base64 = image_to_base64(image)\n",
        "        campos_estructura = json.dumps(estructura_aprendida, indent=2, ensure_ascii=False)\n",
        "\n",
        "        completion = client.chat.completions.create(\n",
        "            model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
        "            messages=[\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": [\n",
        "                        {\n",
        "                            \"type\": \"text\",\n",
        "                            \"text\": f\"\"\"Extrae informaci√≥n de esta imagen de factura usando EXACTAMENTE esta estructura JSON:\n",
        "\n",
        "{campos_estructura}\n",
        "\n",
        "REGLAS ESTRICTAS:\n",
        "- Usa los mismos nombres de campos exactamente\n",
        "- Si no encuentras un campo, usa null (sin comillas)\n",
        "- TODOS los strings deben estar entre comillas dobles\n",
        "- NO uses comillas simples\n",
        "- CIERRA todos los corchetes y llaves\n",
        "- NO incluyas texto adicional fuera del JSON\n",
        "- VERIFICA que el JSON est√© completo y v√°lido\n",
        "\n",
        "Responde √öNICAMENTE con JSON v√°lido, nada m√°s:\"\"\"\n",
        "                        },\n",
        "                        {\n",
        "                            \"type\": \"image_url\",\n",
        "                            \"image_url\": {\"url\": image_base64}\n",
        "                        }\n",
        "                    ]\n",
        "                }\n",
        "            ],\n",
        "            temperature=0.05,\n",
        "            max_tokens=800\n",
        "        )\n",
        "\n",
        "        result = completion.choices[0].message.content.strip()\n",
        "\n",
        "        # Limpieza m√°s agresiva\n",
        "        result = result.replace('```json', '').replace('```', '').strip()\n",
        "\n",
        "        # Remover texto antes y despu√©s del JSON si existe\n",
        "        start_idx = result.find('{')\n",
        "        end_idx = result.rfind('}')\n",
        "\n",
        "        if start_idx != -1 and end_idx != -1:\n",
        "            result = result[start_idx:end_idx + 1]\n",
        "\n",
        "        # Intentar parsear\n",
        "        parsed_result = json.loads(result)\n",
        "        return parsed_result\n",
        "\n",
        "    except json.JSONDecodeError as e:\n",
        "        return {\"error\": f\"JSON malformado: {str(e)}\", \"raw_response\": result[:200]}\n",
        "    except Exception as e:\n",
        "        return {\"error\": f\"Error: {str(e)}\"}\n",
        "\n",
        "# CARGA DE DATOS\n",
        "print(\"=\"*50)\n",
        "print(\"CARGANDO DATOS\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "if USE_LOCAL_FOLDER:\n",
        "    print(f\"Usando carpeta local: {CARPETA_FACTURAS}\")\n",
        "    print(f\"Formatos soportados: {', '.join(FORMATOS_IMAGEN)}\")\n",
        "\n",
        "    try:\n",
        "        documents_data = load_local_images(CARPETA_FACTURAS, NUM_DOCUMENTOS)\n",
        "        print(f\"Encontradas {len(documents_data)} im√°genes\")\n",
        "\n",
        "        # Separar ejemplo y documentos a procesar\n",
        "        ejemplo_doc = documents_data[0]\n",
        "        documentos_procesar = documents_data[1:NUM_DOCUMENTOS + 1]\n",
        "\n",
        "        print(f\"Usando {ejemplo_doc['filename']} como ejemplo\")\n",
        "        print(f\"Procesando {len(documentos_procesar)} documentos\")\n",
        "\n",
        "        source_info = f\"Carpeta: {CARPETA_FACTURAS}\"\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error cargando carpeta local: {e}\")\n",
        "        print(\"Fallback a dataset...\")\n",
        "        USE_LOCAL_FOLDER = False\n",
        "\n",
        "if not USE_LOCAL_FOLDER:\n",
        "    print(f\"Usando dataset: {DATASET_NAME}\")\n",
        "    print(f\"Documentos a procesar: {NUM_DOCUMENTOS}\")\n",
        "\n",
        "    try:\n",
        "        dataset = load_dataset(DATASET_NAME, split=f\"train[:{NUM_DOCUMENTOS + 1}]\")\n",
        "        documents = list(dataset)\n",
        "        print(f\"Dataset cargado: {len(documents)} documentos\")\n",
        "\n",
        "        # Separar ejemplo y documentos a procesar\n",
        "        ejemplo_doc = {'image': documents[0]['image'], 'filename': 'dataset_ejemplo'}\n",
        "        documentos_procesar = [{'image': doc['image'], 'filename': f'dataset_doc_{i+2}'}\n",
        "                             for i, doc in enumerate(documents[1:NUM_DOCUMENTOS + 1])]\n",
        "\n",
        "        print(f\"Usando documento 1 como ejemplo\")\n",
        "        print(f\"Procesando documentos 2-{len(documentos_procesar) + 1}\")\n",
        "\n",
        "        source_info = f\"Dataset: {DATASET_NAME}\"\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error cargando dataset: {e}\")\n",
        "        exit()\n",
        "\n",
        "# EJECUCI√ìN PRINCIPAL\n",
        "print(f\"\\n{'='*50}\")\n",
        "print(\"FASE 1: AN√ÅLISIS DE EJEMPLO\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "estructura_json = analyze_example_structure(ejemplo_doc['image'])\n",
        "\n",
        "print(f\"\\n{'='*50}\")\n",
        "print(\"FASE 2: PROCESAMIENTO DOCUMENTOS\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "try:\n",
        "    print(f\"Procesando {len(documentos_procesar)} documentos\")\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for i, doc in enumerate(documentos_procesar):\n",
        "        print(f\"\\nDocumento {i+1} ({doc['filename']}):\")\n",
        "\n",
        "        try:\n",
        "            image = doc['image']\n",
        "\n",
        "            # OCR\n",
        "            ocr_text = extract_text_tesseract(image)\n",
        "            print(f\"  OCR: {len(ocr_text)} caracteres\")\n",
        "\n",
        "            # Visi√≥n\n",
        "            print(f\"  Analizando con visi√≥n...\")\n",
        "            vision_data = analyze_with_vision(image, estructura_json)\n",
        "\n",
        "            result = {\n",
        "                \"id\": i+1,\n",
        "                \"filename\": doc['filename'],\n",
        "                \"ocr_length\": len(ocr_text),\n",
        "                \"ocr_sample\": ocr_text[:200] + \"...\" if len(ocr_text) > 200 else ocr_text,\n",
        "                \"vision_analysis\": vision_data,\n",
        "                \"status\": \"processed\" if not vision_data.get(\"error\") else \"error\"\n",
        "            }\n",
        "\n",
        "            results.append(result)\n",
        "            print(f\"  Estado: {result['status']}\")\n",
        "\n",
        "            if result['status'] == 'error':\n",
        "                print(f\"  Error: {vision_data.get('error', 'Desconocido')}\")\n",
        "            else:\n",
        "                campos = len([v for v in vision_data.values() if v and str(v).lower() != \"null\"])\n",
        "                print(f\"  Campos detectados: {campos}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  Error general: {e}\")\n",
        "            results.append({\"id\": i+1, \"filename\": doc['filename'], \"status\": \"failed\", \"error\": str(e)})\n",
        "\n",
        "    # Resultados finales\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(\"RESULTADOS\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    successful = [r for r in results if r.get('status') == 'processed']\n",
        "    print(f\"Exitosos: {len(successful)}/{len(results)}\")\n",
        "\n",
        "    if successful:\n",
        "        print(\"\\nMejor ejemplo procesado:\")\n",
        "        best = max(successful,\n",
        "                  key=lambda x: len(str(x.get('vision_analysis', {}))))\n",
        "        print(f\"Archivo: {best['filename']}\")\n",
        "        print(json.dumps(best['vision_analysis'], indent=2, ensure_ascii=False))\n",
        "\n",
        "        print(f\"\\nTexto OCR (muestra):\")\n",
        "        print(f\"{'-'*30}\")\n",
        "        print(best.get('ocr_sample', 'Sin texto'))\n",
        "\n",
        "    # Guardar\n",
        "    with open('resultados_ocr.json', 'w', encoding='utf-8') as f:\n",
        "        json.dump({\n",
        "            \"configuracion\": {\n",
        "                \"fuente\": source_info,\n",
        "                \"usar_carpeta_local\": USE_LOCAL_FOLDER,\n",
        "                \"num_documentos\": NUM_DOCUMENTOS,\n",
        "                \"ejemplo_usado\": ejemplo_doc['filename']\n",
        "            },\n",
        "            \"estructura_aprendida\": estructura_json,\n",
        "            \"documentos_procesados\": results\n",
        "        }, f, indent=2, ensure_ascii=False, default=str)\n",
        "    print(\"\\nGuardado en: resultados_ocr.json\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#OCR Mistral"
      ],
      "metadata": {
        "id": "eyBWMSUxLn_s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# OCR Mistral - Procesamiento de PDFs locales\n",
        "\n",
        "\"\"\"\n",
        "ARQUITECTURA DEL SISTEMA OCR MISTRAL - PROCESAMIENTO DE PDFs\n",
        "\n",
        "1. Configuraci√≥n de entorno: Inicializaci√≥n de cliente Mistral y carpetas de trabajo\n",
        "2. Ingesta de documentos: Descarga autom√°tica + subida manual de archivos PDF\n",
        "3. OCR avanzado: Extracci√≥n de texto, markdown e im√°genes con Mistral OCR API\n",
        "4. An√°lisis inteligente: LLM procesa contenido y genera metadatos estructurados\n",
        "5. Extracci√≥n multimedia: Im√°genes embebidas guardadas como archivos independientes\n",
        "6. Persistencia m√∫ltiple: Salida en formatos TXT, Markdown y JSON con resumen\n",
        "\"\"\"\n",
        "\n",
        "!pip install mistralai -q\n",
        "\n",
        "import json\n",
        "import base64\n",
        "import os\n",
        "import requests\n",
        "from pathlib import Path\n",
        "from mistralai import Mistral, DocumentURLChunk, ImageURLChunk, TextChunk\n",
        "from google.colab import userdata, files\n",
        "\n",
        "# Configuraci√≥n\n",
        "OCR_FOLDER = '/content/archivos_ocr'\n",
        "RESULTS_FOLDER = '/content/resultados_ocr'\n",
        "PAPER_URL = \"https://github.com/CamiloVga/Curso-IA-Para-Ciencia-de-Datos/raw/main/Paper%20Attention%20Is%20All%20You%20Need.pdf\"\n",
        "PAPER_FILENAME = \"Paper_Attention_Is_All_You_Need.pdf\"\n",
        "\n",
        "# Setup cliente Mistral\n",
        "client = Mistral(api_key=userdata.get('MISTRAL'))\n",
        "\n",
        "def create_folders():\n",
        "    \"\"\"Crea carpetas necesarias\"\"\"\n",
        "    os.makedirs(OCR_FOLDER, exist_ok=True)\n",
        "    os.makedirs(RESULTS_FOLDER, exist_ok=True)\n",
        "\n",
        "def download_paper():\n",
        "    \"\"\"Descarga paper desde GitHub\"\"\"\n",
        "    try:\n",
        "        create_folders()\n",
        "        response = requests.get(PAPER_URL, timeout=30)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        paper_path = os.path.join(OCR_FOLDER, PAPER_FILENAME)\n",
        "        with open(paper_path, 'wb') as f:\n",
        "            f.write(response.content)\n",
        "\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"Error descargando: {str(e)}\")\n",
        "        return False\n",
        "\n",
        "def upload_files():\n",
        "    \"\"\"Permite subir archivos adicionales\"\"\"\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    for filename, content in uploaded.items():\n",
        "        dest_path = os.path.join(OCR_FOLDER, filename)\n",
        "        with open(dest_path, 'wb') as f:\n",
        "            f.write(content)\n",
        "\n",
        "    return list(uploaded.keys())\n",
        "\n",
        "def list_pdf_files():\n",
        "    \"\"\"Lista archivos PDF en carpeta\"\"\"\n",
        "    if not os.path.exists(OCR_FOLDER):\n",
        "        return []\n",
        "\n",
        "    pdf_files = [f for f in os.listdir(OCR_FOLDER) if f.lower().endswith('.pdf')]\n",
        "    return sorted(pdf_files)\n",
        "\n",
        "def process_pdf_with_mistral(pdf_filename):\n",
        "    \"\"\"Procesa PDF usando Mistral OCR\"\"\"\n",
        "    try:\n",
        "        pdf_path = Path(os.path.join(OCR_FOLDER, pdf_filename))\n",
        "        if not pdf_path.is_file():\n",
        "            raise FileNotFoundError(f\"Archivo {pdf_filename} no encontrado\")\n",
        "\n",
        "        # Subir PDF al servicio OCR\n",
        "        uploaded_file = client.files.upload(\n",
        "            file={\n",
        "                \"file_name\": pdf_path.stem,\n",
        "                \"content\": pdf_path.read_bytes(),\n",
        "            },\n",
        "            purpose=\"ocr\",\n",
        "        )\n",
        "\n",
        "        # Obtener URL firmada\n",
        "        signed_url = client.files.get_signed_url(file_id=uploaded_file.id, expiry=1)\n",
        "\n",
        "        # Procesar con OCR\n",
        "        pdf_response = client.ocr.process(\n",
        "            document=DocumentURLChunk(document_url=signed_url.url),\n",
        "            model=\"mistral-ocr-latest\",\n",
        "            include_image_base64=True\n",
        "        )\n",
        "\n",
        "        return pdf_response\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error procesando {pdf_filename}: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "def extract_text_and_markdown(ocr_response):\n",
        "    \"\"\"Extrae texto y markdown del OCR\"\"\"\n",
        "    combined_text = \"\"\n",
        "    combined_markdown = \"\"\n",
        "\n",
        "    if ocr_response and hasattr(ocr_response, 'pages'):\n",
        "        for page in ocr_response.pages:\n",
        "            if hasattr(page, 'text') and page.text:\n",
        "                combined_text += page.text + \"\\n\\n\"\n",
        "            if hasattr(page, 'markdown') and page.markdown:\n",
        "                combined_markdown += page.markdown + \"\\n\\n\"\n",
        "\n",
        "    return combined_text.strip(), combined_markdown.strip()\n",
        "\n",
        "def analyze_with_chat_model(text, filename):\n",
        "    \"\"\"Analiza texto con modelo de chat\"\"\"\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"pixtral-12b-latest\",\n",
        "            messages=[{\n",
        "                \"role\": \"user\",\n",
        "                \"content\": f\"\"\"Analiza este documento y extrae informaci√≥n estructurada en JSON:\n",
        "\n",
        "Archivo: {filename}\n",
        "Texto: {text[:3000]}\n",
        "\n",
        "Extrae y devuelve JSON con:\n",
        "- title: t√≠tulo del documento\n",
        "- document_type: tipo de documento\n",
        "- main_topics: lista de temas principales\n",
        "- key_points: puntos clave\n",
        "- language: idioma detectado\n",
        "- summary: resumen breve\n",
        "\n",
        "Responde solo con JSON v√°lido:\"\"\"\n",
        "            }],\n",
        "            temperature=0.1,\n",
        "            max_tokens=800\n",
        "        )\n",
        "\n",
        "        result_text = response.choices[0].message.content.strip()\n",
        "        result_text = result_text.replace('json', '').replace('', '').strip()\n",
        "\n",
        "        # Extraer JSON\n",
        "        start = result_text.find('{')\n",
        "        end = result_text.rfind('}')\n",
        "        if start != -1 and end != -1:\n",
        "            result_text = result_text[start:end + 1]\n",
        "\n",
        "        return json.loads(result_text)\n",
        "\n",
        "    except Exception as e:\n",
        "        return {\n",
        "            \"title\": filename,\n",
        "            \"document_type\": \"documento\",\n",
        "            \"main_topics\": [\"contenido general\"],\n",
        "            \"key_points\": [\"informaci√≥n extra√≠da por OCR\"],\n",
        "            \"language\": \"unknown\",\n",
        "            \"summary\": text[:200] + \"...\" if len(text) > 200 else text\n",
        "        }\n",
        "\n",
        "def extract_images_from_ocr(ocr_response, base_filename):\n",
        "    \"\"\"Extrae im√°genes del OCR\"\"\"\n",
        "    extracted_images = []\n",
        "\n",
        "    if not ocr_response or not hasattr(ocr_response, 'pages'):\n",
        "        return extracted_images\n",
        "\n",
        "    for page_idx, page in enumerate(ocr_response.pages):\n",
        "        if hasattr(page, 'images'):\n",
        "            for img_idx, img in enumerate(page.images):\n",
        "                if hasattr(img, 'image_base64') and img.image_base64:\n",
        "                    try:\n",
        "                        # Procesar datos base64\n",
        "                        base64_data = img.image_base64\n",
        "                        if \",\" in base64_data:\n",
        "                            base64_data = base64_data.split(\",\", 1)[1]\n",
        "\n",
        "                        # Guardar imagen\n",
        "                        img_filename = f\"{base_filename}_page{page_idx+1}_img{img_idx+1}.jpg\"\n",
        "                        img_path = os.path.join(RESULTS_FOLDER, img_filename)\n",
        "\n",
        "                        with open(img_path, 'wb') as f:\n",
        "                            f.write(base64.b64decode(base64_data))\n",
        "\n",
        "                        extracted_images.append(img_filename)\n",
        "\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error guardando imagen: {e}\")\n",
        "\n",
        "    return extracted_images\n",
        "\n",
        "def save_results(filename, text, markdown, structured_data, extracted_images):\n",
        "    \"\"\"Guarda resultados en diferentes formatos\"\"\"\n",
        "    base_name = os.path.splitext(filename)[0]\n",
        "\n",
        "    # Texto plano\n",
        "    text_file = os.path.join(RESULTS_FOLDER, f\"{base_name}.txt\")\n",
        "    with open(text_file, 'w', encoding='utf-8') as f:\n",
        "        f.write(text)\n",
        "\n",
        "    # Markdown estructurado\n",
        "    title = structured_data.get('title', 'Documento sin t√≠tulo')\n",
        "    markdown_content = f\"\"\"# {title}\n",
        "\n",
        "Archivo: {filename}\n",
        "Tipo: {structured_data.get('document_type', 'Documento')}\n",
        "Idioma: {structured_data.get('language', 'No detectado')}\n",
        "\n",
        "## Resumen\n",
        "{structured_data.get('summary', 'No disponible')}\n",
        "\n",
        "## Temas Principales\n",
        "{chr(10).join(f\"- {topic}\" for topic in structured_data.get('main_topics', []))}\n",
        "\n",
        "## Puntos Clave\n",
        "{chr(10).join(f\"- {point}\" for point in structured_data.get('key_points', []))}\n",
        "\n",
        "## Im√°genes Extra√≠das\n",
        "{chr(10).join(f\"- {img}\" for img in extracted_images) if extracted_images else \"No se extrajeron im√°genes\"}\n",
        "\n",
        "## Contenido Completo\n",
        "{text}\n",
        "\n",
        "## Markdown Original\n",
        "{markdown}\n",
        "\"\"\"\n",
        "\n",
        "    markdown_file = os.path.join(RESULTS_FOLDER, f\"{base_name}.md\")\n",
        "    with open(markdown_file, 'w', encoding='utf-8') as f:\n",
        "        f.write(markdown_content)\n",
        "\n",
        "    # JSON estructurado\n",
        "    json_data = {\n",
        "        \"filename\": filename,\n",
        "        \"extracted_images\": extracted_images,\n",
        "        \"text_length\": len(text),\n",
        "        \"structured_analysis\": structured_data,\n",
        "        \"full_text\": text,\n",
        "        \"markdown\": markdown\n",
        "    }\n",
        "\n",
        "    json_file = os.path.join(RESULTS_FOLDER, f\"{base_name}.json\")\n",
        "    with open(json_file, 'w', encoding='utf-8') as f:\n",
        "        json.dump(json_data, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    return {\n",
        "        \"text_file\": text_file,\n",
        "        \"markdown_file\": markdown_file,\n",
        "        \"json_file\": json_file,\n",
        "        \"images\": extracted_images\n",
        "    }\n",
        "\n",
        "def process_all_pdfs():\n",
        "    \"\"\"Procesa todos los PDFs en carpeta\"\"\"\n",
        "    pdf_files = list_pdf_files()\n",
        "\n",
        "    if not pdf_files:\n",
        "        print(\"No se encontraron archivos PDF\")\n",
        "        return []\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for i, pdf_file in enumerate(pdf_files):\n",
        "        print(f\"Procesando {i+1}/{len(pdf_files)}: {pdf_file}\")\n",
        "\n",
        "        try:\n",
        "            # Procesar con OCR\n",
        "            ocr_response = process_pdf_with_mistral(pdf_file)\n",
        "            if not ocr_response:\n",
        "                results.append({\"filename\": pdf_file, \"status\": \"error\", \"error\": \"No OCR response\"})\n",
        "                continue\n",
        "\n",
        "            # Extraer contenido\n",
        "            text, markdown = extract_text_and_markdown(ocr_response)\n",
        "            structured_data = analyze_with_chat_model(text, pdf_file)\n",
        "\n",
        "            # Extraer im√°genes\n",
        "            base_name = os.path.splitext(pdf_file)[0]\n",
        "            extracted_images = extract_images_from_ocr(ocr_response, base_name)\n",
        "\n",
        "            # Guardar resultados\n",
        "            saved_files = save_results(pdf_file, text, markdown, structured_data, extracted_images)\n",
        "\n",
        "            results.append({\n",
        "                \"filename\": pdf_file,\n",
        "                \"status\": \"success\",\n",
        "                \"text_length\": len(text),\n",
        "                \"images_extracted\": len(extracted_images),\n",
        "                \"structured_data\": structured_data,\n",
        "                \"saved_files\": saved_files\n",
        "            })\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error procesando {pdf_file}: {str(e)}\")\n",
        "            results.append({\"filename\": pdf_file, \"status\": \"error\", \"error\": str(e)})\n",
        "\n",
        "    return results\n",
        "\n",
        "def main():\n",
        "    \"\"\"Funci√≥n principal\"\"\"\n",
        "    print(\"OCR MISTRAL - PROCESADOR DE PDFs\")\n",
        "\n",
        "    # Crear carpetas\n",
        "    create_folders()\n",
        "\n",
        "    # Descargar paper\n",
        "    download_success = download_paper()\n",
        "    if not download_success:\n",
        "        print(\"Descarga autom√°tica fall√≥\")\n",
        "\n",
        "    # Verificar archivos\n",
        "    existing_files = list_pdf_files()\n",
        "    if not existing_files:\n",
        "        print(\"No hay archivos PDF. Ejecuta upload_files() para subir archivos.\")\n",
        "        return []\n",
        "\n",
        "    # Procesar PDFs\n",
        "    results = process_all_pdfs()\n",
        "\n",
        "    # Resumen\n",
        "    successful = [r for r in results if r['status'] == 'success']\n",
        "    errors = [r for r in results if r['status'] == 'error']\n",
        "\n",
        "    print(f\"Procesados: {len(successful)}/{len(results)}\")\n",
        "    print(f\"Errores: {len(errors)}\")\n",
        "\n",
        "    if successful:\n",
        "        total_images = sum(r.get('images_extracted', 0) for r in successful)\n",
        "        print(f\"Im√°genes extra√≠das: {total_images}\")\n",
        "\n",
        "    # Guardar resumen\n",
        "    summary_file = os.path.join(RESULTS_FOLDER, \"resumen_procesamiento.json\")\n",
        "    with open(summary_file, 'w', encoding='utf-8') as f:\n",
        "        json.dump({\n",
        "            \"total_processed\": len(results),\n",
        "            \"successful\": len(successful),\n",
        "            \"errors\": len(errors),\n",
        "            \"results\": results\n",
        "        }, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    return results\n",
        "\n",
        "# Funciones auxiliares\n",
        "def download_paper_only():\n",
        "    return download_paper()\n",
        "\n",
        "def process_paper_only():\n",
        "    paper_path = os.path.join(OCR_FOLDER, PAPER_FILENAME)\n",
        "    if not os.path.exists(paper_path):\n",
        "        print(\"Paper no encontrado. Ejecuta download_paper_only() primero.\")\n",
        "        return None\n",
        "\n",
        "    return process_pdf_with_mistral(PAPER_FILENAME)\n",
        "\n",
        "# Ejecutar\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "OHEWR0VWLqI2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}