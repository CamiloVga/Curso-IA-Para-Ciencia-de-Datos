{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPf1kCrmbEOQcFLDF5StRFU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CamiloVga/Curso-IA-Para-Ciencia-de-Datos/blob/main/Script_Sesi%C3%B3n_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# IA para la Ciencia de Datos\n",
        "## Universidad de los Andes\n",
        "\n",
        "**Profesor:** Camilo Vega - AI/ML Engineer  \n",
        "**LinkedIn:** https://www.linkedin.com/in/camilo-vega-169084b1/\n",
        "\n",
        "---\n",
        "\n",
        "## Gu√≠a: OCR y Extracci√≥n de Datos - An√°lisis Automatizado de Documentos\n",
        "\n",
        "Este notebook presenta **2 implementaciones pr√°cticas de OCR**:\n",
        "\n",
        "1. **Tesseract + RAG Database** - An√°lisis de facturas/recibos con extracci√≥n estructurada\n",
        "2. **Mistral OCR + RAG Documents** - Procesamiento de peri√≥dicos con an√°lisis multimodal\n",
        "\n",
        "### Requisitos\n",
        "- **APIs:** Groq API token, HuggingFace access\n",
        "- **GPU:** Opcional para modelos locales\n",
        "- **Datasets:** Se descargan autom√°ticamente desde HuggingFace\n",
        "\n",
        "## Configuraci√≥n APIs\n",
        "- **Groq API:**\n",
        "  1. [Crear token](https://console.groq.com/keys)\n",
        "  2. En Colab: üîë Secrets ‚Üí Agregar `GROQ_KEY` ‚Üí Pegar tu token\n",
        "\n"
      ],
      "metadata": {
        "id": "RxoZ4Vc9n2v3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OCR Treseract +LLM"
      ],
      "metadata": {
        "id": "wSmtIvXGLit6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nNlkBjQ9krrC"
      },
      "outputs": [],
      "source": [
        "# OCR con Tesseract + Visi√≥n Groq - Extracci√≥n de Datos (con soporte PDF)\n",
        "\n",
        "\"\"\"\n",
        "ARQUITECTURA DEL SISTEMA OCR + VISION LLM\n",
        "\n",
        "1. Carga de datos: Ingesta flexible desde HuggingFace datasets o carpetas locales (im√°genes + PDF)\n",
        "2. An√°lisis de esquema: Primer documento define estructura JSON autom√°ticamente\n",
        "3. Extracci√≥n h√≠brida: OCR (Tesseract) + Vision LLM (Groq) en paralelo\n",
        "4. Mapeo inteligente: LLM estructura datos seg√∫n esquema aprendido\n",
        "5. Procesamiento batch: Aplica pipeline a todos los documentos\n",
        "6. Salida JSON: Resultados estructurados con metadatos y validaci√≥n\n",
        "\"\"\"\n",
        "\n",
        "# Instalaciones necesarias\n",
        "!pip install tesseract pytesseract Pillow datasets groq pdf2image -q\n",
        "!apt update && apt install tesseract-ocr poppler-utils -y -q\n",
        "\n",
        "import json\n",
        "import pytesseract\n",
        "import base64\n",
        "import io\n",
        "import os\n",
        "import glob\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "from datasets import load_dataset\n",
        "from groq import Groq\n",
        "from google.colab import userdata\n",
        "from pdf2image import convert_from_path\n",
        "import tempfile\n",
        "\n",
        "# HIPERPAR√ÅMETROS CONFIGURABLES\n",
        "USE_LOCAL_FOLDER = False  # True: usar carpeta local, False: usar dataset\n",
        "CARPETA_FACTURAS = \"/content/facturas\"  # Carpeta con facturas propias\n",
        "DATASET_NAME = \"v2run/invoices-donut-data-v1\"  # Dataset de facturas HF\n",
        "NUM_DOCUMENTOS = 10  # N√∫mero de documentos a procesar en fase 2\n",
        "FORMATOS_IMAGEN = [\"*.png\", \"*.jpg\", \"*.jpeg\", \"*.tiff\", \"*.bmp\"]\n",
        "FORMATOS_PDF = [\"*.pdf\"]\n",
        "PDF_DPI = 200  # Resoluci√≥n para conversi√≥n PDF a imagen\n",
        "\n",
        "# Configuraci√≥n\n",
        "client = Groq(api_key=userdata.get('GROQ_KEY'))\n",
        "\n",
        "def get_document_files(carpeta, num_max):\n",
        "    \"\"\"Obtiene lista de archivos de imagen y PDF de la carpeta\"\"\"\n",
        "    archivos = []\n",
        "    # Im√°genes\n",
        "    for formato in FORMATOS_IMAGEN:\n",
        "        archivos.extend(glob.glob(os.path.join(carpeta, formato)))\n",
        "        archivos.extend(glob.glob(os.path.join(carpeta, formato.upper())))\n",
        "    # PDFs\n",
        "    for formato in FORMATOS_PDF:\n",
        "        archivos.extend(glob.glob(os.path.join(carpeta, formato)))\n",
        "        archivos.extend(glob.glob(os.path.join(carpeta, formato.upper())))\n",
        "    return sorted(archivos)[:num_max + 1]  # +1 para el ejemplo\n",
        "\n",
        "def pdf_to_image(pdf_path, page_num=0):\n",
        "    \"\"\"Convierte p√°gina espec√≠fica de PDF a imagen PIL\"\"\"\n",
        "    try:\n",
        "        # Convertir PDF a im√°genes\n",
        "        images = convert_from_path(pdf_path, dpi=PDF_DPI, first_page=page_num+1, last_page=page_num+1)\n",
        "        if images:\n",
        "            return images[0]\n",
        "        else:\n",
        "            raise Exception(\"No se pudo convertir PDF a imagen\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error convirtiendo PDF {pdf_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "def load_local_documents(carpeta, num_docs):\n",
        "    \"\"\"Carga documentos (im√°genes y PDFs) desde carpeta local\"\"\"\n",
        "    archivos = get_document_files(carpeta, num_docs)\n",
        "\n",
        "    if not archivos:\n",
        "        raise Exception(f\"No se encontraron documentos en: {carpeta}\")\n",
        "\n",
        "    documents_data = []\n",
        "    for archivo in archivos:\n",
        "        file_path = Path(archivo)\n",
        "        file_ext = file_path.suffix.lower()\n",
        "\n",
        "        try:\n",
        "            if file_ext == '.pdf':\n",
        "                # Convertir primera p√°gina del PDF a imagen\n",
        "                img = pdf_to_image(archivo, page_num=0)\n",
        "                if img is None:\n",
        "                    print(f\"Saltando PDF problem√°tico: {file_path.name}\")\n",
        "                    continue\n",
        "                document_type = \"pdf\"\n",
        "            else:\n",
        "                # Cargar imagen directamente\n",
        "                img = Image.open(archivo)\n",
        "                document_type = \"image\"\n",
        "\n",
        "            documents_data.append({\n",
        "                'image': img,\n",
        "                'filename': file_path.name,\n",
        "                'type': document_type,\n",
        "                'original_path': archivo\n",
        "            })\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error procesando {file_path.name}: {e}\")\n",
        "            continue\n",
        "\n",
        "    return documents_data\n",
        "\n",
        "def image_to_base64(image):\n",
        "    \"\"\"Convierte imagen PIL a base64\"\"\"\n",
        "    buffer = io.BytesIO()\n",
        "    image.save(buffer, format='PNG')\n",
        "    img_str = base64.b64encode(buffer.getvalue()).decode()\n",
        "    return f\"data:image/png;base64,{img_str}\"\n",
        "\n",
        "def analyze_example_structure(ejemplo_imagen):\n",
        "    \"\"\"Analiza la imagen de ejemplo para definir la estructura JSON\"\"\"\n",
        "    try:\n",
        "        image_base64 = image_to_base64(ejemplo_imagen)\n",
        "\n",
        "        print(\"Analizando documento de ejemplo para definir estructura...\")\n",
        "\n",
        "        completion = client.chat.completions.create(\n",
        "            model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
        "            messages=[\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": [\n",
        "                        {\n",
        "                            \"type\": \"text\",\n",
        "                            \"text\": \"\"\"Analiza esta imagen de factura/documento y define una estructura JSON que capture TODOS los campos visibles.\n",
        "\n",
        "Crea un formato JSON completo que incluya:\n",
        "- N√∫meros de identificaci√≥n\n",
        "- Fechas\n",
        "- Informaci√≥n de cliente/proveedor\n",
        "- Productos/servicios con precios\n",
        "- Totales y subtotales\n",
        "\n",
        "Responde SOLO con la estructura JSON:\"\"\"\n",
        "                        },\n",
        "                        {\n",
        "                            \"type\": \"image_url\",\n",
        "                            \"image_url\": {\"url\": image_base64}\n",
        "                        }\n",
        "                    ]\n",
        "                }\n",
        "            ],\n",
        "            temperature=0.1,\n",
        "            max_tokens=600\n",
        "        )\n",
        "\n",
        "        result = completion.choices[0].message.content.strip()\n",
        "        result = result.replace('```json', '').replace('```', '').strip()\n",
        "\n",
        "        estructura = json.loads(result)\n",
        "        print(\"Estructura definida:\")\n",
        "        print(json.dumps(estructura, indent=2, ensure_ascii=False))\n",
        "\n",
        "        return estructura\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error analizando ejemplo: {e}\")\n",
        "        return {\n",
        "            \"invoice_number\": \"\",\n",
        "            \"date\": \"\",\n",
        "            \"customer_name\": \"\",\n",
        "            \"total_amount\": 0\n",
        "        }\n",
        "\n",
        "def extract_text_tesseract(image):\n",
        "    \"\"\"Extrae texto con Tesseract\"\"\"\n",
        "    config = '--oem 3 --psm 6'\n",
        "    return pytesseract.image_to_string(image, config=config).strip()\n",
        "\n",
        "def analyze_with_vision(image, estructura_aprendida):\n",
        "    \"\"\"Analiza documentos con el modelo de visi√≥n correcto\"\"\"\n",
        "    try:\n",
        "        image_base64 = image_to_base64(image)\n",
        "        campos_estructura = json.dumps(estructura_aprendida, indent=2, ensure_ascii=False)\n",
        "\n",
        "        completion = client.chat.completions.create(\n",
        "            model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
        "            messages=[\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": [\n",
        "                        {\n",
        "                            \"type\": \"text\",\n",
        "                            \"text\": f\"\"\"Extrae informaci√≥n de esta imagen de factura usando EXACTAMENTE esta estructura JSON:\n",
        "\n",
        "{campos_estructura}\n",
        "\n",
        "REGLAS ESTRICTAS:\n",
        "- Usa los mismos nombres de campos exactamente\n",
        "- Si no encuentras un campo, usa null (sin comillas)\n",
        "- TODOS los strings deben estar entre comillas dobles\n",
        "- NO uses comillas simples\n",
        "- CIERRA todos los corchetes y llaves\n",
        "- NO incluyas texto adicional fuera del JSON\n",
        "- VERIFICA que el JSON est√© completo y v√°lido\n",
        "\n",
        "Responde √öNICAMENTE con JSON v√°lido, nada m√°s:\"\"\"\n",
        "                        },\n",
        "                        {\n",
        "                            \"type\": \"image_url\",\n",
        "                            \"image_url\": {\"url\": image_base64}\n",
        "                        }\n",
        "                    ]\n",
        "                }\n",
        "            ],\n",
        "            temperature=0.05,\n",
        "            max_tokens=800\n",
        "        )\n",
        "\n",
        "        result = completion.choices[0].message.content.strip()\n",
        "\n",
        "        # Limpieza m√°s agresiva\n",
        "        result = result.replace('```json', '').replace('```', '').strip()\n",
        "\n",
        "        # Remover texto antes y despu√©s del JSON si existe\n",
        "        start_idx = result.find('{')\n",
        "        end_idx = result.rfind('}')\n",
        "\n",
        "        if start_idx != -1 and end_idx != -1:\n",
        "            result = result[start_idx:end_idx + 1]\n",
        "\n",
        "        # Intentar parsear\n",
        "        parsed_result = json.loads(result)\n",
        "        return parsed_result\n",
        "\n",
        "    except json.JSONDecodeError as e:\n",
        "        return {\"error\": f\"JSON malformado: {str(e)}\", \"raw_response\": result[:200]}\n",
        "    except Exception as e:\n",
        "        return {\"error\": f\"Error: {str(e)}\"}\n",
        "\n",
        "# CARGA DE DATOS\n",
        "print(\"=\"*50)\n",
        "print(\"CARGANDO DATOS\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "if USE_LOCAL_FOLDER:\n",
        "    print(f\"Usando carpeta local: {CARPETA_FACTURAS}\")\n",
        "    print(f\"Formatos soportados: {', '.join(FORMATOS_IMAGEN + FORMATOS_PDF)}\")\n",
        "    print(f\"DPI para conversi√≥n PDF: {PDF_DPI}\")\n",
        "\n",
        "    try:\n",
        "        documents_data = load_local_documents(CARPETA_FACTURAS, NUM_DOCUMENTOS)\n",
        "        print(f\"Encontrados {len(documents_data)} documentos\")\n",
        "\n",
        "        # Mostrar tipos de documentos encontrados\n",
        "        tipos = {}\n",
        "        for doc in documents_data:\n",
        "            tipos[doc['type']] = tipos.get(doc['type'], 0) + 1\n",
        "        print(f\"Tipos: {tipos}\")\n",
        "\n",
        "        # Separar ejemplo y documentos a procesar\n",
        "        ejemplo_doc = documents_data[0]\n",
        "        documentos_procesar = documents_data[1:NUM_DOCUMENTOS + 1]\n",
        "\n",
        "        print(f\"Usando {ejemplo_doc['filename']} ({ejemplo_doc['type']}) como ejemplo\")\n",
        "        print(f\"Procesando {len(documentos_procesar)} documentos\")\n",
        "\n",
        "        source_info = f\"Carpeta: {CARPETA_FACTURAS}\"\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error cargando carpeta local: {e}\")\n",
        "        print(\"Fallback a dataset...\")\n",
        "        USE_LOCAL_FOLDER = False\n",
        "\n",
        "if not USE_LOCAL_FOLDER:\n",
        "    print(f\"Usando dataset: {DATASET_NAME}\")\n",
        "    print(f\"Documentos a procesar: {NUM_DOCUMENTOS}\")\n",
        "\n",
        "    try:\n",
        "        dataset = load_dataset(DATASET_NAME, split=f\"train[:{NUM_DOCUMENTOS + 1}]\")\n",
        "        documents = list(dataset)\n",
        "        print(f\"Dataset cargado: {len(documents)} documentos\")\n",
        "\n",
        "        # Separar ejemplo y documentos a procesar\n",
        "        ejemplo_doc = {'image': documents[0]['image'], 'filename': 'dataset_ejemplo', 'type': 'image'}\n",
        "        documentos_procesar = [{'image': doc['image'], 'filename': f'dataset_doc_{i+2}', 'type': 'image'}\n",
        "                             for i, doc in enumerate(documents[1:NUM_DOCUMENTOS + 1])]\n",
        "\n",
        "        print(f\"Usando documento 1 como ejemplo\")\n",
        "        print(f\"Procesando documentos 2-{len(documentos_procesar) + 1}\")\n",
        "\n",
        "        source_info = f\"Dataset: {DATASET_NAME}\"\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error cargando dataset: {e}\")\n",
        "        exit()\n",
        "\n",
        "# EJECUCI√ìN PRINCIPAL\n",
        "print(f\"\\n{'='*50}\")\n",
        "print(\"FASE 1: AN√ÅLISIS DE EJEMPLO\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "estructura_json = analyze_example_structure(ejemplo_doc['image'])\n",
        "\n",
        "print(f\"\\n{'='*50}\")\n",
        "print(\"FASE 2: PROCESAMIENTO DOCUMENTOS\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "try:\n",
        "    print(f\"Procesando {len(documentos_procesar)} documentos\")\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for i, doc in enumerate(documentos_procesar):\n",
        "        print(f\"\\nDocumento {i+1} ({doc['filename']}) - {doc['type'].upper()}:\")\n",
        "\n",
        "        try:\n",
        "            image = doc['image']\n",
        "\n",
        "            # OCR\n",
        "            ocr_text = extract_text_tesseract(image)\n",
        "            print(f\"  OCR: {len(ocr_text)} caracteres\")\n",
        "\n",
        "            # Visi√≥n\n",
        "            print(f\"  Analizando con visi√≥n...\")\n",
        "            vision_data = analyze_with_vision(image, estructura_json)\n",
        "\n",
        "            result = {\n",
        "                \"id\": i+1,\n",
        "                \"filename\": doc['filename'],\n",
        "                \"document_type\": doc['type'],\n",
        "                \"ocr_length\": len(ocr_text),\n",
        "                \"ocr_sample\": ocr_text[:200] + \"...\" if len(ocr_text) > 200 else ocr_text,\n",
        "                \"vision_analysis\": vision_data,\n",
        "                \"status\": \"processed\" if not vision_data.get(\"error\") else \"error\"\n",
        "            }\n",
        "\n",
        "            results.append(result)\n",
        "            print(f\"  Estado: {result['status']}\")\n",
        "\n",
        "            if result['status'] == 'error':\n",
        "                print(f\"  Error: {vision_data.get('error', 'Desconocido')}\")\n",
        "            else:\n",
        "                campos = len([v for v in vision_data.values() if v and str(v).lower() != \"null\"])\n",
        "                print(f\"  Campos detectados: {campos}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  Error general: {e}\")\n",
        "            results.append({\n",
        "                \"id\": i+1,\n",
        "                \"filename\": doc['filename'],\n",
        "                \"document_type\": doc.get('type', 'unknown'),\n",
        "                \"status\": \"failed\",\n",
        "                \"error\": str(e)\n",
        "            })\n",
        "\n",
        "    # Resultados finales\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(\"RESULTADOS\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    successful = [r for r in results if r.get('status') == 'processed']\n",
        "    print(f\"Exitosos: {len(successful)}/{len(results)}\")\n",
        "\n",
        "    # Estad√≠sticas por tipo\n",
        "    tipos_procesados = {}\n",
        "    for r in results:\n",
        "        doc_type = r.get('document_type', 'unknown')\n",
        "        if doc_type not in tipos_procesados:\n",
        "            tipos_procesados[doc_type] = {'total': 0, 'exitosos': 0}\n",
        "        tipos_procesados[doc_type]['total'] += 1\n",
        "        if r.get('status') == 'processed':\n",
        "            tipos_procesados[doc_type]['exitosos'] += 1\n",
        "\n",
        "    print(\"\\nEstad√≠sticas por tipo:\")\n",
        "    for tipo, stats in tipos_procesados.items():\n",
        "        print(f\"  {tipo.upper()}: {stats['exitosos']}/{stats['total']}\")\n",
        "\n",
        "    if successful:\n",
        "        print(\"\\nMejor ejemplo procesado:\")\n",
        "        best = max(successful,\n",
        "                  key=lambda x: len(str(x.get('vision_analysis', {}))))\n",
        "        print(f\"Archivo: {best['filename']} ({best['document_type']})\")\n",
        "        print(json.dumps(best['vision_analysis'], indent=2, ensure_ascii=False))\n",
        "\n",
        "        print(f\"\\nTexto OCR (muestra):\")\n",
        "        print(f\"{'-'*30}\")\n",
        "        print(best.get('ocr_sample', 'Sin texto'))\n",
        "\n",
        "    # Guardar\n",
        "    with open('resultados_ocr.json', 'w', encoding='utf-8') as f:\n",
        "        json.dump({\n",
        "            \"configuracion\": {\n",
        "                \"fuente\": source_info,\n",
        "                \"usar_carpeta_local\": USE_LOCAL_FOLDER,\n",
        "                \"num_documentos\": NUM_DOCUMENTOS,\n",
        "                \"ejemplo_usado\": ejemplo_doc['filename'],\n",
        "                \"ejemplo_tipo\": ejemplo_doc['type'],\n",
        "                \"pdf_dpi\": PDF_DPI,\n",
        "                \"formatos_soportados\": FORMATOS_IMAGEN + FORMATOS_PDF\n",
        "            },\n",
        "            \"estructura_aprendida\": estructura_json,\n",
        "            \"documentos_procesados\": results,\n",
        "            \"estadisticas_tipos\": tipos_procesados\n",
        "        }, f, indent=2, ensure_ascii=False, default=str)\n",
        "    print(\"\\nGuardado en: resultados_ocr.json\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# EJEMPLO DE EXTRACCI√ìN CON DOCUMENTOS ESCANEADOS\n",
        "\n",
        "import requests\n",
        "from IPython.display import display\n",
        "\n",
        "print(\"\\n--- PROCESAMIENTO EJEMPLO MANUAL ---\")\n",
        "\n",
        "# Descargar factura desde GitHub\n",
        "FACTURA_URL = \"https://github.com/CamiloVga/Curso-IA-Para-Ciencia-de-Datos/raw/main/Ejemplo%20Factura%20Escaneada.jpg\"\n",
        "\n",
        "response = requests.get(FACTURA_URL, timeout=30)\n",
        "with open(\"/content/Ejemplo Factura Escaneada.jpg\", 'wb') as f:\n",
        "    f.write(response.content)\n",
        "\n",
        "EJEMPLO_DOCUMENTO = \"/content/Ejemplo Factura Escaneada.jpg\"\n",
        "\n",
        "if os.path.exists(EJEMPLO_DOCUMENTO):\n",
        "    # Cargar documento (PDF o imagen)\n",
        "    file_ext = Path(EJEMPLO_DOCUMENTO).suffix.lower()\n",
        "    if file_ext == '.pdf':\n",
        "        imagen = pdf_to_image(EJEMPLO_DOCUMENTO, page_num=0)\n",
        "    else:\n",
        "        imagen = Image.open(EJEMPLO_DOCUMENTO)\n",
        "\n",
        "    # Mostrar imagen\n",
        "    display(imagen.resize((imagen.width // 2, imagen.height // 2)))\n",
        "\n",
        "    # Procesar\n",
        "    ocr_text = extract_text_tesseract(imagen)\n",
        "    estructura = analyze_example_structure(imagen)\n",
        "    resultado = analyze_with_vision(imagen, estructura)\n",
        "\n",
        "    # Mostrar\n",
        "    print(f\"Archivo: {Path(EJEMPLO_DOCUMENTO).name}\")\n",
        "\n",
        "else:\n",
        "    print(f\"Archivo no encontrado: {EJEMPLO_DOCUMENTO}\")"
      ],
      "metadata": {
        "id": "b66s3_fDc_Wj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#OCR Mistral"
      ],
      "metadata": {
        "id": "eyBWMSUxLn_s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# OCR Mistral - Procesamiento de PDFs locales\n",
        "\n",
        "\"\"\"\n",
        "ARQUITECTURA DEL SISTEMA OCR MISTRAL - PROCESAMIENTO DE PDFs\n",
        "\n",
        "1. Configuraci√≥n de entorno: Inicializaci√≥n de cliente Mistral y carpetas de trabajo\n",
        "2. Ingesta de documentos: Descarga autom√°tica + subida manual de archivos PDF\n",
        "3. OCR avanzado: Extracci√≥n de texto, markdown e im√°genes con Mistral OCR API\n",
        "4. An√°lisis inteligente: LLM procesa contenido y genera metadatos estructurados\n",
        "5. Extracci√≥n multimedia: Im√°genes embebidas guardadas como archivos independientes\n",
        "6. Persistencia m√∫ltiple: Salida en formatos Markdown y JSON con resumen\n",
        "7. Visualizaci√≥n: Display de im√°genes y fragmentos importantes\n",
        "8. Documentaci√≥n Mistral OCR:https://mistral.ai/news/mistral-ocr\n",
        "\"\"\"\n",
        "\n",
        "!pip install mistralai pillow -q\n",
        "\n",
        "import json\n",
        "import base64\n",
        "import os\n",
        "import requests\n",
        "from pathlib import Path\n",
        "from mistralai import Mistral, DocumentURLChunk, ImageURLChunk, TextChunk\n",
        "from google.colab import userdata, files\n",
        "from PIL import Image\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "# Configuraci√≥n\n",
        "OCR_FOLDER = '/content/archivos_ocr'\n",
        "RESULTS_FOLDER = '/content/resultados_ocr'\n",
        "PAPER_URL = \"https://github.com/CamiloVga/Curso-IA-Para-Ciencia-de-Datos/raw/main/Paper%20Attention%20Is%20All%20You%20Need.pdf\"\n",
        "PAPER_FILENAME = \"Paper_Attention_Is_All_You_Need.pdf\"\n",
        "\n",
        "# Setup cliente Mistral\n",
        "client = Mistral(api_key=userdata.get('MISTRAL'))\n",
        "\n",
        "def create_folders():\n",
        "    \"\"\"Crea carpetas necesarias\"\"\"\n",
        "    os.makedirs(OCR_FOLDER, exist_ok=True)\n",
        "    os.makedirs(RESULTS_FOLDER, exist_ok=True)\n",
        "\n",
        "def download_paper():\n",
        "    \"\"\"Descarga paper desde GitHub\"\"\"\n",
        "    try:\n",
        "        create_folders()\n",
        "        response = requests.get(PAPER_URL, timeout=30)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        paper_path = os.path.join(OCR_FOLDER, PAPER_FILENAME)\n",
        "        with open(paper_path, 'wb') as f:\n",
        "            f.write(response.content)\n",
        "\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"Error descargando: {str(e)}\")\n",
        "        return False\n",
        "\n",
        "def upload_files():\n",
        "    \"\"\"Permite subir archivos adicionales\"\"\"\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    for filename, content in uploaded.items():\n",
        "        dest_path = os.path.join(OCR_FOLDER, filename)\n",
        "        with open(dest_path, 'wb') as f:\n",
        "            f.write(content)\n",
        "\n",
        "    return list(uploaded.keys())\n",
        "\n",
        "def list_pdf_files():\n",
        "    \"\"\"Lista archivos PDF en carpeta\"\"\"\n",
        "    if not os.path.exists(OCR_FOLDER):\n",
        "        return []\n",
        "\n",
        "    pdf_files = [f for f in os.listdir(OCR_FOLDER) if f.lower().endswith('.pdf')]\n",
        "    return sorted(pdf_files)\n",
        "\n",
        "def process_pdf_with_mistral(pdf_filename):\n",
        "    \"\"\"Procesa PDF usando Mistral OCR\"\"\"\n",
        "    try:\n",
        "        pdf_path = Path(os.path.join(OCR_FOLDER, pdf_filename))\n",
        "        if not pdf_path.is_file():\n",
        "            raise FileNotFoundError(f\"Archivo {pdf_filename} no encontrado\")\n",
        "\n",
        "        print(f\"Subiendo {pdf_filename} a Mistral OCR...\")\n",
        "\n",
        "        # Subir PDF al servicio OCR\n",
        "        uploaded_file = client.files.upload(\n",
        "            file={\n",
        "                \"file_name\": pdf_path.stem,\n",
        "                \"content\": pdf_path.read_bytes(),\n",
        "            },\n",
        "            purpose=\"ocr\",\n",
        "        )\n",
        "\n",
        "        # Obtener URL firmada\n",
        "        signed_url = client.files.get_signed_url(file_id=uploaded_file.id, expiry=1)\n",
        "\n",
        "        print(f\"Procesando con OCR...\")\n",
        "\n",
        "        # Procesar con OCR\n",
        "        pdf_response = client.ocr.process(\n",
        "            document=DocumentURLChunk(document_url=signed_url.url),\n",
        "            model=\"mistral-ocr-latest\",\n",
        "            include_image_base64=True\n",
        "        )\n",
        "\n",
        "        print(f\"OCR completado - {len(pdf_response.pages)} p√°ginas procesadas\")\n",
        "        return pdf_response\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error procesando {pdf_filename}: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "def extract_text_and_markdown(ocr_response):\n",
        "    \"\"\"Extrae texto y markdown del OCR\"\"\"\n",
        "    combined_text = \"\"\n",
        "    combined_markdown = \"\"\n",
        "\n",
        "    if ocr_response and hasattr(ocr_response, 'pages'):\n",
        "        for page in ocr_response.pages:\n",
        "            if hasattr(page, 'markdown') and page.markdown:\n",
        "                combined_text += page.markdown + \"\\n\\n\"\n",
        "                combined_markdown += page.markdown + \"\\n\\n\"\n",
        "\n",
        "    return combined_text.strip(), combined_markdown.strip()\n",
        "\n",
        "def analyze_with_chat_model(text, filename):\n",
        "    \"\"\"Analiza texto con modelo de chat\"\"\"\n",
        "    try:\n",
        "        response = client.chat.complete(\n",
        "            model=\"pixtral-12b-latest\",\n",
        "            messages=[{\n",
        "                \"role\": \"user\",\n",
        "                \"content\": f\"\"\"Analiza este documento y extrae informaci√≥n estructurada en JSON:\n",
        "\n",
        "Archivo: {filename}\n",
        "Texto: {text[:3000]}\n",
        "\n",
        "Extrae y devuelve JSON con:\n",
        "- title: t√≠tulo del documento\n",
        "- document_type: tipo de documento\n",
        "- main_topics: lista de temas principales\n",
        "- key_points: puntos clave\n",
        "- language: idioma detectado\n",
        "- summary: resumen breve\n",
        "\n",
        "Responde solo con JSON v√°lido:\"\"\"\n",
        "            }],\n",
        "            temperature=0.1,\n",
        "            max_tokens=800\n",
        "        )\n",
        "\n",
        "        result_text = response.choices[0].message.content.strip()\n",
        "        result_text = result_text.replace('```json', '').replace('```', '').strip()\n",
        "\n",
        "        # Extraer JSON\n",
        "        start = result_text.find('{')\n",
        "        end = result_text.rfind('}')\n",
        "        if start != -1 and end != -1:\n",
        "            result_text = result_text[start:end + 1]\n",
        "\n",
        "        return json.loads(result_text)\n",
        "\n",
        "    except Exception as e:\n",
        "        return {\n",
        "            \"title\": filename,\n",
        "            \"document_type\": \"documento\",\n",
        "            \"main_topics\": [\"contenido general\"],\n",
        "            \"key_points\": [\"informaci√≥n extra√≠da por OCR\"],\n",
        "            \"language\": \"unknown\",\n",
        "            \"summary\": text[:200] + \"...\" if len(text) > 200 else text\n",
        "        }\n",
        "\n",
        "def extract_images_from_ocr(ocr_response, base_filename):\n",
        "    \"\"\"Extrae im√°genes del OCR y las muestra\"\"\"\n",
        "    extracted_images = []\n",
        "\n",
        "    if not ocr_response or not hasattr(ocr_response, 'pages'):\n",
        "        return extracted_images\n",
        "\n",
        "    for page_idx, page in enumerate(ocr_response.pages):\n",
        "        if hasattr(page, 'images'):\n",
        "            for img_idx, img in enumerate(page.images):\n",
        "                if hasattr(img, 'image_base64') and img.image_base64:\n",
        "                    try:\n",
        "                        # Procesar datos base64\n",
        "                        base64_data = img.image_base64\n",
        "                        if \",\" in base64_data:\n",
        "                            base64_data = base64_data.split(\",\", 1)[1]\n",
        "\n",
        "                        # Guardar imagen\n",
        "                        img_filename = f\"{base_filename}_page{page_idx+1}_img{img_idx+1}.jpg\"\n",
        "                        img_path = os.path.join(RESULTS_FOLDER, img_filename)\n",
        "\n",
        "                        with open(img_path, 'wb') as f:\n",
        "                            f.write(base64.b64decode(base64_data))\n",
        "\n",
        "                        # Mostrar imagen extra√≠da\n",
        "                        print(f\"\\nImagen extra√≠da: P√°gina {page_idx+1}, Imagen {img_idx+1}\")\n",
        "                        extracted_img = Image.open(img_path)\n",
        "                        display(extracted_img.resize((min(400, extracted_img.width),\n",
        "                                                    min(300, extracted_img.height))))\n",
        "\n",
        "                        extracted_images.append(img_filename)\n",
        "\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error guardando imagen: {e}\")\n",
        "\n",
        "    return extracted_images\n",
        "\n",
        "def display_content_preview(text, structured_data):\n",
        "    \"\"\"Muestra preview del contenido extra√≠do\"\"\"\n",
        "    print(\"\\n--- PREVIEW DEL CONTENIDO ---\")\n",
        "\n",
        "    # Mostrar fragmento del texto\n",
        "    preview_text = text[:800] + \"...\" if len(text) > 800 else text\n",
        "    print(f\"Fragmento del texto extra√≠do ({len(text)} caracteres total):\")\n",
        "    print(\"-\" * 50)\n",
        "    print(preview_text)\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    # Mostrar an√°lisis estructurado\n",
        "    print(\"\\nAn√°lisis estructurado:\")\n",
        "    display(Markdown(f\"\"\"\n",
        "### {structured_data.get('title', 'Sin t√≠tulo')}\n",
        "**Tipo:** {structured_data.get('document_type', 'N/A')}\n",
        "**Idioma:** {structured_data.get('language', 'N/A')}\n",
        "\n",
        "**Resumen:**\n",
        "{structured_data.get('summary', 'No disponible')}\n",
        "\n",
        "**Temas principales:**\n",
        "{', '.join(structured_data.get('main_topics', []))}\n",
        "\"\"\"))\n",
        "\n",
        "def save_results(filename, text, markdown, structured_data, extracted_images):\n",
        "    \"\"\"Guarda resultados en Markdown y JSON (sin TXT)\"\"\"\n",
        "    base_name = os.path.splitext(filename)[0]\n",
        "\n",
        "    # Markdown estructurado\n",
        "    title = structured_data.get('title', 'Documento sin t√≠tulo')\n",
        "    markdown_content = f\"\"\"# {title}\n",
        "\n",
        "Archivo: {filename}\n",
        "Tipo: {structured_data.get('document_type', 'Documento')}\n",
        "Idioma: {structured_data.get('language', 'No detectado')}\n",
        "\n",
        "## Resumen\n",
        "{structured_data.get('summary', 'No disponible')}\n",
        "\n",
        "## Temas Principales\n",
        "{chr(10).join(f\"- {topic}\" for topic in structured_data.get('main_topics', []))}\n",
        "\n",
        "## Puntos Clave\n",
        "{chr(10).join(f\"- {point}\" for point in structured_data.get('key_points', []))}\n",
        "\n",
        "## Im√°genes Extra√≠das\n",
        "{chr(10).join(f\"- {img}\" for img in extracted_images) if extracted_images else \"No se extrajeron im√°genes\"}\n",
        "\n",
        "## Contenido Completo\n",
        "{text}\n",
        "\n",
        "## Markdown Original\n",
        "{markdown}\n",
        "\"\"\"\n",
        "\n",
        "    markdown_file = os.path.join(RESULTS_FOLDER, f\"{base_name}.md\")\n",
        "    with open(markdown_file, 'w', encoding='utf-8') as f:\n",
        "        f.write(markdown_content)\n",
        "\n",
        "    # JSON estructurado\n",
        "    json_data = {\n",
        "        \"filename\": filename,\n",
        "        \"extracted_images\": extracted_images,\n",
        "        \"text_length\": len(text),\n",
        "        \"structured_analysis\": structured_data,\n",
        "        \"full_text\": text,\n",
        "        \"markdown\": markdown\n",
        "    }\n",
        "\n",
        "    json_file = os.path.join(RESULTS_FOLDER, f\"{base_name}.json\")\n",
        "    with open(json_file, 'w', encoding='utf-8') as f:\n",
        "        json.dump(json_data, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    return {\n",
        "        \"markdown_file\": markdown_file,\n",
        "        \"json_file\": json_file,\n",
        "        \"images\": extracted_images\n",
        "    }\n",
        "\n",
        "def process_all_pdfs():\n",
        "    \"\"\"Procesa todos los PDFs en carpeta\"\"\"\n",
        "    pdf_files = list_pdf_files()\n",
        "\n",
        "    if not pdf_files:\n",
        "        print(\"No se encontraron archivos PDF\")\n",
        "        return []\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for i, pdf_file in enumerate(pdf_files):\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"Procesando {i+1}/{len(pdf_files)}: {pdf_file}\")\n",
        "        print(f\"{'='*60}\")\n",
        "\n",
        "        try:\n",
        "            # Procesar con OCR\n",
        "            ocr_response = process_pdf_with_mistral(pdf_file)\n",
        "            if not ocr_response:\n",
        "                results.append({\"filename\": pdf_file, \"status\": \"error\", \"error\": \"No OCR response\"})\n",
        "                continue\n",
        "\n",
        "            # Extraer contenido\n",
        "            text, markdown = extract_text_and_markdown(ocr_response)\n",
        "            structured_data = analyze_with_chat_model(text, pdf_file)\n",
        "\n",
        "            # Mostrar preview del contenido\n",
        "            display_content_preview(text, structured_data)\n",
        "\n",
        "            # Extraer im√°genes (con display autom√°tico)\n",
        "            base_name = os.path.splitext(pdf_file)[0]\n",
        "            extracted_images = extract_images_from_ocr(ocr_response, base_name)\n",
        "\n",
        "            # Guardar resultados\n",
        "            saved_files = save_results(pdf_file, text, markdown, structured_data, extracted_images)\n",
        "\n",
        "            results.append({\n",
        "                \"filename\": pdf_file,\n",
        "                \"status\": \"success\",\n",
        "                \"text_length\": len(text),\n",
        "                \"images_extracted\": len(extracted_images),\n",
        "                \"structured_data\": structured_data,\n",
        "                \"saved_files\": saved_files\n",
        "            })\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error procesando {pdf_file}: {str(e)}\")\n",
        "            results.append({\"filename\": pdf_file, \"status\": \"error\", \"error\": str(e)})\n",
        "\n",
        "    return results\n",
        "\n",
        "def main():\n",
        "    \"\"\"Funci√≥n principal\"\"\"\n",
        "    print(\"OCR MISTRAL - PROCESADOR DE PDFs CON VISUALIZACIONES\")\n",
        "\n",
        "    # Crear carpetas\n",
        "    create_folders()\n",
        "\n",
        "    # Descargar paper\n",
        "    download_success = download_paper()\n",
        "    if not download_success:\n",
        "        print(\"Descarga autom√°tica fall√≥\")\n",
        "\n",
        "    # Verificar archivos\n",
        "    existing_files = list_pdf_files()\n",
        "    if not existing_files:\n",
        "        print(\"No hay archivos PDF. Ejecuta upload_files() para subir archivos.\")\n",
        "        return []\n",
        "\n",
        "    # Procesar PDFs\n",
        "    results = process_all_pdfs()\n",
        "\n",
        "    # Resumen\n",
        "    successful = [r for r in results if r['status'] == 'success']\n",
        "    errors = [r for r in results if r['status'] == 'error']\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"RESUMEN FINAL\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"Procesados: {len(successful)}/{len(results)}\")\n",
        "    print(f\"Errores: {len(errors)}\")\n",
        "\n",
        "    if successful:\n",
        "        total_images = sum(r.get('images_extracted', 0) for r in successful)\n",
        "        print(f\"Im√°genes extra√≠das y mostradas: {total_images}\")\n",
        "\n",
        "    # Guardar resumen\n",
        "    summary_file = os.path.join(RESULTS_FOLDER, \"resumen_procesamiento.json\")\n",
        "    with open(summary_file, 'w', encoding='utf-8') as f:\n",
        "        json.dump({\n",
        "            \"total_processed\": len(results),\n",
        "            \"successful\": len(successful),\n",
        "            \"errors\": len(errors),\n",
        "            \"results\": results\n",
        "        }, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    return results\n",
        "\n",
        "# Funciones auxiliares\n",
        "def download_paper_only():\n",
        "    return download_paper()\n",
        "\n",
        "def process_paper_only():\n",
        "    paper_path = os.path.join(OCR_FOLDER, PAPER_FILENAME)\n",
        "    if not os.path.exists(paper_path):\n",
        "        print(\"Paper no encontrado. Ejecuta download_paper_only() primero.\")\n",
        "        return None\n",
        "\n",
        "    return process_pdf_with_mistral(PAPER_FILENAME)\n",
        "\n",
        "# Ejecutar\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "OHEWR0VWLqI2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# EJEMPLO DE EXTRACCI√ìN CON DOCUMENTOS ESCANEADOS\n",
        "\n",
        "import base64\n",
        "import requests\n",
        "from PIL import Image\n",
        "from IPython.display import display\n",
        "\n",
        "# Descargar y procesar imagen\n",
        "RECEIPT_URL = \"https://raw.githubusercontent.com/mistralai/cookbook/refs/heads/main/mistral/ocr/receipt.png\"\n",
        "receipt_path = os.path.join(OCR_FOLDER, \"receipt.png\")\n",
        "\n",
        "# Descarga\n",
        "response = requests.get(RECEIPT_URL, timeout=30)\n",
        "with open(receipt_path, 'wb') as f:\n",
        "    f.write(response.content)\n",
        "\n",
        "# Mostrar imagen\n",
        "img = Image.open(receipt_path)\n",
        "display(img.resize((img.width // 3, img.height // 3)))\n",
        "\n",
        "# OCR\n",
        "encoded = base64.b64encode(Path(receipt_path).read_bytes()).decode()\n",
        "base64_data_url = f\"data:image/jpeg;base64,{encoded}\"\n",
        "\n",
        "image_response = client.ocr.process(\n",
        "    document=ImageURLChunk(image_url=base64_data_url),\n",
        "    model=\"mistral-ocr-latest\"\n",
        ")\n",
        "\n",
        "# Mostrar resultado\n",
        "ocr_text = image_response.pages[0].markdown\n",
        "print(\"Contenido extra√≠do:\")\n",
        "print(ocr_text)\n",
        "\n",
        "# An√°lisis estructurado\n",
        "chat_response = client.chat.complete(\n",
        "    model=\"pixtral-12b-latest\",\n",
        "    messages=[{\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "            ImageURLChunk(image_url=base64_data_url),\n",
        "            TextChunk(text=f\"OCR: {ocr_text}\\nConvierte a JSON estructurado:\")\n",
        "        ]\n",
        "    }],\n",
        "    response_format={\"type\": \"json_object\"},\n",
        "    temperature=0\n",
        ")\n",
        "\n",
        "# Resultado final\n",
        "result = json.loads(chat_response.choices[0].message.content)\n",
        "print(\"\\nAn√°lisis estructurado:\")\n",
        "print(json.dumps(result, indent=2, ensure_ascii=False))\n",
        "\n",
        "# Guardar\n",
        "with open(os.path.join(RESULTS_FOLDER, \"receipt_analysis.json\"), 'w') as f:\n",
        "    json.dump({\"ocr\": ocr_text, \"analysis\": result}, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "print(\"Completado\")"
      ],
      "metadata": {
        "id": "sZrYTZncm9Tl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}