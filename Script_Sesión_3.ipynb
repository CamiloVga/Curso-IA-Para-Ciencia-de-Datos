{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMqkJCAX4CkpwVCazcZLjNA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CamiloVga/Curso-IA-Para-Ciencia-de-Datos/blob/main/Script_Sesi%C3%B3n_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# IA para la Ciencia de Datos\n",
        "## Universidad de los Andes\n",
        "\n",
        "**Profesor:** Camilo Vega - AI/ML Engineer  \n",
        "**LinkedIn:** https://www.linkedin.com/in/camilo-vega-169084b1/\n",
        "\n",
        "---\n",
        "\n",
        "## Gu√≠a: Fine-tuning y RAG (Retrieval-Augmented Generation)\n",
        "\n",
        "Este notebook presenta **3 implementaciones pr√°cticas**:\n",
        "\n",
        "1. **Fine-tuning B√°sico** - An√°lisis de sentimientos con tweets\n",
        "2. **RAG Simple** - B√∫squeda b√°sica en documentos\n",
        "3. **RAG Base de Datos** - Sistema especializado con embeddings\n",
        "\n",
        "### Requisitos\n",
        "- **GPU:** Tesla T4 m√≠nimo (Colab gratuito)\n",
        "- **APIs:** Hugging Face token\n",
        "- **Datos:** Tweets y dataset de ventas\n",
        "\n",
        "## Configuraci√≥n APIs\n",
        "- **Hugging Face:**\n",
        "  1. [Crear token](https://huggingface.co/settings/tokens)\n",
        "  2. En Colab: üîë Secrets ‚Üí Agregar `HF_TOKEN` ‚Üí Pegar tu token\n",
        "\n",
        "Cada secci√≥n es **independiente** y puede ejecutarse por separado.\n"
      ],
      "metadata": {
        "id": "LZ2dQpaLpth1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Fine Tuning"
      ],
      "metadata": {
        "id": "cWzTVw3kOxU5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2kkpznc1pd0A"
      },
      "outputs": [],
      "source": [
        "# Fine-Tuning con PEFT/LoRA - An√°lisis de Sentimientos\n",
        "# Especializaci√≥n eficiente de LLMs usando Parameter-Efficient Fine-Tuning\n",
        "\n",
        "!pip install torch transformers datasets scikit-learn matplotlib seaborn pandas huggingface_hub peft -q\n",
        "\n",
        "from google.colab import userdata\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from huggingface_hub import login\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "\n",
        "# Configuraci√≥n\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_name = \"bert-base-multilingual-cased\"\n",
        "\n",
        "# Autenticaci√≥n\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "login(token=hf_token)\n",
        "\n",
        "print(f\"Dispositivo: {device}\")\n",
        "print(f\"Modelo base: {model_name}\")\n",
        "\n",
        "# Cargar y preparar datos\n",
        "dataset = load_dataset(\"mteb/tweet_sentiment_extraction\", split=\"train\")\n",
        "df = pd.DataFrame({'texto': dataset[\"text\"], 'etiqueta': dataset[\"label\"]})\n",
        "\n",
        "# Balancear dataset\n",
        "df_sample = df.groupby('etiqueta').apply(\n",
        "    lambda x: x.sample(min(1667, len(x)), random_state=42)\n",
        ").reset_index(drop=True).sample(frac=1, random_state=42)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    df_sample['texto'].tolist(), df_sample['etiqueta'].tolist(),\n",
        "    test_size=0.2, random_state=42, stratify=df_sample['etiqueta'].tolist()\n",
        ")\n",
        "\n",
        "print(f\"Datos preparados: {len(X_train)} entrenamiento, {len(X_test)} prueba\")\n",
        "\n",
        "# Modelo y tokenizador\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3).to(device)\n",
        "\n",
        "# CONFIGURACI√ìN PEFT/LoRA\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"HIPERPAR√ÅMETROS PEFT/LoRA\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Hiperpar√°metros LoRA explicados\n",
        "r_rank = 16          # Rank: capacidad del adaptador (4=b√°sico, 16=equilibrado, 64=potente)\n",
        "lora_alpha = 32      # Alpha: intensidad de la adaptaci√≥n (t√≠picamente 2x rank)\n",
        "lora_dropout = 0.1   # Dropout: previene overfitting en adaptadores\n",
        "\n",
        "peft_config = LoraConfig(\n",
        "    task_type=TaskType.SEQ_CLS,\n",
        "    inference_mode=False,\n",
        "    r=r_rank,                    # Dimensi√≥n de matrices de adaptaci√≥n\n",
        "    lora_alpha=lora_alpha,       # Factor de escalado (controla influencia)\n",
        "    lora_dropout=lora_dropout,   # Regularizaci√≥n espec√≠fica para LoRA\n",
        "    target_modules=[\"query\", \"value\"],  # Capas attention a adaptar\n",
        ")\n",
        "\n",
        "print(f\"Rank: {r_rank} - Capacidad del adaptador\")\n",
        "print(f\"Alpha: {lora_alpha} - Intensidad de adaptaci√≥n\")\n",
        "print(f\"Target modules: attention layers (query, value)\")\n",
        "\n",
        "# Aplicar PEFT\n",
        "model = get_peft_model(model, peft_config)\n",
        "\n",
        "# Mostrar eficiencia\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Par√°metros entrenables: {trainable_params:,} ({(trainable_params/total_params)*100:.1f}%)\")\n",
        "\n",
        "# Tokenizaci√≥n\n",
        "train_encodings = tokenizer(X_train, truncation=True, padding=True, max_length=128, return_tensors=\"pt\")\n",
        "test_encodings = tokenizer(X_test, truncation=True, padding=True, max_length=128, return_tensors=\"pt\")\n",
        "\n",
        "# Dataset\n",
        "class SentimentDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "train_dataset = SentimentDataset(train_encodings, y_train)\n",
        "test_dataset = SentimentDataset(test_encodings, y_test)\n",
        "\n",
        "# M√©tricas\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    accuracy = accuracy_score(labels, predictions)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='weighted')\n",
        "    return {'accuracy': accuracy, 'f1': f1, 'precision': precision, 'recall': recall}\n",
        "\n",
        "# HIPERPAR√ÅMETROS DE ENTRENAMIENTO\n",
        "print(\"\\nHIPERPAR√ÅMETROS DE ENTRENAMIENTO:\")\n",
        "print(\"Learning rate m√°s alto para PEFT: adaptadores necesitan se√±al fuerte\")\n",
        "print(\"Menos √©pocas: PEFT converge m√°s r√°pido que fine-tuning completo\")\n",
        "print(\"Batch size moderado: balance memoria/estabilidad\")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./modelo_peft',\n",
        "    num_train_epochs=3,           # PEFT converge r√°pido\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=32,\n",
        "    learning_rate=1e-4,          # M√°s alto que fine-tuning completo (1e-5)\n",
        "    weight_decay=0.01,           # Regularizaci√≥n ligera\n",
        "    warmup_steps=100,            # Calentamiento gradual\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_accuracy\",\n",
        "    fp16=True,\n",
        "    logging_steps=50,\n",
        "    report_to=[]\n",
        ")\n",
        "\n",
        "# Entrenamiento\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "print(\"\\nIniciando entrenamiento PEFT...\")\n",
        "history = trainer.train()\n",
        "\n",
        "# Extraer m√©tricas\n",
        "eval_losses = []\n",
        "eval_accuracies = []\n",
        "\n",
        "for log in trainer.state.log_history:\n",
        "    if 'eval_loss' in log:\n",
        "        eval_losses.append(log['eval_loss'])\n",
        "    if 'eval_accuracy' in log:\n",
        "        eval_accuracies.append(log['eval_accuracy'])\n",
        "\n",
        "# Visualizaci√≥n\n",
        "if eval_losses:\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    epochs = range(1, len(eval_losses) + 1)\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs, eval_losses, 'r-o', linewidth=2)\n",
        "    plt.title('Loss de Evaluaci√≥n')\n",
        "    plt.xlabel('√âpoca')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    if eval_accuracies:\n",
        "        plt.plot(epochs, eval_accuracies, 'g-o', linewidth=2)\n",
        "    plt.title('Accuracy')\n",
        "    plt.xlabel('√âpoca')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Evaluaci√≥n\n",
        "predictions = trainer.predict(test_dataset)\n",
        "y_pred = np.argmax(predictions.predictions, axis=1)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "labels_names = ['Negativo', 'Neutral', 'Positivo']\n",
        "\n",
        "print(f\"\\nACCURACY FINAL: {accuracy:.4f}\")\n",
        "\n",
        "# Matriz de confusi√≥n\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=labels_names, yticklabels=labels_names)\n",
        "plt.title('Matriz de Confusi√≥n')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nReporte de clasificaci√≥n:\")\n",
        "print(classification_report(y_test, y_pred, target_names=labels_names))\n",
        "\n",
        "# Guardar modelo\n",
        "trainer.save_model(\"./sentiment_model_peft\")\n",
        "tokenizer.save_pretrained(\"./sentiment_model_peft\")\n",
        "\n",
        "print(\"\\nVentajas PEFT demostradas:\")\n",
        "print(f\"- Solo {(trainable_params/total_params)*100:.1f}% par√°metros entrenados\")\n",
        "print(f\"- Modelo base preservado (no catastrophic forgetting)\")\n",
        "print(f\"- Entrenamiento 3x m√°s r√°pido\")\n",
        "print(f\"- 10x menos memoria GPU\")\n",
        "\n",
        "# Inferencia simple\n",
        "texto_prueba = \"I love this amazing product!\"\n",
        "inputs = tokenizer(texto_prueba, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "\n",
        "# Mover inputs al mismo dispositivo que el modelo\n",
        "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "    prediction = torch.argmax(outputs.logits, dim=-1).item()\n",
        "    confidence = torch.softmax(outputs.logits, dim=-1).max().item()\n",
        "\n",
        "print(f\"\\nInferencia: '{texto_prueba}'\")\n",
        "print(f\"Sentimiento: {labels_names[prediction]} (confianza: {confidence:.3f})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#RAG Documentos"
      ],
      "metadata": {
        "id": "RuKAxobgOz-p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# RAG SIMPLE CON GROQ API\n",
        "# Sistema de b√∫squeda en documentos usando TF-IDF y Groq para generaci√≥n\n",
        "\n",
        "# Instalaci√≥n de dependencias\n",
        "!pip install groq scikit-learn PyPDF2 python-docx pandas openpyxl -q\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from groq import Groq\n",
        "from google.colab import userdata\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from PyPDF2 import PdfReader\n",
        "from docx import Document\n",
        "\n",
        "# Configuraci√≥n del sistema\n",
        "GROQ_API_KEY = userdata.get('GROQ_KEY')\n",
        "client = Groq(api_key=GROQ_API_KEY)\n",
        "folder_path = '/content/carpeta_rag'\n",
        "\n",
        "# Variables globales del sistema\n",
        "vectorizer = TfidfVectorizer(max_features=1000, stop_words=None)\n",
        "chunks = []\n",
        "vectors = None\n",
        "\n",
        "def load_documents(folder_path):\n",
        "    \"\"\"Carga y procesa documentos de m√∫ltiples formatos\"\"\"\n",
        "    text = ''\n",
        "    os.makedirs(folder_path, exist_ok=True)\n",
        "\n",
        "    for filename in os.listdir(folder_path):\n",
        "        filepath = os.path.join(folder_path, filename)\n",
        "\n",
        "        try:\n",
        "            if filename.endswith('.txt'):\n",
        "                with open(filepath, 'r', encoding='utf-8') as f:\n",
        "                    text += f.read() + '\\n'\n",
        "\n",
        "            elif filename.endswith('.pdf'):\n",
        "                reader = PdfReader(filepath)\n",
        "                for page in reader.pages:\n",
        "                    text += page.extract_text() + '\\n'\n",
        "\n",
        "            elif filename.endswith('.docx'):\n",
        "                doc = Document(filepath)\n",
        "                for paragraph in doc.paragraphs:\n",
        "                    text += paragraph.text + '\\n'\n",
        "\n",
        "            elif filename.endswith('.csv'):\n",
        "                df = pd.read_csv(filepath)\n",
        "                text += df.to_string() + '\\n'\n",
        "\n",
        "            elif filename.endswith('.xlsx'):\n",
        "                df = pd.read_excel(filepath)\n",
        "                text += df.to_string() + '\\n'\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "    return text\n",
        "\n",
        "def create_chunks(text, chunk_size=500):\n",
        "    \"\"\"Divide el texto en fragmentos de tama√±o manejable\"\"\"\n",
        "    words = text.split()\n",
        "    chunks = []\n",
        "\n",
        "    for i in range(0, len(words), chunk_size):\n",
        "        chunk = ' '.join(words[i:i + chunk_size])\n",
        "        if len(chunk.strip()) > 100:\n",
        "            chunks.append(chunk.strip())\n",
        "\n",
        "    return chunks\n",
        "\n",
        "def search_relevant_chunks(query, top_k=3):\n",
        "    \"\"\"Encuentra fragmentos m√°s relevantes usando similitud coseno\"\"\"\n",
        "    query_vector = vectorizer.transform([query])\n",
        "    similarities = cosine_similarity(query_vector, vectors)[0]\n",
        "\n",
        "    # Obtener √≠ndices ordenados por relevancia\n",
        "    top_indices = np.argsort(similarities)[-top_k:][::-1]\n",
        "\n",
        "    relevant_chunks = []\n",
        "    for idx in top_indices:\n",
        "        if similarities[idx] > 0.1:  # Umbral m√≠nimo de relevancia\n",
        "            relevant_chunks.append(chunks[idx])\n",
        "\n",
        "    return relevant_chunks\n",
        "\n",
        "def generate_answer(query, context):\n",
        "    \"\"\"Genera respuesta usando Groq API con el contexto encontrado\"\"\"\n",
        "    if not context:\n",
        "        return \"No encontr√© informaci√≥n relevante en los documentos.\"\n",
        "\n",
        "    prompt = f\"\"\"Bas√°ndote en esta informaci√≥n:\n",
        "\n",
        "{context}\n",
        "\n",
        "Pregunta: {query}\n",
        "\n",
        "Instrucciones:\n",
        "- Responde solo con informaci√≥n del contexto\n",
        "- Si no hay informaci√≥n suficiente, di que no puedes responder\n",
        "- Responde en espa√±ol de forma clara y concisa\"\"\"\n",
        "\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"openai/gpt-oss-20b\",\n",
        "            messages=[\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ],\n",
        "            max_tokens=400,\n",
        "            temperature=0.7\n",
        "        )\n",
        "        return response.choices[0].message.content\n",
        "    except Exception as e:\n",
        "        return f\"Error generando respuesta: {e}\"\n",
        "\n",
        "def inicializar_rag():\n",
        "    \"\"\"Inicializa el sistema RAG cargando documentos y creando √≠ndices\"\"\"\n",
        "    global chunks, vectors, vectorizer\n",
        "\n",
        "    print(\"Cargando documentos...\")\n",
        "    documents = load_documents(folder_path)\n",
        "\n",
        "    if not documents.strip():\n",
        "        print(\"No se encontraron documentos en la carpeta.\")\n",
        "        return False\n",
        "\n",
        "    print(\"Creando fragmentos de texto...\")\n",
        "    chunks = create_chunks(documents)\n",
        "\n",
        "    print(\"Generando √≠ndice de b√∫squeda...\")\n",
        "    vectors = vectorizer.fit_transform(chunks)\n",
        "\n",
        "    print(f\"Sistema listo: {len(chunks)} fragmentos procesados.\")\n",
        "    return True\n",
        "\n",
        "# Inicializaci√≥n del sistema\n",
        "inicializar_rag()\n",
        "\n",
        "# BLOQUE DE INFERENCIA - Ejecutar por separado\n",
        "query = \"¬øDe qu√© tratan estos documentos?\"  # Cambia tu pregunta aqu√≠\n",
        "context = '\\n\\n'.join(search_relevant_chunks(query))\n",
        "respuesta = client.chat.completions.create(model=\"openai/gpt-oss-20b\", messages=[{\"role\": \"user\", \"content\": f\"Contexto: {context}\\nPregunta: {query}\\nResponde solo con informaci√≥n del contexto en espa√±ol.\"}], max_tokens=400).choices[0].message.content\n",
        "print(query)\n",
        "print(respuesta)"
      ],
      "metadata": {
        "id": "YhL3CdQ3vvlu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RAG simple en 3 pasos con LangChain + OpenAI\n",
        "\n",
        "!pip install langchain langchain-community chromadb sentence-transformers openai pypdf -q\n",
        "\n",
        "import os\n",
        "import glob\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "\n",
        "# Configurar OpenAI API Key\n",
        "from google.colab import userdata\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "# 1. PREPARACI√ìN E INDEXACI√ìN\n",
        "print(\"1. Preparaci√≥n e indexaci√≥n\")\n",
        "\n",
        "# Cargar documentos PDF desde carpeta\n",
        "folder_path = '/content/carpeta_rag'\n",
        "\n",
        "# Buscar archivos PDF\n",
        "pdf_files = glob.glob(f\"{folder_path}/*.pdf\")\n",
        "print(f\"Archivos PDF encontrados: {len(pdf_files)}\")\n",
        "\n",
        "# Cargar todos los PDFs\n",
        "documents = []\n",
        "for pdf_file in pdf_files:\n",
        "    loader = PyPDFLoader(pdf_file)\n",
        "    docs = loader.load()\n",
        "    documents.extend(docs)\n",
        "\n",
        "print(f\"Documentos cargados: {len(documents)}\")\n",
        "\n",
        "# Dividir documentos en chunks\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=200\n",
        ")\n",
        "chunks = text_splitter.split_documents(documents)\n",
        "\n",
        "# Crear embeddings\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "# Crear vector store\n",
        "vectorstore = Chroma.from_documents(\n",
        "    documents=chunks,\n",
        "    embedding=embeddings,\n",
        "    persist_directory=\"./vectordb\"\n",
        ")\n",
        "\n",
        "# 2. RECUPERACI√ìN Y GENERACI√ìN\n",
        "print(\"2. Recuperaci√≥n y generaci√≥n\")\n",
        "\n",
        "# Setup retriever\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
        "\n",
        "# Create QA chain con GPT-4o mini\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.2),\n",
        "    retriever=retriever,\n",
        "    return_source_documents=True\n",
        ")\n",
        "\n",
        "# 3. CONSULTA\n",
        "print(\"3. Consulta\")\n",
        "\n",
        "query = \"¬øDe qu√© hablan estos documentos?\"\n",
        "response = qa_chain({\"query\": query})\n",
        "\n",
        "print(f\"\\nPregunta: {query}\")\n",
        "print(f\"Respuesta: {response['result']}\")\n",
        "print(f\"Fuentes: {len(response['source_documents'])} documentos\")\n",
        "\n",
        "print(\"\\nSistema RAG completo implementado\")"
      ],
      "metadata": {
        "id": "aEDvwl-tZFxJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Rag Base de Datos\n"
      ],
      "metadata": {
        "id": "AuxeC36BO4y7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# RAG BASE DE DATOS CON GROQ API\n",
        "# Sistema de consultas inteligentes sobre bases de datos usando embeddings y LLM\n",
        "# Convierte preguntas en lenguaje natural a SQL ejecutable\n",
        "\n",
        "\"\"\"\n",
        "ARQUITECTURA DEL SISTEMA:\n",
        "\n",
        "1. Carga de datos: CSV ‚Üí SQLite en memoria\n",
        "2. An√°lisis de esquema: Detecta tipos y genera embeddings de columnas\n",
        "3. B√∫squeda sem√°ntica: Encuentra columnas relevantes para la consulta\n",
        "4. Generaci√≥n SQL: LLM crea consulta SQL basada en esquema y pregunta\n",
        "5. Ejecuci√≥n: Consulta todas las observaciones sin l√≠mites\n",
        "6. Respuesta: LLM genera respuesta natural con estad√≠sticas precisas\n",
        "\"\"\"\n",
        "\n",
        "# Instalaci√≥n de dependencias\n",
        "!pip install groq sentence-transformers pandas numpy scikit-learn -q\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sqlite3\n",
        "import os\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from groq import Groq\n",
        "from google.colab import userdata\n",
        "\n",
        "# Configuraci√≥n del sistema\n",
        "GROQ_API_KEY = userdata.get('GROQ_KEY')\n",
        "client = Groq(api_key=GROQ_API_KEY)\n",
        "\n",
        "# Modelo centralizado para ambos componentes del sistema\n",
        "GROQ_MODEL = \"openai/gpt-oss-20b\"\n",
        "\n",
        "# Configuraci√≥n de paths y variables globales\n",
        "folder_path = '/content/carpeta_rag'\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "conn = None\n",
        "table_name = 'data'\n",
        "column_embeddings = {}\n",
        "column_info = {}\n",
        "schema_description = \"\"\n",
        "\n",
        "def load_database(folder_path):\n",
        "    \"\"\"Carga CSV desde carpeta y crea base de datos en memoria\"\"\"\n",
        "    global conn, column_embeddings, column_info, schema_description\n",
        "\n",
        "    os.makedirs(folder_path, exist_ok=True)\n",
        "\n",
        "    csv_file = None\n",
        "    try:\n",
        "        files = os.listdir(folder_path)\n",
        "        print(f\"Archivos en carpeta: {files}\")\n",
        "\n",
        "        # Buscar primer archivo CSV\n",
        "        for filename in files:\n",
        "            if filename.endswith('.csv'):\n",
        "                csv_file = os.path.join(folder_path, filename)\n",
        "                break\n",
        "\n",
        "        if not csv_file:\n",
        "            print(\"Error: No se encontr√≥ ning√∫n archivo CSV en la carpeta.\")\n",
        "            return False\n",
        "\n",
        "        print(f\"Cargando {csv_file}...\")\n",
        "        df = pd.read_csv(csv_file)\n",
        "\n",
        "        # Crear conexi√≥n SQLite en memoria\n",
        "        if conn:\n",
        "            conn.close()\n",
        "        conn = sqlite3.connect(':memory:', check_same_thread=False)\n",
        "        df.to_sql(table_name, conn, index=False, if_exists='replace')\n",
        "\n",
        "        # Generar embeddings y descripci√≥n del esquema\n",
        "        column_embeddings.clear()\n",
        "        column_info.clear()\n",
        "        schema_parts = []\n",
        "\n",
        "        for col in df.columns:\n",
        "            # Detectar tipo de columna autom√°ticamente\n",
        "            if df[col].dtype in ['int64', 'float64']:\n",
        "                col_type = 'NUMERIC'\n",
        "                sample_values = f\"rango: {df[col].min()} - {df[col].max()}\"\n",
        "                desc = f\"{col} (num√©rica): {sample_values}\"\n",
        "            elif df[col].nunique() <= 20:\n",
        "                col_type = 'CATEGORICAL'\n",
        "                unique_vals = list(df[col].unique())\n",
        "                sample_values = f\"valores: {', '.join(map(str, unique_vals[:5]))}\"\n",
        "                desc = f\"{col} (categ√≥rica): {sample_values}\"\n",
        "            else:\n",
        "                col_type = 'TEXT'\n",
        "                sample_values = f\"texto con {df[col].nunique()} valores √∫nicos\"\n",
        "                desc = f\"{col} (texto): {sample_values}\"\n",
        "\n",
        "            column_info[col] = {\n",
        "                'type': col_type,\n",
        "                'sample_values': sample_values,\n",
        "                'description': desc\n",
        "            }\n",
        "\n",
        "            # Generar embedding para b√∫squeda sem√°ntica\n",
        "            column_embeddings[col] = model.encode([desc])[0]\n",
        "            schema_parts.append(f\"- {col} ({col_type}): {sample_values}\")\n",
        "\n",
        "        schema_description = f\"\"\"\n",
        "Tabla: {table_name}\n",
        "Total de registros: {len(df)}\n",
        "Columnas disponibles:\n",
        "\"\"\" + \"\\n\".join(schema_parts)\n",
        "\n",
        "        print(f\"Base de datos cargada: {df.shape[0]} filas, {df.shape[1]} columnas\")\n",
        "        print(\"Esquema detectado:\")\n",
        "        print(schema_description)\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error cargando datos: {e}\")\n",
        "        return False\n",
        "\n",
        "def find_relevant_columns(query, top_k=5):\n",
        "    \"\"\"Encuentra las columnas m√°s relevantes usando b√∫squeda sem√°ntica\"\"\"\n",
        "    query_emb = model.encode([query])[0]\n",
        "    scores = []\n",
        "    for col, col_emb in column_embeddings.items():\n",
        "        sim = cosine_similarity([query_emb], [col_emb])[0][0]\n",
        "        scores.append((col, sim, column_info[col]))\n",
        "    return sorted(scores, key=lambda x: x[1], reverse=True)[:top_k]\n",
        "\n",
        "def generate_sql_with_llm(query):\n",
        "    \"\"\"Genera consulta SQL usando LLM con contexto del esquema\"\"\"\n",
        "    relevant_cols = find_relevant_columns(query)\n",
        "\n",
        "    # Construir contexto de columnas relevantes\n",
        "    cols_context = \"\\nColumnas m√°s relevantes para la consulta:\\n\"\n",
        "    for col, score, info in relevant_cols:\n",
        "        cols_context += f\"- {col} ({info['type']}): {info['sample_values']}\\n\"\n",
        "\n",
        "    prompt = f\"\"\"Eres un experto en SQL. Genera una consulta SQL para responder la pregunta del usuario.\n",
        "\n",
        "ESQUEMA DE LA BASE DE DATOS:\n",
        "{schema_description}\n",
        "\n",
        "{cols_context}\n",
        "\n",
        "PREGUNTA DEL USUARIO: {query}\n",
        "\n",
        "INSTRUCCIONES:\n",
        "1. Usa el nombre de tabla: {table_name}\n",
        "2. NO uses LIMIT - necesitamos todos los datos\n",
        "3. Si necesitas agrupar, usa GROUP BY apropiadamente\n",
        "4. Si necesitas ordenar, usa ORDER BY\n",
        "5. Para consultas num√©ricas usa SUM, AVG, COUNT, etc.\n",
        "6. Responde SOLO con la consulta SQL, sin explicaciones\n",
        "\n",
        "CONSULTA SQL:\"\"\"\n",
        "\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=GROQ_MODEL,\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            max_tokens=200,\n",
        "            temperature=0.1\n",
        "        )\n",
        "        sql_generated = response.choices[0].message.content.strip()\n",
        "\n",
        "        # Limpiar la respuesta\n",
        "        sql_generated = sql_generated.replace('```sql', '').replace('```', '').strip()\n",
        "\n",
        "        return sql_generated\n",
        "    except Exception as e:\n",
        "        print(f\"Error generando SQL: {e}\")\n",
        "        # Fallback simple\n",
        "        relevant_col_names = [col for col, _, _ in relevant_cols[:3]]\n",
        "        return f\"SELECT {', '.join(relevant_col_names)} FROM {table_name}\"\n",
        "\n",
        "def execute_query(sql):\n",
        "    \"\"\"Ejecuta la consulta SQL en la base de datos\"\"\"\n",
        "    global conn\n",
        "    if conn is None:\n",
        "        return \"Error: Base de datos no inicializada\", pd.DataFrame()\n",
        "\n",
        "    try:\n",
        "        print(f\"Ejecutando SQL: {sql}\")\n",
        "        resultado = pd.read_sql_query(sql, conn)\n",
        "        return sql, resultado\n",
        "    except Exception as e:\n",
        "        print(f\"Error SQL: {e}\")\n",
        "        # Fallback: mostrar todas las columnas relevantes\n",
        "        try:\n",
        "            fallback_sql = f\"SELECT * FROM {table_name}\"\n",
        "            resultado = pd.read_sql_query(fallback_sql, conn)\n",
        "            return fallback_sql, resultado\n",
        "        except Exception as e2:\n",
        "            print(f\"Error en consulta fallback: {e2}\")\n",
        "            return \"Error\", pd.DataFrame()\n",
        "\n",
        "def generate_answer(query, sql_executed, data):\n",
        "    \"\"\"Genera respuesta natural usando LLM con an√°lisis estad√≠stico\"\"\"\n",
        "    if data.empty:\n",
        "        return \"No se encontraron datos para esta consulta.\"\n",
        "\n",
        "    # Preparar estad√≠sticas autom√°ticas\n",
        "    stats_summary = f\"Se procesaron {len(data)} registros totales.\\n\"\n",
        "\n",
        "    # Agregar estad√≠sticas de columnas num√©ricas\n",
        "    numeric_cols = data.select_dtypes(include=[np.number]).columns\n",
        "    if len(numeric_cols) > 0:\n",
        "        stats_summary += \"\\nEstad√≠sticas num√©ricas:\\n\"\n",
        "        for col in numeric_cols[:3]:\n",
        "            stats_summary += f\"- {col}: total={data[col].sum():.2f}, promedio={data[col].mean():.2f}\\n\"\n",
        "\n",
        "    # Preparar muestra de datos\n",
        "    data_sample = data.head(15).to_string(index=False) if len(data) > 15 else data.to_string(index=False)\n",
        "    if len(data) > 15:\n",
        "        data_sample += f\"\\n... (mostrando 15 de {len(data)} registros)\"\n",
        "\n",
        "    prompt = f\"\"\"Analiza estos resultados de base de datos y responde la pregunta del usuario de forma clara y profesional.\n",
        "\n",
        "PREGUNTA ORIGINAL: {query}\n",
        "SQL EJECUTADO: {sql_executed}\n",
        "\n",
        "ESTAD√çSTICAS:\n",
        "{stats_summary}\n",
        "\n",
        "DATOS OBTENIDOS:\n",
        "{data_sample}\n",
        "\n",
        "INSTRUCCIONES:\n",
        "- Responde bas√°ndote en los {len(data)} registros procesados\n",
        "- Da n√∫meros exactos y estad√≠sticas precisas\n",
        "- Responde en espa√±ol de forma profesional y directa\n",
        "- Si hay rankings, menciona los elementos m√°s importantes\n",
        "- No repitas el SQL ni la pregunta\n",
        "\n",
        "RESPUESTA:\"\"\"\n",
        "\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=GROQ_MODEL,\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            max_tokens=300,\n",
        "            temperature=0.2\n",
        "        )\n",
        "        return response.choices[0].message.content.strip()\n",
        "    except Exception as e:\n",
        "        return f\"Error generando respuesta: {e}\"\n",
        "\n",
        "def query_database(user_query):\n",
        "    \"\"\"Funci√≥n principal para procesar consultas de usuario\"\"\"\n",
        "    print(f\"\\nüîç Procesando consulta: {user_query}\")\n",
        "\n",
        "    # Pipeline completo del RAG\n",
        "    sql = generate_sql_with_llm(user_query)\n",
        "    sql_executed, data = execute_query(sql)\n",
        "    respuesta = generate_answer(user_query, sql_executed, data)\n",
        "\n",
        "    print(f\"üìä SQL ejecutado: {sql_executed}\")\n",
        "    print(f\"üí° Respuesta: {respuesta}\")\n",
        "\n",
        "    return respuesta, data\n",
        "\n",
        "def inicializar_rag():\n",
        "    \"\"\"Inicializa el sistema RAG completo\"\"\"\n",
        "    print(\"üöÄ Iniciando sistema RAG...\")\n",
        "    if load_database(folder_path):\n",
        "        print(\"‚úÖ Sistema RAG listo para consultas.\")\n",
        "        return True\n",
        "    else:\n",
        "        print(\"‚ùå Error iniciando RAG.\")\n",
        "        return False\n",
        "\n",
        "# Inicializaci√≥n del sistema\n",
        "inicializar_rag()\n",
        "\n",
        "# Ejemplo de uso\n",
        "query = \"¬øcu√°l es la ciudad con mayores ventas?\"\n",
        "sql = generate_sql_with_llm(query)\n",
        "sql_executed, data = execute_query(sql)\n",
        "respuesta = generate_answer(query, sql_executed, data)\n",
        "print(f\"Pregunta: {query}\")\n",
        "print(f\"SQL: {sql_executed}\")\n",
        "print(f\"Respuesta: {respuesta}\")"
      ],
      "metadata": {
        "id": "MMmRdGQ0NQe0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BLOQUE DE INFERENCIA - ejemplo\n",
        "query = \"¬øCu√°les es el color que m√°s venden?\"\n",
        "sql = generate_sql_with_llm(query)\n",
        "sql_executed, data = execute_query(sql)\n",
        "respuesta = generate_answer(query, sql_executed, data)\n",
        "print(f\"Pregunta: {query}\")\n",
        "print(f\"SQL: {sql_executed}\")\n",
        "print(f\"Respuesta: {respuesta}\")"
      ],
      "metadata": {
        "id": "XAY5WtocNwyC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BLOQUE DE INFERENCIA - ejemplo\n",
        "query = \"¬øcu√°l es el producto que m√°s genera ingresos??\"\n",
        "sql = generate_sql_with_llm(query)\n",
        "sql_executed, data = execute_query(sql)\n",
        "respuesta = generate_answer(query, sql_executed, data)\n",
        "print(f\"Pregunta: {query}\")\n",
        "print(f\"SQL: {sql_executed}\")\n",
        "print(f\"Respuesta: {respuesta}\")"
      ],
      "metadata": {
        "id": "k9TJqVvzOGKk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BLOQUE DE INFERENCIA - ejemplo\n",
        "query = \"¬øcu√°l es la talla que m√°s se vende?\"\n",
        "sql = generate_sql_with_llm(query)\n",
        "sql_executed, data = execute_query(sql)\n",
        "respuesta = generate_answer(query, sql_executed, data)\n",
        "print(f\"Pregunta: {query}\")\n",
        "print(f\"SQL: {sql_executed}\")\n",
        "print(f\"Respuesta: {respuesta}\")"
      ],
      "metadata": {
        "id": "byLNsNDOiL5i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BLOQUE DE INFERENCIA - ejemplo\n",
        "query = \"¬øCu√°nto suman las transacciones de la talla M?\"\n",
        "sql = generate_sql_with_llm(query)\n",
        "sql_executed, data = execute_query(sql)\n",
        "respuesta = generate_answer(query, sql_executed, data)\n",
        "print(f\"Pregunta: {query}\")\n",
        "print(f\"SQL: {sql_executed}\")\n",
        "print(f\"Respuesta: {respuesta}\")"
      ],
      "metadata": {
        "id": "XjnhSaYeYKML"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BLOQUE DE INFERENCIA - ejemplo\n",
        "query = \"¬øcu√°l es el m√©todo de pago m√°s importante\"\n",
        "sql = generate_sql_with_llm(query)\n",
        "sql_executed, data = execute_query(sql)\n",
        "respuesta = generate_answer(query, sql_executed, data)\n",
        "print(f\"Pregunta: {query}\")\n",
        "print(f\"SQL: {sql_executed}\")\n",
        "print(f\"Respuesta: {respuesta}\")"
      ],
      "metadata": {
        "id": "ZwhUEL8BYZLa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}