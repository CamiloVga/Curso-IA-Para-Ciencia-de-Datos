{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CamiloVga/Curso-IA-Para-Ciencia-de-Datos/blob/main/Script_Sesi%C3%B3n_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wizyzEVJPPix"
      },
      "source": [
        "# IA para la Ciencia de Datos\n",
        "## Universidad de los Andes\n",
        "\n",
        "**Profesor:** Camilo Vega - AI/ML Engineer  \n",
        "**LinkedIn:** https://www.linkedin.com/in/camilo-vega-169084b1/\n",
        "\n",
        "---\n",
        "\n",
        "## Gu√≠a: GenBI - Interfaces Conversacionales de Datos con LLM\n",
        "\n",
        "Este notebook presenta **2 implementaciones pr√°cticas**:\n",
        "\n",
        "1. **RAG con Gradio** - Chat de documentos con interfaz simple\n",
        "2. **Text-to SQL y MatplotLib y Seaborn** - Consultas naturales a bases de datos\n",
        "\n",
        "### Requisitos\n",
        "- **APIs:** Groq API token\n",
        "- **GPU:** Opcional para modelos locales\n",
        "- **Datos:** Documentos PDF/DOCX/TXT/CSV para subir\n",
        "\n",
        "## Configuraci√≥n APIs\n",
        "- **Groq API:**\n",
        "  1. [Crear token](https://console.groq.com/keys)\n",
        "  2. En Colab: üîë Secrets ‚Üí Agregar `GROQ_KEY` ‚Üí Pegar tu token\n",
        "\n",
        "Cada secci√≥n es **independiente** y puede ejecutarse por separado"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqfevid61eyO"
      },
      "source": [
        "#Gradio para RAG con Documentos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O5FKCZKxPPHH"
      },
      "outputs": [],
      "source": [
        "# --- 1. Instalaci√≥n de Dependencias ---\n",
        "!pip install gradio groq scikit-learn PyPDF2 python-docx pandas openpyxl -q\n",
        "\n",
        "# --- 2. Importaci√≥n de Librer√≠as ---\n",
        "import os\n",
        "import gradio as gr\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from groq import Groq\n",
        "from google.colab import userdata\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from PyPDF2 import PdfReader\n",
        "from docx import Document\n",
        "\n",
        "# --- 3. Configuraci√≥n Global ---\n",
        "try:\n",
        "    GROQ_API_KEY = userdata.get('GROQ_KEY')\n",
        "    client = Groq(api_key=GROQ_API_KEY)\n",
        "    GROQ_MODEL = \"llama-3.1-8b-instant\"\n",
        "except (ImportError, KeyError):\n",
        "    print(\"ADVERTENCIA: No se encontr√≥ la clave de API de Groq.\")\n",
        "    client = None\n",
        "    GROQ_MODEL = None\n",
        "\n",
        "\n",
        "# --- 4. Clase Principal del Motor RAG ---\n",
        "class DocumentRAG:\n",
        "    \"\"\"Encapsula la l√≥gica para procesar documentos, crear vectores y generar respuestas.\"\"\"\n",
        "    def __init__(self):\n",
        "        self.vectorizer = TfidfVectorizer(max_features=2000, ngram_range=(1, 2), min_df=2)\n",
        "        self.chunks = []\n",
        "        self.vectors = None\n",
        "        self.is_ready = False\n",
        "\n",
        "    def load_documents(self, files):\n",
        "        if not files: return \"No se subieron archivos.\"\n",
        "        combined_text = \"\"\n",
        "        processed_files = []\n",
        "        for file in files:\n",
        "            filename = os.path.basename(file.name)\n",
        "            try:\n",
        "                if filename.endswith('.txt'):\n",
        "                    with open(file.name, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "                        combined_text += f.read() + \"\\n\\n\"\n",
        "                elif filename.endswith('.pdf'):\n",
        "                    reader = PdfReader(file.name)\n",
        "                    for page in reader.pages: combined_text += (page.extract_text() or \"\") + \"\\n\"\n",
        "                elif filename.endswith('.docx'):\n",
        "                    doc = Document(file.name)\n",
        "                    for p in doc.paragraphs: combined_text += p.text + \"\\n\"\n",
        "                elif filename.endswith(('.csv', '.xlsx')):\n",
        "                    df = pd.read_csv(file.name) if filename.endswith('.csv') else pd.read_excel(file.name)\n",
        "                    combined_text += df.to_string() + \"\\n\\n\"\n",
        "                processed_files.append(filename)\n",
        "            except Exception:\n",
        "                continue\n",
        "        if combined_text.strip():\n",
        "            self._create_knowledge_base(combined_text)\n",
        "            return f\"‚úÖ {len(processed_files)} archivos procesados. {len(self.chunks)} fragmentos de conocimiento creados.\"\n",
        "        return \"‚ùå No se pudo extraer texto de los archivos.\"\n",
        "\n",
        "    def _create_knowledge_base(self, text):\n",
        "        text = text.replace('\\n', ' ').strip()\n",
        "        sentences = text.split('.')\n",
        "        current_chunk = \"\"\n",
        "        for sentence in sentences:\n",
        "            if len(current_chunk) + len(sentence) < 1000:\n",
        "                current_chunk += sentence + \". \"\n",
        "            else:\n",
        "                if len(current_chunk) > 150: self.chunks.append(current_chunk.strip())\n",
        "                current_chunk = sentence + \". \"\n",
        "        if len(current_chunk) > 150: self.chunks.append(current_chunk.strip())\n",
        "        if self.chunks:\n",
        "            self.vectors = self.vectorizer.fit_transform(self.chunks)\n",
        "            self.is_ready = True\n",
        "\n",
        "    def get_response(self, message):\n",
        "        if not self.is_ready or not client:\n",
        "            return \"El sistema no est√° listo. Por favor, carga documentos primero.\"\n",
        "        query_vector = self.vectorizer.transform([message])\n",
        "        similarities = cosine_similarity(query_vector, self.vectors)[0]\n",
        "        top_indices = [idx for idx in np.argsort(similarities)[-3:][::-1] if similarities[idx] > 0.05]\n",
        "        if not top_indices:\n",
        "            return \"No encontr√© informaci√≥n relevante para tu pregunta.\"\n",
        "        context = \"\\n\\n---\\n\\n\".join([self.chunks[idx] for idx in top_indices])\n",
        "        prompt = f\"Bas√°ndote en este contexto:\\n\\n{context}\\n\\nPregunta: {message}\\n\\nResponde en espa√±ol, de forma concisa y solo con la informaci√≥n proporcionada.\"\n",
        "        try:\n",
        "            response = client.chat.completions.create(\n",
        "                model=GROQ_MODEL, messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                max_tokens=500, temperature=0.1)\n",
        "            return response.choices[0].message.content\n",
        "        except Exception as e:\n",
        "            return f\"Error con el modelo de lenguaje: {e}\"\n",
        "\n",
        "# --- 5. Funciones de la Interfaz Gradio ---\n",
        "def handle_file_upload(files, rag_state):\n",
        "    rag_state = DocumentRAG()\n",
        "    status = rag_state.load_documents(files)\n",
        "    return status, rag_state\n",
        "\n",
        "def get_chat_response(message, history, rag_state):\n",
        "    if not rag_state.is_ready:\n",
        "        return \"Por favor, carga primero algunos documentos para empezar a chatear.\"\n",
        "    return rag_state.get_response(message)\n",
        "\n",
        "# --- 6. Creaci√≥n de la Interfaz de Usuario ---\n",
        "def create_interface():\n",
        "    with gr.Blocks(theme=gr.themes.Soft(), title=\"Chat de Documentos\") as app:\n",
        "        rag_state = gr.State(DocumentRAG())\n",
        "        with gr.Row():\n",
        "            with gr.Column(scale=1):\n",
        "                gr.Markdown(\"### 1. Carga tus Documentos\")\n",
        "                file_upload = gr.File(label=\"Sube archivos (.pdf, .docx, .txt)\", file_count=\"multiple\")\n",
        "                upload_status = gr.Textbox(label=\"Estado\", interactive=False)\n",
        "            with gr.Column(scale=2):\n",
        "                gr.Markdown(\"### 2. Chatea con ellos\")\n",
        "                chat_interface = gr.ChatInterface(\n",
        "                    fn=get_chat_response,\n",
        "                    additional_inputs=[rag_state],\n",
        "                    examples=[\n",
        "                        [\"¬øCu√°l es el tema principal de los documentos?\"],\n",
        "                        [\"Resume los puntos clave en tres vi√±etas\"],\n",
        "                        [\"¬øHay alguna conclusi√≥n importante?\"]\n",
        "                    ]\n",
        "                )\n",
        "        file_upload.change(\n",
        "            fn=handle_file_upload,\n",
        "            inputs=[file_upload, rag_state],\n",
        "            outputs=[upload_status, rag_state]\n",
        "        )\n",
        "    return app\n",
        "\n",
        "# --- 8. Lanzamiento de la Aplicaci√≥n ---\n",
        "if __name__ == \"__main__\":\n",
        "    app = create_interface()\n",
        "    app.launch(share=True, debug=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdrCIkY21qfm"
      },
      "source": [
        "#Gradio para RAG Base Datos y Gr√°ficas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A43YnC2mQsiB"
      },
      "outputs": [],
      "source": [
        "# --- 1. Instalaci√≥n de Dependencias ---\n",
        "# Instala todas las librer√≠as necesarias para el funcionamiento del script.\n",
        "!pip install groq sentence-transformers pandas numpy scikit-learn gradio matplotlib seaborn plotly openpyxl -q\n",
        "\n",
        "# --- 2. Importaci√≥n de Librer√≠as ---\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sqlite3\n",
        "import warnings\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from groq import Groq\n",
        "import gradio as gr\n",
        "import plotly.express as px\n",
        "from google.colab import userdata\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# --- 3. Configuraci√≥n Global ---\n",
        "# Define constantes y configuraciones que se utilizar√°n en toda la aplicaci√≥n.\n",
        "\n",
        "# Clave de API para el modelo de lenguaje (se obtiene de los secrets de Google Colab).\n",
        "try:\n",
        "    GROQ_API_KEY = userdata.get('GROQ_KEY')\n",
        "    client = Groq(api_key=GROQ_API_KEY)\n",
        "except (ImportError, KeyError):\n",
        "    print(\"ADVERTENCIA: No se encontr√≥ la clave de API de Groq. El sistema funcionar√° sin LLM.\")\n",
        "    client = None\n",
        "\n",
        "# Modelos a utilizar.\n",
        "GROQ_MODEL = \"llama-3.1-8b-instant\"\n",
        "EMBEDDING_MODEL = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Constantes del sistema.\n",
        "TABLE_NAME = 'data'\n",
        "MAX_ROWS_FOR_ANALYSIS = 5000 # L√≠mite para visualizaci√≥n en gr√°ficas para no saturar el navegador.\n",
        "\n",
        "\n",
        "# --- 4. Clase Principal del Sistema RAG ---\n",
        "class DatabaseRAG:\n",
        "    \"\"\"\n",
        "    Encapsula toda la l√≥gica para cargar datos, generar SQL, ejecutar consultas\n",
        "    y crear respuestas. Cada sesi√≥n de usuario tendr√° su propia instancia de esta clase.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"Constructor: Inicializa las variables de estado para una sesi√≥n.\"\"\"\n",
        "        self.conn = None\n",
        "        self.column_embeddings = {}\n",
        "        self.column_info = {}\n",
        "        self.schema_description = \"\"\n",
        "        self.dataframe_head = pd.DataFrame()\n",
        "\n",
        "    def load_file(self, file_path):\n",
        "        \"\"\"Carga un archivo (CSV, XLSX, JSON) en una base de datos SQLite en memoria.\"\"\"\n",
        "        try:\n",
        "            # Lee el archivo seg√∫n su extensi√≥n.\n",
        "            if file_path.lower().endswith('.csv'):\n",
        "                df = pd.read_csv(file_path)\n",
        "            elif file_path.lower().endswith(('.xlsx', '.xls')):\n",
        "                df = pd.read_excel(file_path)\n",
        "            elif file_path.lower().endswith('.json'):\n",
        "                df = pd.read_json(file_path)\n",
        "            else:\n",
        "                return False, \"Formato no soportado. Use CSV, XLSX o JSON.\"\n",
        "\n",
        "            if df.empty:\n",
        "                return False, \"El archivo est√° vac√≠o.\"\n",
        "\n",
        "            # Crea la base de datos en memoria y carga los datos.\n",
        "            self.conn = sqlite3.connect(':memory:', check_same_thread=False)\n",
        "            df.to_sql(TABLE_NAME, self.conn, index=False, if_exists='replace')\n",
        "\n",
        "            # Genera la descripci√≥n del esquema y los embeddings de las columnas.\n",
        "            self._generate_schema(df)\n",
        "\n",
        "            return True, f\"‚úÖ Archivo cargado: {df.shape[0]} filas, {df.shape[1]} columnas.\"\n",
        "\n",
        "        except Exception as e:\n",
        "            return False, f\"Error cargando archivo: {str(e)}\"\n",
        "\n",
        "    def _generate_schema(self, df):\n",
        "        \"\"\"Analiza el DataFrame para generar una descripci√≥n del esquema y embeddings de columnas.\"\"\"\n",
        "        schema_parts = []\n",
        "        for col in df.columns:\n",
        "            # Detecta el tipo de dato de cada columna.\n",
        "            if pd.api.types.is_numeric_dtype(df[col]):\n",
        "                col_type = 'NUMERIC'\n",
        "                sample_values = f\"rango: {df[col].min():.2f} - {df[col].max():.2f}\"\n",
        "            elif df[col].nunique() <= 20:\n",
        "                col_type = 'CATEGORICAL'\n",
        "                unique_vals = [str(val) for val in df[col].unique()[:5]]\n",
        "                sample_values = f\"valores: {', '.join(unique_vals)}\"\n",
        "            else:\n",
        "                col_type = 'TEXT'\n",
        "                sample_values = f\"texto con {df[col].nunique()} valores √∫nicos\"\n",
        "\n",
        "            # Crea una descripci√≥n para la columna y genera su embedding.\n",
        "            desc = f\"{col} ({col_type}): {sample_values}\"\n",
        "            self.column_info[col] = {'type': col_type, 'sample_values': sample_values}\n",
        "            self.column_embeddings[col] = EMBEDDING_MODEL.encode([desc])[0]\n",
        "            schema_parts.append(f\"- {desc}\")\n",
        "\n",
        "        self.schema_description = f\"Tabla: {TABLE_NAME}\\nTotal de registros: {len(df)}\\nColumnas:\\n\" + \"\\n\".join(schema_parts)\n",
        "\n",
        "    def find_relevant_columns(self, query, top_k=5):\n",
        "        \"\"\"Encuentra las columnas m√°s relevantes para una consulta usando b√∫squeda sem√°ntica.\"\"\"\n",
        "        query_emb = EMBEDDING_MODEL.encode([query])[0]\n",
        "        scores = [(col, cosine_similarity([query_emb], [emb])[0][0]) for col, emb in self.column_embeddings.items()]\n",
        "        return sorted(scores, key=lambda x: x[1], reverse=True)[:top_k]\n",
        "\n",
        "    def generate_sql(self, query):\n",
        "        \"\"\"Genera una consulta SQL a partir de la pregunta del usuario usando el LLM.\"\"\"\n",
        "        if not self.conn or not client:\n",
        "            return \"SELECT 1\"\n",
        "\n",
        "        relevant_cols_info = self.find_relevant_columns(query)\n",
        "        cols_context = \"\\nColumnas m√°s relevantes:\\n\" + \"\\n\".join([f\"- {col} ({self.column_info[col]['type']}): {self.column_info[col]['sample_values']}\" for col, _ in relevant_cols_info])\n",
        "\n",
        "        # El prompt gu√≠a al LLM para que genere un SQL correcto y seguro.\n",
        "        prompt = f\"\"\"Eres un experto en SQL. Genera una consulta SQL para la pregunta del usuario.\n",
        "\n",
        "ESQUEMA:\n",
        "{self.schema_description}\n",
        "{cols_context}\n",
        "\n",
        "PREGUNTA: {query}\n",
        "\n",
        "INSTRUCCIONES:\n",
        "1. Usa el nombre de tabla: {TABLE_NAME}\n",
        "2. Si la pregunta pide una lista de datos, usa 'LIMIT 1000' para proteger la memoria.\n",
        "3. NO uses LIMIT para agregaciones (COUNT, SUM, AVG).\n",
        "4. Responde SOLO con la consulta SQL.\n",
        "\n",
        "CONSULTA SQL:\"\"\"\n",
        "\n",
        "        try:\n",
        "            # Llama a la API del LLM.\n",
        "            response = client.chat.completions.create(model=GROQ_MODEL, messages=[{\"role\": \"user\", \"content\": prompt}], max_tokens=200, temperature=0.1)\n",
        "            return response.choices[0].message.content.strip().replace('```sql', '').replace('```', '').strip()\n",
        "        except Exception as e:\n",
        "            print(f\"Error generando SQL: {e}\")\n",
        "            return f\"SELECT * FROM {TABLE_NAME} LIMIT 100\" # Fallback seguro.\n",
        "\n",
        "    def execute_query(self, sql):\n",
        "        \"\"\"Ejecuta la consulta SQL en la base de datos en memoria.\"\"\"\n",
        "        if not self.conn:\n",
        "            return \"Error: Base de datos no inicializada\", pd.DataFrame()\n",
        "        try:\n",
        "            return sql, pd.read_sql_query(sql, self.conn)\n",
        "        except Exception as e:\n",
        "            # Si falla la consulta, intenta con una consulta de respaldo.\n",
        "            fallback_sql = f\"SELECT * FROM {TABLE_NAME} LIMIT 100\"\n",
        "            try:\n",
        "                return fallback_sql, pd.read_sql_query(fallback_sql, self.conn)\n",
        "            except Exception as e2:\n",
        "                return f\"Error Cr√≠tico: {e2}\", pd.DataFrame()\n",
        "\n",
        "    def generate_answer(self, query, sql_executed, data):\n",
        "        \"\"\"Genera una respuesta en lenguaje natural basada en los resultados de la consulta.\"\"\"\n",
        "        if not client or data.empty:\n",
        "            return \"No se encontraron datos o el servicio LLM no est√° disponible.\"\n",
        "\n",
        "        # Prepara un resumen de los datos para d√°rselo como contexto al LLM.\n",
        "        data_sample_str = data.head(15).to_string(index=False)\n",
        "        if len(data) > 15:\n",
        "            data_sample_str += f\"\\n... (mostrando 15 de {len(data)} registros)\"\n",
        "\n",
        "        prompt = f\"\"\"Analiza estos resultados y responde la pregunta del usuario de forma clara.\n",
        "\n",
        "PREGUNTA ORIGINAL: {query}\n",
        "DATOS OBTENIDOS (procesando {len(data)} registros):\n",
        "{data_sample_str}\n",
        "\n",
        "INSTRUCCIONES:\n",
        "- Responde en espa√±ol, bas√°ndote en los datos.\n",
        "- Da n√∫meros exactos y conclusiones directas.\n",
        "- No repitas la pregunta.\n",
        "\n",
        "RESPUESTA:\"\"\"\n",
        "        try:\n",
        "            # Llama a la API del LLM para generar la respuesta.\n",
        "            response = client.chat.completions.create(model=GROQ_MODEL, messages=[{\"role\": \"user\", \"content\": prompt}], max_tokens=300, temperature=0.2)\n",
        "            return response.choices[0].message.content.strip()\n",
        "        except Exception as e:\n",
        "            return f\"Error generando respuesta: {e}\"\n",
        "\n",
        "    def create_chart(self, data):\n",
        "        \"\"\"Crea una gr√°fica de Plotly autom√°ticamente basada en los tipos de datos.\"\"\"\n",
        "        if data.empty:\n",
        "            return None\n",
        "\n",
        "        # Usa una muestra si los datos son muy grandes para graficar.\n",
        "        chart_data = data.sample(MAX_ROWS_FOR_ANALYSIS) if len(data) > MAX_ROWS_FOR_ANALYSIS else data\n",
        "\n",
        "        try:\n",
        "            numeric_cols = chart_data.select_dtypes(include=np.number).columns.tolist()\n",
        "            categorical_cols = chart_data.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "            fig = None\n",
        "\n",
        "            # L√≥gica para decidir qu√© tipo de gr√°fica es m√°s apropiada.\n",
        "            if len(numeric_cols) >= 1 and len(categorical_cols) >= 1:\n",
        "                grouped_data = data.groupby(categorical_cols[0])[numeric_cols[0]].sum().nlargest(20).reset_index()\n",
        "                fig = px.bar(grouped_data, x=categorical_cols[0], y=numeric_cols[0], title=f\"{numeric_cols[0]} por {categorical_cols[0]} (Top 20)\")\n",
        "            elif len(numeric_cols) >= 2:\n",
        "                fig = px.scatter(chart_data, x=numeric_cols[0], y=numeric_cols[1], title=f\"{numeric_cols[1]} vs {numeric_cols[0]}\")\n",
        "            elif len(numeric_cols) == 1:\n",
        "                fig = px.histogram(chart_data, x=numeric_cols[0], title=f\"Distribuci√≥n de {numeric_cols[0]}\")\n",
        "            elif len(categorical_cols) >= 1:\n",
        "                counts = data[categorical_cols[0]].value_counts().nlargest(20)\n",
        "                fig = px.bar(x=counts.index, y=counts.values, title=f\"Frecuencia de {categorical_cols[0]} (Top 20)\")\n",
        "\n",
        "            if fig:\n",
        "                fig.update_layout(height=400, template=\"plotly_white\")\n",
        "                return fig\n",
        "        except Exception as e:\n",
        "            print(f\"Error creando gr√°fica: {e}\")\n",
        "        return None\n",
        "\n",
        "    def should_create_chart(self, query):\n",
        "        \"\"\"Determina si la consulta del usuario sugiere la creaci√≥n de una gr√°fica.\"\"\"\n",
        "        chart_keywords = ['gr√°fica', 'grafica', 'gr√°fico', 'grafico', 'chart', 'plot', 'visualiza', 'muestra', 'distribuci√≥n', 'tendencia']\n",
        "        return any(keyword in query.lower() for keyword in chart_keywords)\n",
        "\n",
        "# --- 5. Funciones de la Interfaz Gradio ---\n",
        "# Estas funciones conectan la l√≥gica del backend (DatabaseRAG) con los componentes de la UI.\n",
        "\n",
        "def process_file_upload(file, rag_state):\n",
        "    \"\"\"Maneja el evento de carga de un archivo.\"\"\"\n",
        "    if file is None:\n",
        "        return \"Por favor, sube un archivo.\", gr.update(interactive=False), rag_state\n",
        "\n",
        "    # Crea una nueva instancia del motor RAG para esta sesi√≥n.\n",
        "    rag_state = DatabaseRAG()\n",
        "    success, message = rag_state.load_file(file.name)\n",
        "\n",
        "    if success:\n",
        "        return message + \"\\n\\n\" + rag_state.schema_description, gr.update(interactive=True), rag_state\n",
        "    else:\n",
        "        return message, gr.update(interactive=False), rag_state\n",
        "\n",
        "def process_user_query(message, history, rag_state):\n",
        "    \"\"\"\n",
        "    Orquesta el flujo completo: SQL -> Ejecuci√≥n -> Respuesta -> Gr√°fica.\n",
        "    NOTA: Esta funci√≥n ya no limpia el cuadro de texto. Eso se hace en un evento .then() separado.\n",
        "    \"\"\"\n",
        "    if not rag_state or not rag_state.conn:\n",
        "        history.append([message, \"Por favor, primero sube un archivo de datos.\"])\n",
        "        return history, None, gr.update(visible=False)\n",
        "\n",
        "    try:\n",
        "        # Ejecuta el pipeline completo.\n",
        "        sql_query = rag_state.generate_sql(message)\n",
        "        sql_executed, data = rag_state.execute_query(sql_query)\n",
        "        text_response = rag_state.generate_answer(message, sql_executed, data)\n",
        "        history.append([message, text_response])\n",
        "\n",
        "        # Crea una gr√°fica si es necesario.\n",
        "        chart = rag_state.create_chart(data) if rag_state.should_create_chart(message) else None\n",
        "\n",
        "        # Devuelve 3 salidas: historial del chat, la figura del gr√°fico y la actualizaci√≥n de visibilidad del gr√°fico.\n",
        "        return history, chart, gr.update(visible=chart is not None)\n",
        "    except Exception as e:\n",
        "        history.append([message, f\"Ocurri√≥ un error: {e}\"])\n",
        "        return history, None, gr.update(visible=False)\n",
        "\n",
        "# --- 6. Creaci√≥n de la Interfaz de Usuario ---\n",
        "def create_interface():\n",
        "    \"\"\"Define y organiza todos los componentes visuales de la aplicaci√≥n Gradio.\"\"\"\n",
        "    with gr.Blocks(title=\"RAG Database Chatbot\", theme=gr.themes.Soft()) as app:\n",
        "        gr.Markdown(\"# ü§ñ RAG Database Chatbot\")\n",
        "        gr.Markdown(\"Sube tu archivo (CSV, XLSX, JSON) y haz preguntas en lenguaje natural.\")\n",
        "\n",
        "        # Almacena el estado (la instancia de DatabaseRAG) para cada sesi√≥n de usuario.\n",
        "        rag_state = gr.State(DatabaseRAG())\n",
        "\n",
        "        # Define la disposici√≥n de los componentes (columnas, filas, etc.).\n",
        "        with gr.Row():\n",
        "            with gr.Column(scale=1):\n",
        "                file_input = gr.File(label=\"üìÅ Cargar Archivo\", file_types=[\".csv\", \".xlsx\", \".xls\", \".json\"])\n",
        "                file_status = gr.Textbox(label=\"Estado del Archivo\", interactive=False, max_lines=10)\n",
        "\n",
        "            with gr.Column(scale=2):\n",
        "                chatbot = gr.Chatbot(label=\"Conversaci√≥n\", height=400, show_copy_button=True, avatar_images=(\"user.png\", \"bot.png\"))\n",
        "                msg_input = gr.Textbox(label=\"Tu pregunta\", placeholder=\"Ej: ¬øCu√°les son los 5 productos m√°s vendidos?\", interactive=False)\n",
        "                with gr.Row():\n",
        "                    send_btn = gr.Button(\"Enviar\", variant=\"primary\")\n",
        "                    clear_btn = gr.Button(\"Limpiar\")\n",
        "\n",
        "        chart_output = gr.Plot(label=\"üìä Gr√°fica Autom√°tica\", visible=False)\n",
        "\n",
        "        # --- 7. Conexi√≥n de Eventos ---\n",
        "        # Asocia las funciones a las acciones del usuario (clics, subidas, etc.).\n",
        "\n",
        "        file_input.change(fn=process_file_upload, inputs=[file_input, rag_state], outputs=[file_status, msg_input, rag_state])\n",
        "\n",
        "        # Define los disparadores para enviar una pregunta (Enter en el textbox o clic en el bot√≥n).\n",
        "        submit_triggers = [msg_input.submit, send_btn.click]\n",
        "\n",
        "        for trigger in submit_triggers:\n",
        "            # 1. Ejecuta la l√≥gica principal. La salida de esta funci√≥n va al chatbot y a la gr√°fica.\n",
        "            main_process = trigger(\n",
        "                fn=process_user_query,\n",
        "                inputs=[msg_input, chatbot, rag_state],\n",
        "                outputs=[chatbot, chart_output, chart_output] # Salidas: chatbot, datos del gr√°fico, visibilidad del gr√°fico\n",
        "            )\n",
        "            # 2. Encadena una acci√≥n para limpiar el cuadro de texto.\n",
        "            main_process.then(fn=lambda: \"\", inputs=None, outputs=[msg_input], queue=False)\n",
        "\n",
        "\n",
        "        def clear_chat():\n",
        "            # Limpia todos los componentes relevantes de la interfaz.\n",
        "            return [], \"\", None, gr.update(visible=False)\n",
        "\n",
        "        clear_btn.click(\n",
        "            fn=clear_chat,\n",
        "            outputs=[chatbot, msg_input, chart_output, chart_output]\n",
        "        )\n",
        "\n",
        "    return app\n",
        "\n",
        "# --- 8. Lanzamiento de la Aplicaci√≥n ---\n",
        "if __name__ == \"__main__\":\n",
        "    # Crea la interfaz y la lanza.\n",
        "    app = create_interface()\n",
        "    app.launch(share=True, show_error=True, debug=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyPfh10/8Tm7hA8tOhyhew7Q",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}