{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMnyQ0aBSSaxjo3dqBYO/YN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CamiloVga/Curso-IA-Para-Ciencia-de-Datos/blob/main/Script_Sesion_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# IA para la Ciencia de Datos\n",
        "## Universidad de los Andes\n",
        "\n",
        "**Profesor:** Camilo Vega - AI/ML Engineer  \n",
        "**LinkedIn:** https://www.linkedin.com/in/camilo-vega-169084b1/\n",
        "\n",
        "---\n",
        "\n",
        "## Gu√≠a: GenBI - Interfaces Conversacionales de Datos con LLM\n",
        "\n",
        "Este notebook presenta **2 implementaciones pr√°cticas**:\n",
        "\n",
        "1. **RAG con Gradio** - Chat de documentos con interfaz simple\n",
        "2. **Text-to SQL y MatplotLib y Seaborn** - Consultas naturales a bases de datos\n",
        "\n",
        "### Requisitos\n",
        "- **APIs:** Groq API token\n",
        "- **GPU:** Opcional para modelos locales\n",
        "- **Datos:** Documentos PDF/DOCX/TXT/CSV para subir\n",
        "\n",
        "## Configuraci√≥n APIs\n",
        "- **Groq API:**\n",
        "  1. [Crear token](https://console.groq.com/keys)\n",
        "  2. En Colab: üîë Secrets ‚Üí Agregar `GROQ_KEY` ‚Üí Pegar tu token\n",
        "\n",
        "Cada secci√≥n es **independiente** y puede ejecutarse por separado"
      ],
      "metadata": {
        "id": "wizyzEVJPPix"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O5FKCZKxPPHH"
      },
      "outputs": [],
      "source": [
        "\n",
        "#=============================================================================\n",
        "# 1. RAG CON GRADIO - CHAT DE DOCUMENTOS\n",
        "#=============================================================================\n",
        "\n",
        "# Instalaciones necesarias\n",
        "!pip install gradio groq scikit-learn PyPDF2 python-docx pandas openpyxl -q\n",
        "\n",
        "import os\n",
        "import gradio as gr\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from groq import Groq\n",
        "from google.colab import userdata\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from PyPDF2 import PdfReader\n",
        "from docx import Document\n",
        "\n",
        "# Configuraci√≥n global\n",
        "GROQ_API_KEY = userdata.get('GROQ_KEY')\n",
        "client = Groq(api_key=GROQ_API_KEY)\n",
        "\n",
        "# Variables globales del sistema RAG\n",
        "vectorizer = TfidfVectorizer(max_features=1000, stop_words=None)\n",
        "chunks = []\n",
        "vectors = None\n",
        "documents_loaded = False\n",
        "\n",
        "def process_uploaded_files(files):\n",
        "    \"\"\"Procesa archivos subidos y extrae texto\"\"\"\n",
        "    if not files:\n",
        "        return \"No se subieron archivos\"\n",
        "\n",
        "    combined_text = \"\"\n",
        "    processed_files = []\n",
        "\n",
        "    for file in files:\n",
        "        try:\n",
        "            file_path = file.name\n",
        "            filename = os.path.basename(file_path)\n",
        "\n",
        "            # Procesamiento seg√∫n tipo de archivo\n",
        "            if filename.endswith('.txt'):\n",
        "                with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                    text = f.read()\n",
        "                    combined_text += text + \"\\n\\n\"\n",
        "\n",
        "            elif filename.endswith('.pdf'):\n",
        "                reader = PdfReader(file_path)\n",
        "                text = \"\"\n",
        "                for page in reader.pages:\n",
        "                    text += page.extract_text()\n",
        "                combined_text += text + \"\\n\\n\"\n",
        "\n",
        "            elif filename.endswith('.docx'):\n",
        "                doc = Document(file_path)\n",
        "                text = \"\"\n",
        "                for paragraph in doc.paragraphs:\n",
        "                    text += paragraph.text + \"\\n\"\n",
        "                combined_text += text + \"\\n\\n\"\n",
        "\n",
        "            elif filename.endswith('.csv'):\n",
        "                df = pd.read_csv(file_path)\n",
        "                combined_text += df.to_string() + \"\\n\\n\"\n",
        "\n",
        "            elif filename.endswith('.xlsx'):\n",
        "                df = pd.read_excel(file_path)\n",
        "                combined_text += df.to_string() + \"\\n\\n\"\n",
        "\n",
        "            processed_files.append(filename)\n",
        "\n",
        "        except Exception as e:\n",
        "            continue\n",
        "\n",
        "    if combined_text.strip():\n",
        "        # Crear chunks para b√∫squeda\n",
        "        create_knowledge_base(combined_text)\n",
        "        return f\"Procesados: {', '.join(processed_files)}\\nDocumentos listos para consultas.\"\n",
        "    else:\n",
        "        return \"Error procesando archivos\"\n",
        "\n",
        "def create_knowledge_base(text):\n",
        "    \"\"\"Crea la base de conocimiento vectorial\"\"\"\n",
        "    global chunks, vectors, vectorizer, documents_loaded\n",
        "\n",
        "    # Dividir texto en chunks\n",
        "    words = text.split()\n",
        "    chunks = []\n",
        "\n",
        "    chunk_size = 500\n",
        "    for i in range(0, len(words), chunk_size):\n",
        "        chunk = ' '.join(words[i:i + chunk_size])\n",
        "        if len(chunk.strip()) > 100:\n",
        "            chunks.append(chunk.strip())\n",
        "\n",
        "    # Crear vectores TF-IDF\n",
        "    if chunks:\n",
        "        vectors = vectorizer.fit_transform(chunks)\n",
        "        documents_loaded = True\n",
        "\n",
        "def search_documents(query, top_k=3):\n",
        "    \"\"\"Busca fragmentos relevantes en los documentos\"\"\"\n",
        "    if not documents_loaded or not chunks:\n",
        "        return []\n",
        "\n",
        "    query_vector = vectorizer.transform([query])\n",
        "    similarities = cosine_similarity(query_vector, vectors)[0]\n",
        "\n",
        "    # Obtener fragmentos m√°s relevantes\n",
        "    top_indices = np.argsort(similarities)[-top_k:][::-1]\n",
        "\n",
        "    relevant_chunks = []\n",
        "    for idx in top_indices:\n",
        "        if similarities[idx] > 0.1:\n",
        "            relevant_chunks.append(chunks[idx])\n",
        "\n",
        "    return relevant_chunks\n",
        "\n",
        "def chat_with_documents(message, history):\n",
        "    \"\"\"Funci√≥n principal del chat\"\"\"\n",
        "    if not documents_loaded:\n",
        "        return \"Primero sube documentos para poder consultar\"\n",
        "\n",
        "    # Buscar contexto relevante\n",
        "    context_chunks = search_documents(message)\n",
        "\n",
        "    if not context_chunks:\n",
        "        return \"No encontr√© informaci√≥n relevante sobre tu consulta\"\n",
        "\n",
        "    # Crear contexto para el LLM\n",
        "    context = \"\\n\\n\".join(context_chunks)\n",
        "\n",
        "    prompt = f\"\"\"Bas√°ndote √∫nicamente en esta informaci√≥n:\n",
        "\n",
        "{context}\n",
        "\n",
        "Pregunta del usuario: {message}\n",
        "\n",
        "Instrucciones:\n",
        "- Responde solo con informaci√≥n del contexto proporcionado\n",
        "- Si no hay informaci√≥n suficiente, menciona que no puedes responder con los documentos disponibles\n",
        "- S√© claro y conciso\n",
        "- Responde en espa√±ol\"\"\"\n",
        "\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"openai/gpt-oss-20b\",  # Modelo\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            max_tokens=500,\n",
        "            temperature=0.3\n",
        "        )\n",
        "        return response.choices[0].message.content\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error al procesar la consulta: {str(e)}\"\n",
        "\n",
        "# Crear interfaz Gradio\n",
        "with gr.Blocks(title=\"Chat de Documentos\", theme=gr.themes.Soft()) as demo:\n",
        "\n",
        "    gr.Markdown(\"# Chat de Documentos con RAG\")\n",
        "    gr.Markdown(\"Sube documentos y haz preguntas sobre su contenido\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=1):\n",
        "            # Panel de carga de archivos\n",
        "            file_upload = gr.File(\n",
        "                label=\"Subir Documentos\",\n",
        "                file_count=\"multiple\",\n",
        "                file_types=[\".pdf\", \".docx\", \".txt\", \".csv\", \".xlsx\"]\n",
        "            )\n",
        "\n",
        "            upload_status = gr.Textbox(\n",
        "                label=\"Estado\",\n",
        "                value=\"Esperando documentos...\",\n",
        "                interactive=False\n",
        "            )\n",
        "\n",
        "        with gr.Column(scale=2):\n",
        "            # Interfaz de chat\n",
        "            chatbot = gr.ChatInterface(\n",
        "                fn=chat_with_documents,\n",
        "                title=\"Asistente de Documentos\",\n",
        "                description=\"Haz preguntas sobre los documentos subidos\",\n",
        "                examples=[\n",
        "                    \"¬øDe qu√© tratan estos documentos?\",\n",
        "                    \"Resume los puntos principales\",\n",
        "                    \"¬øQu√© conclusiones se mencionan?\"\n",
        "                ]\n",
        "            )\n",
        "\n",
        "    # Conectar eventos\n",
        "    file_upload.change(\n",
        "        fn=process_uploaded_files,\n",
        "        inputs=[file_upload],\n",
        "        outputs=[upload_status]\n",
        "    )\n",
        "\n",
        "# Lanzar la aplicaci√≥n\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch(share=True, debug=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# RAG Database Chatbot with Gradio\n",
        "# Sistema de consultas inteligentes con interfaz de chat y gr√°ficas autom√°ticas\n",
        "\n",
        "# Instalaci√≥n de dependencias\n",
        "!pip install groq sentence-transformers pandas numpy scikit-learn gradio matplotlib seaborn plotly openpyxl -q\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sqlite3\n",
        "import json\n",
        "import io\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from groq import Groq\n",
        "import gradio as gr\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "from google.colab import userdata\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configuraci√≥n global con secretos de Colab\n",
        "GROQ_API_KEY = userdata.get('GROQ_KEY')\n",
        "client = Groq(api_key=GROQ_API_KEY)\n",
        "GROQ_MODEL = \"llama-3.1-8b-instant\"  # Modelo m√°s estable\n",
        "\n",
        "# Variables globales del sistema\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "conn = None\n",
        "table_name = 'data'\n",
        "column_embeddings = {}\n",
        "column_info = {}\n",
        "schema_description = \"\"\n",
        "current_data = None\n",
        "\n",
        "class DatabaseRAG:\n",
        "    def __init__(self):\n",
        "        self.conn = None\n",
        "        self.column_embeddings = {}\n",
        "        self.column_info = {}\n",
        "        self.schema_description = \"\"\n",
        "        self.current_data = None\n",
        "\n",
        "    def load_file(self, file_path):\n",
        "        \"\"\"Carga archivos CSV, XLSX o JSON y crea base de datos en memoria\"\"\"\n",
        "        try:\n",
        "            file_extension = file_path.lower().split('.')[-1]\n",
        "\n",
        "            if file_extension == 'csv':\n",
        "                df = pd.read_csv(file_path)\n",
        "            elif file_extension in ['xlsx', 'xls']:\n",
        "                df = pd.read_excel(file_path)\n",
        "            elif file_extension == 'json':\n",
        "                df = pd.read_json(file_path)\n",
        "            else:\n",
        "                return False, \"Formato no soportado. Use CSV, XLSX o JSON.\"\n",
        "\n",
        "            if df.empty:\n",
        "                return False, \"El archivo est√° vac√≠o.\"\n",
        "\n",
        "            # Crear conexi√≥n SQLite en memoria\n",
        "            if self.conn:\n",
        "                self.conn.close()\n",
        "            self.conn = sqlite3.connect(':memory:', check_same_thread=False)\n",
        "            df.to_sql(table_name, self.conn, index=False, if_exists='replace')\n",
        "\n",
        "            # Guardar datos actuales para gr√°ficas\n",
        "            self.current_data = df\n",
        "\n",
        "            # Generar embeddings y descripci√≥n del esquema\n",
        "            self._generate_schema(df)\n",
        "\n",
        "            return True, f\"‚úÖ Archivo cargado: {df.shape[0]} filas, {df.shape[1]} columnas\"\n",
        "\n",
        "        except Exception as e:\n",
        "            return False, f\"Error cargando archivo: {str(e)}\"\n",
        "\n",
        "    def _generate_schema(self, df):\n",
        "        \"\"\"Genera esquema y embeddings de columnas\"\"\"\n",
        "        self.column_embeddings.clear()\n",
        "        self.column_info.clear()\n",
        "        schema_parts = []\n",
        "\n",
        "        for col in df.columns:\n",
        "            # Detectar tipo de columna\n",
        "            if df[col].dtype in ['int64', 'float64']:\n",
        "                col_type = 'NUMERIC'\n",
        "                sample_values = f\"rango: {df[col].min():.2f} - {df[col].max():.2f}\"\n",
        "            elif df[col].nunique() <= 20:\n",
        "                col_type = 'CATEGORICAL'\n",
        "                unique_vals = list(df[col].unique())\n",
        "                sample_values = f\"valores: {', '.join(map(str, unique_vals[:5]))}\"\n",
        "            else:\n",
        "                col_type = 'TEXT'\n",
        "                sample_values = f\"texto con {df[col].nunique()} valores √∫nicos\"\n",
        "\n",
        "            desc = f\"{col} ({col_type}): {sample_values}\"\n",
        "            self.column_info[col] = {\n",
        "                'type': col_type,\n",
        "                'sample_values': sample_values,\n",
        "                'description': desc\n",
        "            }\n",
        "\n",
        "            # Generar embedding\n",
        "            self.column_embeddings[col] = model.encode([desc])[0]\n",
        "            schema_parts.append(f\"- {col} ({col_type}): {sample_values}\")\n",
        "\n",
        "        self.schema_description = f\"\"\"\n",
        "Tabla: {table_name}\n",
        "Total de registros: {len(df)}\n",
        "Columnas disponibles:\n",
        "\"\"\" + \"\\n\".join(schema_parts)\n",
        "\n",
        "    def find_relevant_columns(self, query, top_k=5):\n",
        "        \"\"\"Encuentra columnas relevantes usando b√∫squeda sem√°ntica\"\"\"\n",
        "        query_emb = model.encode([query])[0]\n",
        "        scores = []\n",
        "        for col, col_emb in self.column_embeddings.items():\n",
        "            sim = cosine_similarity([query_emb], [col_emb])[0][0]\n",
        "            scores.append((col, sim, self.column_info[col]))\n",
        "        return sorted(scores, key=lambda x: x[1], reverse=True)[:top_k]\n",
        "\n",
        "    def generate_sql(self, query):\n",
        "        \"\"\"Genera consulta SQL usando LLM - ID√âNTICO AL SCRIPT ORIGINAL\"\"\"\n",
        "        if not self.conn:\n",
        "            return \"SELECT 1\"  # Consulta dummy si no hay datos\n",
        "\n",
        "        relevant_cols = self.find_relevant_columns(query)\n",
        "\n",
        "        # Construir contexto de columnas relevantes - IGUAL AL ORIGINAL\n",
        "        cols_context = \"\\nColumnas m√°s relevantes para la consulta:\\n\"\n",
        "        for col, score, info in relevant_cols:\n",
        "            cols_context += f\"- {col} ({info['type']}): {info['sample_values']}\\n\"\n",
        "\n",
        "        prompt = f\"\"\"Eres un experto en SQL. Genera una consulta SQL para responder la pregunta del usuario.\n",
        "\n",
        "ESQUEMA DE LA BASE DE DATOS:\n",
        "{self.schema_description}\n",
        "\n",
        "{cols_context}\n",
        "\n",
        "PREGUNTA DEL USUARIO: {query}\n",
        "\n",
        "INSTRUCCIONES:\n",
        "1. Usa el nombre de tabla: {table_name}\n",
        "2. NO uses LIMIT - necesitamos todos los datos\n",
        "3. Si necesitas agrupar, usa GROUP BY apropiadamente\n",
        "4. Si necesitas ordenar, usa ORDER BY\n",
        "5. Para consultas num√©ricas usa SUM, AVG, COUNT, etc.\n",
        "6. Responde SOLO con la consulta SQL, sin explicaciones\n",
        "\n",
        "CONSULTA SQL:\"\"\"\n",
        "\n",
        "        try:\n",
        "            response = client.chat.completions.create(\n",
        "                model=GROQ_MODEL,\n",
        "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                max_tokens=200,\n",
        "                temperature=0.1\n",
        "            )\n",
        "            sql_generated = response.choices[0].message.content.strip()\n",
        "\n",
        "            # Limpiar la respuesta - IGUAL AL ORIGINAL\n",
        "            sql_generated = sql_generated.replace('```sql', '').replace('```', '').strip()\n",
        "\n",
        "            return sql_generated\n",
        "        except Exception as e:\n",
        "            print(f\"Error generando SQL: {e}\")\n",
        "            # Fallback simple - SIN LIMIT\n",
        "            relevant_col_names = [col for col, _, _ in relevant_cols[:3]]\n",
        "            return f\"SELECT {', '.join(relevant_col_names)} FROM {table_name}\"\n",
        "\n",
        "    def execute_query(self, sql):\n",
        "        \"\"\"Ejecuta la consulta SQL en la base de datos - ID√âNTICO AL ORIGINAL\"\"\"\n",
        "        if not self.conn:\n",
        "            return \"Error: Base de datos no inicializada\", pd.DataFrame()\n",
        "\n",
        "        try:\n",
        "            print(f\"Ejecutando SQL: {sql}\")\n",
        "            resultado = pd.read_sql_query(sql, self.conn)\n",
        "            return sql, resultado\n",
        "        except Exception as e:\n",
        "            print(f\"Error SQL: {e}\")\n",
        "            # Fallback: mostrar todas las columnas relevantes - SIN LIMIT\n",
        "            try:\n",
        "                fallback_sql = f\"SELECT * FROM {table_name}\"\n",
        "                resultado = pd.read_sql_query(fallback_sql, self.conn)\n",
        "                return fallback_sql, resultado\n",
        "            except Exception as e2:\n",
        "                print(f\"Error en consulta fallback: {e2}\")\n",
        "                return \"Error\", pd.DataFrame()\n",
        "\n",
        "    def should_create_chart(self, query):\n",
        "        \"\"\"Determina si se debe crear una gr√°fica basado en la consulta\"\"\"\n",
        "        chart_keywords = [\n",
        "            'gr√°fica', 'grafica', 'gr√°fico', 'grafico', 'chart', 'plot',\n",
        "            'visualiza', 'muestra', 'dibuja', 'representa',\n",
        "            'comparar', 'comparaci√≥n', 'distribuci√≥n', 'tendencia',\n",
        "            'ranking', 'top', 'mayor', 'menor', 'evoluci√≥n'\n",
        "        ]\n",
        "        query_lower = query.lower()\n",
        "        return any(keyword in query_lower for keyword in chart_keywords)\n",
        "\n",
        "    def create_chart(self, data, query):\n",
        "        \"\"\"Crea gr√°fica autom√°tica basada en los datos - SIN LIMITACIONES DE REGISTROS\"\"\"\n",
        "        if data.empty:\n",
        "            return None\n",
        "\n",
        "        try:\n",
        "            # Detectar tipos de columnas\n",
        "            numeric_cols = data.select_dtypes(include=[np.number]).columns.tolist()\n",
        "            categorical_cols = data.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "            fig = None\n",
        "\n",
        "            # L√≥gica de gr√°ficas autom√°ticas - PROCESANDO TODOS LOS DATOS\n",
        "            if len(numeric_cols) >= 1 and len(categorical_cols) >= 1:\n",
        "                # Gr√°fica de barras: categ√≥rica vs num√©rica\n",
        "                cat_col = categorical_cols[0]\n",
        "                num_col = numeric_cols[0]\n",
        "\n",
        "                # Procesar TODOS los datos, solo limitar visualizaci√≥n si hay demasiados valores √∫nicos\n",
        "                chart_data = data.groupby(cat_col)[num_col].sum().reset_index()\n",
        "\n",
        "                # Solo mostrar top 20 para legibilidad, pero calculado sobre TODOS los datos\n",
        "                if chart_data.shape[0] > 20:\n",
        "                    chart_data = chart_data.nlargest(20, num_col)\n",
        "\n",
        "                fig = px.bar(\n",
        "                    chart_data,\n",
        "                    x=cat_col,\n",
        "                    y=num_col,\n",
        "                    title=f\"{num_col} por {cat_col} (Top 20 de {data[cat_col].nunique()} categor√≠as)\",\n",
        "                    template=\"plotly_white\"\n",
        "                )\n",
        "                fig.update_xaxes(tickangle=45)\n",
        "\n",
        "            elif len(numeric_cols) >= 2:\n",
        "                # Scatter plot para dos variables num√©ricas - TODOS LOS DATOS\n",
        "                fig = px.scatter(\n",
        "                    data,\n",
        "                    x=numeric_cols[0],\n",
        "                    y=numeric_cols[1],\n",
        "                    title=f\"{numeric_cols[1]} vs {numeric_cols[0]} ({len(data)} registros)\",\n",
        "                    template=\"plotly_white\"\n",
        "                )\n",
        "\n",
        "            elif len(numeric_cols) == 1:\n",
        "                # Histograma para una variable num√©rica - TODOS LOS DATOS\n",
        "                fig = px.histogram(\n",
        "                    data,\n",
        "                    x=numeric_cols[0],\n",
        "                    title=f\"Distribuci√≥n de {numeric_cols[0]} ({len(data)} registros)\",\n",
        "                    template=\"plotly_white\"\n",
        "                )\n",
        "\n",
        "            elif len(categorical_cols) >= 1:\n",
        "                # Gr√°fica de conteo para categ√≥ricas - TODOS LOS DATOS\n",
        "                cat_col = categorical_cols[0]\n",
        "                value_counts = data[cat_col].value_counts()\n",
        "\n",
        "                # Solo mostrar top 20 para legibilidad visual\n",
        "                if len(value_counts) > 20:\n",
        "                    display_counts = value_counts.head(20)\n",
        "                    title_suffix = f\" (Top 20 de {len(value_counts)} categor√≠as)\"\n",
        "                else:\n",
        "                    display_counts = value_counts\n",
        "                    title_suffix = f\" ({len(value_counts)} categor√≠as total)\"\n",
        "\n",
        "                fig = px.bar(\n",
        "                    x=display_counts.index,\n",
        "                    y=display_counts.values,\n",
        "                    title=f\"Frecuencia de {cat_col}{title_suffix}\",\n",
        "                    template=\"plotly_white\"\n",
        "                )\n",
        "                fig.update_xaxes(tickangle=45)\n",
        "\n",
        "            if fig:\n",
        "                fig.update_layout(height=400, showlegend=False)\n",
        "                return fig\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error creando gr√°fica: {e}\")\n",
        "\n",
        "        return None\n",
        "\n",
        "    def generate_answer(self, query, sql_executed, data):\n",
        "        \"\"\"Genera respuesta natural usando LLM con an√°lisis estad√≠stico - ID√âNTICO AL ORIGINAL\"\"\"\n",
        "        if data.empty:\n",
        "            return \"No se encontraron datos para esta consulta.\"\n",
        "\n",
        "        # Preparar estad√≠sticas autom√°ticas - IGUAL AL ORIGINAL\n",
        "        stats_summary = f\"Se procesaron {len(data)} registros totales.\\n\"\n",
        "\n",
        "        # Agregar estad√≠sticas de columnas num√©ricas\n",
        "        numeric_cols = data.select_dtypes(include=[np.number]).columns\n",
        "        if len(numeric_cols) > 0:\n",
        "            stats_summary += \"\\nEstad√≠sticas num√©ricas:\\n\"\n",
        "            for col in numeric_cols[:3]:\n",
        "                stats_summary += f\"- {col}: total={data[col].sum():.2f}, promedio={data[col].mean():.2f}\\n\"\n",
        "\n",
        "        # Preparar muestra de datos - IGUAL AL ORIGINAL\n",
        "        data_sample = data.head(15).to_string(index=False) if len(data) > 15 else data.to_string(index=False)\n",
        "        if len(data) > 15:\n",
        "            data_sample += f\"\\n... (mostrando 15 de {len(data)} registros)\"\n",
        "\n",
        "        prompt = f\"\"\"Analiza estos resultados de base de datos y responde la pregunta del usuario de forma clara y profesional.\n",
        "\n",
        "PREGUNTA ORIGINAL: {query}\n",
        "SQL EJECUTADO: {sql_executed}\n",
        "\n",
        "ESTAD√çSTICAS:\n",
        "{stats_summary}\n",
        "\n",
        "DATOS OBTENIDOS:\n",
        "{data_sample}\n",
        "\n",
        "INSTRUCCIONES:\n",
        "- Responde bas√°ndote en los {len(data)} registros procesados\n",
        "- Da n√∫meros exactos y estad√≠sticas precisas\n",
        "- Responde en espa√±ol de forma profesional y directa\n",
        "- Si hay rankings, menciona los elementos m√°s importantes\n",
        "- No repitas el SQL ni la pregunta\n",
        "\n",
        "RESPUESTA:\"\"\"\n",
        "\n",
        "        try:\n",
        "            response = client.chat.completions.create(\n",
        "                model=GROQ_MODEL,\n",
        "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                max_tokens=300,\n",
        "                temperature=0.2\n",
        "            )\n",
        "            return response.choices[0].message.content.strip()\n",
        "        except Exception as e:\n",
        "            return f\"Error generando respuesta: {e}\"\n",
        "\n",
        "# Instancia global del sistema RAG\n",
        "rag_system = DatabaseRAG()\n",
        "\n",
        "def process_file_upload(file):\n",
        "    \"\"\"Procesa la carga de archivos\"\"\"\n",
        "    if file is None:\n",
        "        return \"Por favor, sube un archivo CSV, XLSX o JSON.\", None\n",
        "\n",
        "    success, message = rag_system.load_file(file.name)\n",
        "\n",
        "    if success:\n",
        "        # Mostrar esquema como bienvenida\n",
        "        return message + \"\\n\\n\" + rag_system.schema_description, gr.update(interactive=True)\n",
        "    else:\n",
        "        return message, gr.update(interactive=False)\n",
        "\n",
        "def chat_response(message, history):\n",
        "    \"\"\"Procesa mensajes del chat y genera respuestas\"\"\"\n",
        "    if not rag_system.conn:\n",
        "        return history + [[message, \"Por favor, primero sube un archivo de datos.\"]]\n",
        "\n",
        "    if not message.strip():\n",
        "        return history + [[message, \"Por favor, escribe una pregunta.\"]]\n",
        "\n",
        "    try:\n",
        "        # Pipeline RAG completo\n",
        "        sql = rag_system.generate_sql(message)\n",
        "        sql_executed, data = rag_system.execute_query(sql)\n",
        "        respuesta = rag_system.generate_answer(message, sql_executed, data)\n",
        "\n",
        "        # Formatear respuesta\n",
        "        full_response = respuesta\n",
        "\n",
        "        # Actualizar historial\n",
        "        new_history = history + [[message, full_response]]\n",
        "\n",
        "        return new_history\n",
        "\n",
        "    except Exception as e:\n",
        "        error_msg = f\"Error procesando consulta: {str(e)}\"\n",
        "        return history + [[message, error_msg]]\n",
        "\n",
        "def get_chart_for_last_query(message):\n",
        "    \"\"\"Genera gr√°fica para la consulta actual\"\"\"\n",
        "    if not rag_system.conn or not message.strip():\n",
        "        return None\n",
        "\n",
        "    if rag_system.should_create_chart(message):\n",
        "        try:\n",
        "            sql = rag_system.generate_sql(message)\n",
        "            _, data = rag_system.execute_query(sql)\n",
        "            return rag_system.create_chart(data, message)\n",
        "        except Exception as e:\n",
        "            print(f\"Error generando gr√°fica: {e}\")\n",
        "            return None\n",
        "\n",
        "    return None\n",
        "\n",
        "# Interfaz Gradio\n",
        "def create_interface():\n",
        "    with gr.Blocks(title=\"RAG Database Chatbot\", theme=gr.themes.Soft()) as app:\n",
        "        gr.Markdown(\"# ü§ñ RAG Database Chatbot\")\n",
        "        gr.Markdown(\"Sube tu archivo de datos (CSV, XLSX, JSON) y haz preguntas en lenguaje natural.\")\n",
        "\n",
        "        with gr.Row():\n",
        "            with gr.Column(scale=1):\n",
        "                file_input = gr.File(\n",
        "                    label=\"üìÅ Cargar Archivo\",\n",
        "                    file_types=[\".csv\", \".xlsx\", \".xls\", \".json\"]\n",
        "                )\n",
        "                file_status = gr.Textbox(\n",
        "                    label=\"Estado del Archivo\",\n",
        "                    interactive=False,\n",
        "                    max_lines=10\n",
        "                )\n",
        "\n",
        "            with gr.Column(scale=2):\n",
        "                chatbot = gr.Chatbot(\n",
        "                    label=\"Conversaci√≥n\",\n",
        "                    height=400,\n",
        "                    show_copy_button=True\n",
        "                )\n",
        "\n",
        "                msg_input = gr.Textbox(\n",
        "                    label=\"Tu pregunta\",\n",
        "                    placeholder=\"Ejemplo: ¬øCu√°les son las ventas por regi√≥n?\",\n",
        "                    interactive=False\n",
        "                )\n",
        "\n",
        "                with gr.Row():\n",
        "                    send_btn = gr.Button(\"Enviar\", variant=\"primary\")\n",
        "                    clear_btn = gr.Button(\"Limpiar Chat\")\n",
        "\n",
        "        # √Årea para gr√°ficas\n",
        "        chart_output = gr.Plot(label=\"üìä Gr√°fica Autom√°tica\", visible=False)\n",
        "\n",
        "        # Event handlers - VERSI√ìN MEJORADA PARA EVITAR BLOQUEOS\n",
        "        file_input.change(\n",
        "            fn=process_file_upload,\n",
        "            inputs=[file_input],\n",
        "            outputs=[file_status, msg_input],\n",
        "            queue=False\n",
        "        )\n",
        "\n",
        "        def submit_message(message, history):\n",
        "            \"\"\"Maneja el env√≠o de mensajes sin bloqueos\"\"\"\n",
        "            if not message.strip():\n",
        "                return history, message, None, gr.update(visible=False)\n",
        "\n",
        "            # Procesar respuesta del chat\n",
        "            new_history = chat_response(message, history)\n",
        "\n",
        "            # Generar gr√°fica si es necesario\n",
        "            chart = get_chart_for_last_query(message)\n",
        "            chart_visible = chart is not None\n",
        "\n",
        "            # Limpiar input y mostrar resultados\n",
        "            return new_history, \"\", chart, gr.update(visible=chart_visible)\n",
        "\n",
        "        # Conectar eventos con mejor manejo\n",
        "        send_btn.click(\n",
        "            fn=submit_message,\n",
        "            inputs=[msg_input, chatbot],\n",
        "            outputs=[chatbot, msg_input, chart_output, chart_output],\n",
        "            queue=True\n",
        "        )\n",
        "\n",
        "        msg_input.submit(\n",
        "            fn=submit_message,\n",
        "            inputs=[msg_input, chatbot],\n",
        "            outputs=[chatbot, msg_input, chart_output, chart_output],\n",
        "            queue=True\n",
        "        )\n",
        "\n",
        "        def clear_chat():\n",
        "            \"\"\"Limpia el chat y oculta gr√°ficas\"\"\"\n",
        "            return [], None, gr.update(visible=False)\n",
        "\n",
        "        clear_btn.click(\n",
        "            fn=clear_chat,\n",
        "            outputs=[chatbot, chart_output, chart_output],\n",
        "            queue=False\n",
        "        )\n",
        "\n",
        "        # Ejemplos\n",
        "        gr.Examples(\n",
        "            examples=[\n",
        "                \"¬øCu√°les son las 10 ciudades con mayores ventas?\",\n",
        "                \"Muestra la distribuci√≥n de edades\",\n",
        "                \"¬øCu√°l es el promedio de ingresos por departamento?\",\n",
        "                \"Grafica las ventas por mes\",\n",
        "                \"¬øQu√© productos tienen mejor rendimiento?\"\n",
        "            ],\n",
        "            inputs=msg_input\n",
        "        )\n",
        "\n",
        "    return app\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Crear y lanzar la aplicaci√≥n\n",
        "    app = create_interface()\n",
        "    app.launch(\n",
        "        share=True,\n",
        "        show_error=True\n",
        "    )"
      ],
      "metadata": {
        "id": "A43YnC2mQsiB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}