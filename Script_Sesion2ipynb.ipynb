{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPZ4E6QG0HHFtkprS+nn9H8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CamiloVga/Curso-IA-Para-Ciencia-de-Datos/blob/main/Script_Sesion2ipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# IA para la Ciencia de Datos\n",
        "## Universidad de los Andes\n",
        "\n",
        "**Profesor:** Camilo Vega - AI/ML Engineer  \n",
        "**LinkedIn:** https://www.linkedin.com/in/camilovegabarbosa/\n",
        "\n",
        "---\n",
        "\n",
        "## Gu√≠a: Ejecutar Modelos de Lenguaje con Llama-2-7b\n",
        "\n",
        "Este notebook presenta **4 m√©todos diferentes** para ejecutar modelos de lenguaje:\n",
        "\n",
        "1. **Transformers Pipeline** - Implementaci√≥n b√°sica\n",
        "2. **Accelerate + BitsAndBytes** - Optimizado para GPU\n",
        "3. **Ollama** - Ejecuci√≥n local simplificada  \n",
        "4. **Groq** - API en la nube\n",
        "\n",
        "### Requisitos\n",
        "- **GPU:** Tesla T4 m√≠nimo (Colab gratuito)\n",
        "- **APIs:** Hugging Face token, Groq API key (opcional)\n",
        "\n",
        "## Personalizaci√≥n\n",
        "\n",
        "```python\n",
        "# Modificar par√°metros en cualquier m√©todo\n",
        "MAX_TOKENS = 1024      # Respuestas m√°s largas\n",
        "TEMPERATURE = 0.9      # M√°s creatividad\n",
        "SYSTEM_PROMPT = \"Tu prompt personalizado\"\n",
        "```\n",
        "\n",
        "### Configuraci√≥n APIs\n",
        "- **Hugging Face:**\n",
        "  1. [Crear token](https://huggingface.co/settings/tokens)\n",
        "  2. En Colab: üîë Secrets ‚Üí Agregar `HF_TOKEN` ‚Üí Pegar tu token\n",
        "- **Groq:**\n",
        "  1. [Registrarse](https://console.groq.com/)\n",
        "  2. En Colab: üîë Secrets ‚Üí Agregar `GROQ_KEY` ‚Üí Pegar tu API key\n",
        "\n",
        "Cada m√©todo es **independiente** y puede ejecutarse por separado."
      ],
      "metadata": {
        "id": "Fp17H7yzPPPI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ciIQHsdhPOnk"
      },
      "outputs": [],
      "source": [
        "# M√âTODO 1: TRANSFORMERS PIPELINE\n",
        "# Implementaci√≥n b√°sica sin optimizaciones\n",
        "\n",
        "# 1. Instalaci√≥n de dependencias\n",
        "!pip install transformers torch -q\n",
        "\n",
        "# 2. Importar bibliotecas\n",
        "from transformers import pipeline\n",
        "import torch\n",
        "from google.colab import userdata\n",
        "\n",
        "# 3. Obtener token de Hugging Face desde secretos de Colab\n",
        "HF_TOKEN = userdata.get('HF_TOKEN')\n",
        "\n",
        "# 4. Par√°metros configurables\n",
        "MAX_TOKENS = 512\n",
        "TEMPERATURE = 0.7\n",
        "SYSTEM_PROMPT = \"Eres un asistente √∫til y educativo. Responde de manera clara y concisa.\"\n",
        "\n",
        "# 5. Cargar modelo usando pipeline con token de secretos\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=\"meta-llama/Llama-2-7b-chat-hf\",\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        "    token=HF_TOKEN  # Token desde secretos de Colab\n",
        ")\n",
        "\n",
        "# 5. Funci√≥n para generar respuestas\n",
        "def generar_respuesta(user_input, system_prompt=SYSTEM_PROMPT, max_tokens=MAX_TOKENS, temperature=TEMPERATURE):\n",
        "    \"\"\"\n",
        "    Genera respuesta usando Transformers Pipeline\n",
        "    \"\"\"\n",
        "    # Crear estructura de mensajes\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": user_input}\n",
        "    ]\n",
        "\n",
        "    # Generar respuesta\n",
        "    result = pipe(\n",
        "        messages,\n",
        "        max_new_tokens=max_tokens,\n",
        "        temperature=temperature,\n",
        "        do_sample=True\n",
        "    )\n",
        "\n",
        "    # Extraer texto de respuesta\n",
        "    response = result[0]['generated_text'][-1]['content']\n",
        "    return response\n",
        "\n",
        "# 6. Ejemplo de uso\n",
        "user_question = \"¬øQu√© es el machine learning?\"\n",
        "respuesta = generar_respuesta(user_question)\n",
        "print(f\"Pregunta: {user_question}\")\n",
        "print(f\"Respuesta: {respuesta}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# M√âTODO 2: ACCELERATE + BITSANDBYTES\n",
        "# Optimizado para GPU con cuantizaci√≥n\n",
        "\n",
        "# 1. Instalaci√≥n de dependencias\n",
        "!pip install transformers torch accelerate bitsandbytes -q\n",
        "\n",
        "# 2. Importar bibliotecas\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import torch\n",
        "from google.colab import userdata\n",
        "\n",
        "# 3. Obtener token de Hugging Face desde secretos de Colab\n",
        "HF_TOKEN = userdata.get('HF_TOKEN')\n",
        "\n",
        "# 4. Par√°metros configurables\n",
        "MAX_TOKENS = 512\n",
        "TEMPERATURE = 0.7\n",
        "SYSTEM_PROMPT = \"Eres un asistente √∫til y educativo. Responde de manera clara y concisa.\"\n",
        "\n",
        "# 4. Configurar cuantizaci√≥n 8-bit para optimizar memoria\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_8bit=True,\n",
        "    bnb_8bit_compute_dtype=torch.float16\n",
        ")\n",
        "\n",
        "# 6. Cargar tokenizador con token de secretos\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    \"meta-llama/Llama-2-7b-chat-hf\",\n",
        "    token=HF_TOKEN\n",
        ")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# 7. Cargar modelo con optimizaciones y token de secretos\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"meta-llama/Llama-2-7b-chat-hf\",\n",
        "    quantization_config=quantization_config,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16,\n",
        "    trust_remote_code=True,\n",
        "    token=HF_TOKEN  # Token desde secretos de Colab\n",
        ")\n",
        "\n",
        "# 7. Funci√≥n para generar respuestas\n",
        "def generar_respuesta_optimizada(user_input, system_prompt=SYSTEM_PROMPT, max_tokens=MAX_TOKENS, temperature=TEMPERATURE):\n",
        "    \"\"\"\n",
        "    Genera respuesta usando modelo optimizado\n",
        "    \"\"\"\n",
        "    # Formato espec√≠fico de Llama-2 para chat\n",
        "    formatted_prompt = f\"<s>[INST] <<SYS>>\\n{system_prompt}\\n<</SYS>>\\n\\n{user_input} [/INST]\"\n",
        "\n",
        "    # Tokenizar entrada\n",
        "    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    # Generar respuesta con optimizaciones\n",
        "    with torch.inference_mode():\n",
        "        outputs = model.generate(\n",
        "            inputs.input_ids,\n",
        "            max_new_tokens=max_tokens,\n",
        "            temperature=temperature,\n",
        "            do_sample=True,\n",
        "            top_p=0.9,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    # Decodificar solo la nueva parte generada\n",
        "    response = tokenizer.decode(\n",
        "        outputs[0][inputs.input_ids.shape[1]:],\n",
        "        skip_special_tokens=True\n",
        "    )\n",
        "\n",
        "    return response\n",
        "\n",
        "# 8. Ejemplo de uso\n",
        "user_question = \"Explica las diferencias entre deep learning y machine learning tradicional.\"\n",
        "respuesta = generar_respuesta_optimizada(user_question)\n",
        "print(f\"Pregunta: {user_question}\")\n",
        "print(f\"Respuesta: {respuesta}\")"
      ],
      "metadata": {
        "id": "hmCYpHllPYw2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# M√âTODO 3: OLLAMA\n",
        "# Ejecuci√≥n local simplificada\n",
        "\n",
        "# 1. Importar bibliotecas\n",
        "import subprocess\n",
        "import json\n",
        "import time\n",
        "\n",
        "# 2. Par√°metros configurables\n",
        "MAX_TOKENS = 512\n",
        "TEMPERATURE = 0.7\n",
        "SYSTEM_PROMPT = \"Eres un asistente √∫til y educativo. Responde de manera clara y concisa.\"\n",
        "MODELO_OLLAMA = \"llama2:7b\"\n",
        "\n",
        "# 3. Instalar Ollama\n",
        "!curl -fsSL https://ollama.com/install.sh | sh\n",
        "\n",
        "# 4. Configurar servidor Ollama\n",
        "!pkill ollama || true  # Detener procesos existentes\n",
        "!nohup /usr/local/bin/ollama serve > ollama_output.log 2>&1 &  # Iniciar servidor\n",
        "\n",
        "# 5. Esperar que el servidor inicie\n",
        "time.sleep(20)  # Aumentar tiempo de espera\n",
        "\n",
        "# 6. Verificar servidor\n",
        "!curl -s http://localhost:11434/api/tags\n",
        "\n",
        "# 7. Descargar modelo Llama-2\n",
        "!/usr/local/bin/ollama pull {MODELO_OLLAMA}  # Usar ruta completa\n",
        "\n",
        "# 8. Funci√≥n para generar respuestas\n",
        "def generar_respuesta_ollama(user_input, system_prompt=SYSTEM_PROMPT, max_tokens=MAX_TOKENS, temperature=TEMPERATURE):\n",
        "    \"\"\"\n",
        "    Genera respuesta usando Ollama\n",
        "    \"\"\"\n",
        "    # Combinar system prompt con entrada del usuario\n",
        "    full_prompt = f\"{system_prompt}\\\\n\\\\nUsuario: {user_input}\\\\nAsistente:\"\n",
        "\n",
        "    # Crear comando curl para API de Ollama\n",
        "    comando = f'''curl -s http://localhost:11434/api/generate -d '{{\"model\": \"{MODELO_OLLAMA}\", \"prompt\": \"{full_prompt}\", \"stream\": false, \"options\": {{\"temperature\": {temperature}, \"num_predict\": {max_tokens}}}}}'\n",
        "    '''\n",
        "\n",
        "    try:\n",
        "        # Ejecutar comando y obtener respuesta\n",
        "        resultado = subprocess.check_output(comando, shell=True, text=True)\n",
        "        respuesta_json = json.loads(resultado)\n",
        "        response = respuesta_json.get('response', 'No se obtuvo respuesta')\n",
        "        return response\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error: {e}\"\n",
        "\n",
        "# 9. Ejemplo de uso\n",
        "user_question = \"¬øCu√°les son las ventajas de usar redes neuronales convolucionales?\"\n",
        "respuesta = generar_respuesta_ollama(user_question)\n",
        "print(f\"Pregunta: {user_question}\")\n",
        "print(f\"Respuesta: {respuesta}\")"
      ],
      "metadata": {
        "id": "u1G24vMZPgzX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# M√âTODO 4: GROQ API\n",
        "# API en la nube de alta velocidad\n",
        "\n",
        "# 1. Instalaci√≥n de dependencias\n",
        "!pip install groq -q\n",
        "\n",
        "# 2. Importar bibliotecas\n",
        "from groq import Groq\n",
        "from google.colab import userdata\n",
        "\n",
        "# 3. Obtener API Key de Groq desde secretos de Colab\n",
        "GROQ_API_KEY = userdata.get('GROQ_KEY')\n",
        "\n",
        "# 4. Par√°metros configurables\n",
        "MAX_TOKENS = 512\n",
        "TEMPERATURE = 0.7\n",
        "SYSTEM_PROMPT = \"Eres un asistente √∫til y educativo. Responde de manera clara y concisa.\"\n",
        "MODELO_GROQ = \"llama3-8b-8192\"  # Modelo actualizado disponible\n",
        "\n",
        "# 5. Inicializar cliente Groq\n",
        "if GROQ_API_KEY:\n",
        "    client = Groq(api_key=GROQ_API_KEY)\n",
        "    print(\"Cliente Groq inicializado\")\n",
        "else:\n",
        "    print(\"Error: GROQ_KEY no encontrado en secretos de Colab\")\n",
        "\n",
        "# 6. Funci√≥n para generar respuestas\n",
        "def generar_respuesta_groq(user_input, system_prompt=SYSTEM_PROMPT, max_tokens=MAX_TOKENS, temperature=TEMPERATURE):\n",
        "    \"\"\"\n",
        "    Genera respuesta usando Groq API\n",
        "    \"\"\"\n",
        "    try:\n",
        "        completion = client.chat.completions.create(\n",
        "            model=MODELO_GROQ,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": system_prompt},\n",
        "                {\"role\": \"user\", \"content\": user_input}\n",
        "            ],\n",
        "            max_tokens=max_tokens,\n",
        "            temperature=temperature\n",
        "        )\n",
        "        return completion.choices[0].message.content\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error: {e}\"\n",
        "\n",
        "# 7. Ejemplo de uso\n",
        "user_question = \"¬øQu√© son los transformers en procesamiento de lenguaje natural?\"\n",
        "respuesta = generar_respuesta_groq(user_question)\n",
        "print(f\"Pregunta: {user_question}\")\n",
        "print(f\"Respuesta: {respuesta}\")"
      ],
      "metadata": {
        "id": "40oOYv3bPl0w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Comparaci√≥n de M√©todos\n",
        "\n",
        "| M√©todo | Velocidad | Uso GPU | Configuraci√≥n | Costo | Ideal para |\n",
        "|--------|-----------|---------|---------------|-------|------------|\n",
        "| Pipeline | Media | Alta | F√°cil | Gratis | Prototipos |\n",
        "| Optimizado | Alta | Media | Intermedia | Gratis | Producci√≥n local |\n",
        "| Ollama | Media | Baja | F√°cil | Gratis | Desarrollo |\n",
        "| Groq | Muy Alta | Ninguna | F√°cil | Freemium | Apps r√°pidas |\n",
        "\n",
        "## Recomendaciones\n",
        "\n",
        "- **Para aprender:** Pipeline\n",
        "- **Para m√°ximo rendimiento:** Optimizado  \n",
        "- **Para desarrollo local:** Ollama\n",
        "- **Para producci√≥n:** Groq\n",
        "\n"
      ],
      "metadata": {
        "id": "8_-oW8QPPwtP"
      }
    }
  ]
}