{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNFKUVsLhuLfnFkJ8c5fYE8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CamiloVga/Curso-IA-Para-Ciencia-de-Datos/blob/main/Script_Sesi%C3%B3n_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# IA para la Ciencia de Datos\n",
        "## Universidad de los Andes\n",
        "\n",
        "**Profesor:** Camilo Vega - AI/ML Engineer  \n",
        "**LinkedIn:** https://www.linkedin.com/in/camilo-vega-169084b1/\n",
        "\n",
        "---\n",
        "\n",
        "## Gu√≠a: Fine-tuning y RAG (Retrieval-Augmented Generation)\n",
        "\n",
        "Este notebook presenta **3 implementaciones pr√°cticas**:\n",
        "\n",
        "1. **Fine-tuning B√°sico** - An√°lisis de sentimientos con tweets\n",
        "2. **RAG Simple** - B√∫squeda b√°sica en documentos\n",
        "3. **RAG Base de Datos** - Sistema especializado con embeddings\n",
        "\n",
        "### Requisitos\n",
        "- **GPU:** Tesla T4 m√≠nimo (Colab gratuito)\n",
        "- **APIs:** Hugging Face token\n",
        "- **Datos:** Tweets y dataset de ventas\n",
        "\n",
        "## Configuraci√≥n APIs\n",
        "- **Hugging Face:**\n",
        "  1. [Crear token](https://huggingface.co/settings/tokens)\n",
        "  2. En Colab: üîë Secrets ‚Üí Agregar `HF_TOKEN` ‚Üí Pegar tu token\n",
        "\n",
        "Cada secci√≥n es **independiente** y puede ejecutarse por separado.\n"
      ],
      "metadata": {
        "id": "LZ2dQpaLpth1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Fine Tuning"
      ],
      "metadata": {
        "id": "cWzTVw3kOxU5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2kkpznc1pd0A"
      },
      "outputs": [],
      "source": [
        "# 1. Instalaciones\n",
        "!pip install torch transformers datasets scikit-learn matplotlib seaborn pandas huggingface_hub -q\n",
        "\n",
        "# 2. Imports y configuraci√≥n\n",
        "from google.colab import userdata\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, classification_report, roc_auc_score\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from sklearn.model_selection import train_test_split\n",
        "from huggingface_hub import login\n",
        "\n",
        "# Configuraci√≥n\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_name = \"bert-base-multilingual-cased\"  # Modelo multiling√ºe para ingl√©s\n",
        "\n",
        "# Autenticaci√≥n\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "login(token=hf_token)\n",
        "print(f\"Dispositivo: {device}\")\n",
        "print(f\"Modelo: {model_name} (multiling√ºe para textos en ingl√©s)\")\n",
        "\n",
        "# 3. Cargar datos\n",
        "print(\"Cargando dataset de tweets...\")\n",
        "dataset = load_dataset(\"mteb/tweet_sentiment_extraction\", split=\"train\")\n",
        "\n",
        "textos = dataset[\"text\"]\n",
        "etiquetas = dataset[\"label\"]  # 0=negative, 1=neutral, 2=positive\n",
        "\n",
        "print(f\"Dataset cargado: {len(textos)} muestras\")\n",
        "print(f\"Distribuci√≥n: {pd.Series(etiquetas).value_counts().sort_index().to_dict()}\")\n",
        "\n",
        "# 4. Filtrar y balancear\n",
        "df = pd.DataFrame({'texto': textos, 'etiqueta': etiquetas})\n",
        "\n",
        "# Tomar 5000 ejemplos balanceados\n",
        "df_sample = df.groupby('etiqueta').apply(\n",
        "    lambda x: x.sample(min(1667, len(x)), random_state=42)\n",
        ").reset_index(drop=True).sample(frac=1, random_state=42)\n",
        "\n",
        "textos_final = df_sample['texto'].tolist()\n",
        "etiquetas_final = df_sample['etiqueta'].tolist()\n",
        "\n",
        "print(f\"Datos balanceados: {len(textos_final)} muestras\")\n",
        "print(f\"Distribuci√≥n final: {pd.Series(etiquetas_final).value_counts().sort_index().to_dict()}\")\n",
        "\n",
        "# 5. Split train/test\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    textos_final, etiquetas_final, test_size=0.2, random_state=42, stratify=etiquetas_final\n",
        ")\n",
        "\n",
        "# 6. Modelo y tokenizador\n",
        "print(\"Cargando modelo...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3).to(device)\n",
        "\n",
        "# 7. Tokenizaci√≥n\n",
        "train_encodings = tokenizer(X_train, truncation=True, padding=True, max_length=128, return_tensors=\"pt\")\n",
        "test_encodings = tokenizer(X_test, truncation=True, padding=True, max_length=128, return_tensors=\"pt\")\n",
        "\n",
        "# 8. Dataset clase\n",
        "class SentimentDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "train_dataset = SentimentDataset(train_encodings, y_train)\n",
        "test_dataset = SentimentDataset(test_encodings, y_test)\n",
        "\n",
        "# 9. M√©tricas (DEFINIR ANTES DE USAR)\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    accuracy = accuracy_score(labels, predictions)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='weighted')\n",
        "    return {'accuracy': accuracy, 'f1': f1, 'precision': precision, 'recall': recall}\n",
        "\n",
        "# 10. Configuraci√≥n de entrenamiento (DEFINIR ANTES DE USAR)\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./modelo',\n",
        "    num_train_epochs=5,  # Reducir √©pocas para evitar divergencia\n",
        "    per_device_train_batch_size=32,  # Batch m√°s grande para estabilidad\n",
        "    per_device_eval_batch_size=64,\n",
        "    learning_rate=1e-5,  # Learning rate mucho m√°s bajo\n",
        "    weight_decay=0.1,    # M√°s regularizaci√≥n\n",
        "    warmup_steps=200,\n",
        "    eval_strategy=\"epoch\",\n",
        "    logging_steps=50,\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_accuracy\",  # Optimizar por accuracy\n",
        "    greater_is_better=True,\n",
        "    report_to=[],\n",
        "    dataloader_drop_last=True,\n",
        "    gradient_checkpointing=True,  # Ahorrar memoria\n",
        "    fp16=True  # Precisi√≥n mixta para estabilidad\n",
        ")\n",
        "\n",
        "# 11. Entrenar modelo est√°ndar (AHORA CON VARIABLES DEFINIDAS)\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"ENTRENANDO...\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Entrenar y capturar historial\n",
        "history = trainer.train()\n",
        "\n",
        "# Extraer m√©tricas del historial de logs\n",
        "train_losses = []\n",
        "eval_losses = []\n",
        "eval_accuracies = []\n",
        "\n",
        "for log in trainer.state.log_history:\n",
        "    if 'train_loss' in log:\n",
        "        train_losses.append(log['train_loss'])\n",
        "    if 'eval_loss' in log:\n",
        "        eval_losses.append(log['eval_loss'])\n",
        "    if 'eval_accuracy' in log:\n",
        "        eval_accuracies.append(log['eval_accuracy'])\n",
        "\n",
        "print(f\"Entrenamiento completado. Logs capturados: {len(eval_losses)} evaluaciones\")\n",
        "\n",
        "# 12. Gr√°ficas usando historial de logs\n",
        "if eval_losses:\n",
        "    plt.figure(figsize=(15, 4))\n",
        "    epochs = range(1, len(eval_losses) + 1)\n",
        "\n",
        "    # P√©rdida de evaluaci√≥n\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.plot(epochs, eval_losses, 'r-o', linewidth=2, markersize=6)\n",
        "    plt.title('P√©rdida de Evaluaci√≥n')\n",
        "    plt.xlabel('√âpoca')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # Accuracy\n",
        "    plt.subplot(1, 3, 2)\n",
        "    if eval_accuracies and len(eval_accuracies) == len(eval_losses):\n",
        "        plt.plot(epochs, eval_accuracies, 'g-o', linewidth=2, markersize=6)\n",
        "    plt.title('Accuracy')\n",
        "    plt.xlabel('√âpoca')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # Train loss si est√° disponible\n",
        "    plt.subplot(1, 3, 3)\n",
        "    if train_losses:\n",
        "        # Si hay m√∫ltiples train losses por √©poca, promediarlos\n",
        "        if len(train_losses) > len(epochs):\n",
        "            steps_per_epoch = len(train_losses) // len(epochs)\n",
        "            train_by_epoch = []\n",
        "            for i in range(len(epochs)):\n",
        "                start = i * steps_per_epoch\n",
        "                end = min((i + 1) * steps_per_epoch, len(train_losses))\n",
        "                if start < len(train_losses):\n",
        "                    epoch_avg = sum(train_losses[start:end]) / len(train_losses[start:end])\n",
        "                    train_by_epoch.append(epoch_avg)\n",
        "\n",
        "            if len(train_by_epoch) == len(epochs):\n",
        "                plt.plot(epochs, train_by_epoch, 'b-s', linewidth=2, alpha=0.7, label='Train Loss')\n",
        "                plt.plot(epochs, eval_losses, 'r-o', linewidth=2, label='Eval Loss')\n",
        "                plt.legend()\n",
        "        else:\n",
        "            plt.plot(epochs, eval_losses, 'r-o', linewidth=2)\n",
        "    else:\n",
        "        plt.plot(epochs, eval_losses, 'r-o', linewidth=2)\n",
        "\n",
        "    plt.title('Curvas de P√©rdida')\n",
        "    plt.xlabel('√âpoca')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"\\nRESUMEN ENTRENAMIENTO:\")\n",
        "    if len(eval_losses) > 1:\n",
        "        print(f\"Loss inicial: {eval_losses[0]:.4f}\")\n",
        "        print(f\"Loss final: {eval_losses[-1]:.4f}\")\n",
        "        print(f\"Mejora: {eval_losses[0] - eval_losses[-1]:.4f}\")\n",
        "\n",
        "    if eval_accuracies and len(eval_accuracies) > 1:\n",
        "        print(f\"Accuracy inicial: {eval_accuracies[0]:.4f}\")\n",
        "        print(f\"Accuracy final: {eval_accuracies[-1]:.4f}\")\n",
        "        print(f\"Mejora: {eval_accuracies[-1] - eval_accuracies[0]:.4f}\")\n",
        "\n",
        "else:\n",
        "    print(\"No se encontraron m√©tricas de evaluaci√≥n en el historial\")\n",
        "\n",
        "# 13. Evaluaci√≥n final\n",
        "predictions = trainer.predict(test_dataset)\n",
        "y_pred = np.argmax(predictions.predictions, axis=1)\n",
        "y_pred_proba = torch.softmax(torch.tensor(predictions.predictions), dim=1).numpy()\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"RESULTADOS\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Verificar que las dimensiones coincidan\n",
        "print(f\"Muestras de prueba: {len(y_test)}\")\n",
        "print(f\"Predicciones: {len(y_pred)}\")\n",
        "\n",
        "# Labels names\n",
        "labels_names = ['Negativo', 'Neutral', 'Positivo']\n",
        "\n",
        "# Solo evaluar si las dimensiones coinciden\n",
        "if len(y_test) == len(y_pred):\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"ACCURACY: {accuracy:.4f}\")\n",
        "\n",
        "    # Matriz de confusi√≥n\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=labels_names, yticklabels=labels_names)\n",
        "    plt.title('Matriz de Confusi√≥n')\n",
        "    plt.show()\n",
        "\n",
        "    # ROC-AUC\n",
        "    y_true_bin = label_binarize(y_test, classes=[0, 1, 2])\n",
        "    roc_auc = roc_auc_score(y_true_bin, y_pred_proba, multi_class='ovr', average='weighted')\n",
        "    print(f\"ROC-AUC: {roc_auc:.4f}\")\n",
        "\n",
        "    # Classification report\n",
        "    print(\"\\nREPORTE:\")\n",
        "    print(classification_report(y_test, y_pred, target_names=labels_names))\n",
        "else:\n",
        "    print(f\"Error: Inconsistencia en dimensiones - y_test: {len(y_test)}, y_pred: {len(y_pred)}\")\n",
        "    print(\"Evaluando solo con las primeras muestras que coincidan...\")\n",
        "\n",
        "    min_len = min(len(y_test), len(y_pred))\n",
        "    y_test_truncated = y_test[:min_len]\n",
        "    y_pred_truncated = y_pred[:min_len]\n",
        "\n",
        "    accuracy = accuracy_score(y_test_truncated, y_pred_truncated)\n",
        "    print(f\"ACCURACY (truncado): {accuracy:.4f}\")\n",
        "\n",
        "    # Matriz de confusi√≥n truncada\n",
        "    cm = confusion_matrix(y_test_truncated, y_pred_truncated)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=labels_names, yticklabels=labels_names)\n",
        "    plt.title('Matriz de Confusi√≥n')\n",
        "    plt.show()\n",
        "\n",
        "# 14. Guardar modelo\n",
        "trainer.save_model(\"./sentiment_model\")\n",
        "tokenizer.save_pretrained(\"./sentiment_model\")\n",
        "print(\"Modelo guardado en ./sentiment_model\")\n",
        "\n",
        "# 15. Inferencia individual simple\n",
        "from transformers import pipeline\n",
        "\n",
        "# Cargar el modelo entrenado\n",
        "sentiment_pipeline = pipeline(\"text-classification\", model=\"./sentiment_model\", tokenizer=\"./sentiment_model\")\n",
        "\n",
        "# Ejemplo de uso: cambiar el texto aqu√≠ para probar\n",
        "texto_prueba = \"I love this amazing product!\"\n",
        "\n",
        "resultado = sentiment_pipeline(texto_prueba)[0]\n",
        "label_idx = int(resultado['label'].split('_')[1])\n",
        "sentimientos = ['Negativo', 'Neutral', 'Positivo']\n",
        "\n",
        "print(\"\\n\" + \"=\"*40)\n",
        "print(\"INFERENCIA INDIVIDUAL\")\n",
        "print(\"=\"*40)\n",
        "print(f\"Texto: '{texto_prueba}'\")\n",
        "print(f\"Sentimiento: {sentimientos[label_idx]}\")\n",
        "print(f\"Confianza: {resultado['score']:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#RAG Documentos"
      ],
      "metadata": {
        "id": "RuKAxobgOz-p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# RAG SIMPLE CON GROQ API\n",
        "# Sistema de b√∫squeda en documentos usando TF-IDF y Groq para generaci√≥n\n",
        "\n",
        "# Instalaci√≥n de dependencias\n",
        "!pip install groq scikit-learn PyPDF2 python-docx pandas openpyxl -q\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from groq import Groq\n",
        "from google.colab import userdata\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from PyPDF2 import PdfReader\n",
        "from docx import Document\n",
        "\n",
        "# Configuraci√≥n del sistema\n",
        "GROQ_API_KEY = userdata.get('GROQ_KEY')\n",
        "client = Groq(api_key=GROQ_API_KEY)\n",
        "folder_path = '/content/carpeta_rag'\n",
        "\n",
        "# Variables globales del sistema\n",
        "vectorizer = TfidfVectorizer(max_features=1000, stop_words=None)\n",
        "chunks = []\n",
        "vectors = None\n",
        "\n",
        "def load_documents(folder_path):\n",
        "    \"\"\"Carga y procesa documentos de m√∫ltiples formatos\"\"\"\n",
        "    text = ''\n",
        "    os.makedirs(folder_path, exist_ok=True)\n",
        "\n",
        "    for filename in os.listdir(folder_path):\n",
        "        filepath = os.path.join(folder_path, filename)\n",
        "\n",
        "        try:\n",
        "            if filename.endswith('.txt'):\n",
        "                with open(filepath, 'r', encoding='utf-8') as f:\n",
        "                    text += f.read() + '\\n'\n",
        "\n",
        "            elif filename.endswith('.pdf'):\n",
        "                reader = PdfReader(filepath)\n",
        "                for page in reader.pages:\n",
        "                    text += page.extract_text() + '\\n'\n",
        "\n",
        "            elif filename.endswith('.docx'):\n",
        "                doc = Document(filepath)\n",
        "                for paragraph in doc.paragraphs:\n",
        "                    text += paragraph.text + '\\n'\n",
        "\n",
        "            elif filename.endswith('.csv'):\n",
        "                df = pd.read_csv(filepath)\n",
        "                text += df.to_string() + '\\n'\n",
        "\n",
        "            elif filename.endswith('.xlsx'):\n",
        "                df = pd.read_excel(filepath)\n",
        "                text += df.to_string() + '\\n'\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "    return text\n",
        "\n",
        "def create_chunks(text, chunk_size=500):\n",
        "    \"\"\"Divide el texto en fragmentos de tama√±o manejable\"\"\"\n",
        "    words = text.split()\n",
        "    chunks = []\n",
        "\n",
        "    for i in range(0, len(words), chunk_size):\n",
        "        chunk = ' '.join(words[i:i + chunk_size])\n",
        "        if len(chunk.strip()) > 100:\n",
        "            chunks.append(chunk.strip())\n",
        "\n",
        "    return chunks\n",
        "\n",
        "def search_relevant_chunks(query, top_k=3):\n",
        "    \"\"\"Encuentra fragmentos m√°s relevantes usando similitud coseno\"\"\"\n",
        "    query_vector = vectorizer.transform([query])\n",
        "    similarities = cosine_similarity(query_vector, vectors)[0]\n",
        "\n",
        "    # Obtener √≠ndices ordenados por relevancia\n",
        "    top_indices = np.argsort(similarities)[-top_k:][::-1]\n",
        "\n",
        "    relevant_chunks = []\n",
        "    for idx in top_indices:\n",
        "        if similarities[idx] > 0.1:  # Umbral m√≠nimo de relevancia\n",
        "            relevant_chunks.append(chunks[idx])\n",
        "\n",
        "    return relevant_chunks\n",
        "\n",
        "def generate_answer(query, context):\n",
        "    \"\"\"Genera respuesta usando Groq API con el contexto encontrado\"\"\"\n",
        "    if not context:\n",
        "        return \"No encontr√© informaci√≥n relevante en los documentos.\"\n",
        "\n",
        "    prompt = f\"\"\"Bas√°ndote en esta informaci√≥n:\n",
        "\n",
        "{context}\n",
        "\n",
        "Pregunta: {query}\n",
        "\n",
        "Instrucciones:\n",
        "- Responde solo con informaci√≥n del contexto\n",
        "- Si no hay informaci√≥n suficiente, di que no puedes responder\n",
        "- Responde en espa√±ol de forma clara y concisa\"\"\"\n",
        "\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"openai/gpt-oss-20b\",\n",
        "            messages=[\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ],\n",
        "            max_tokens=400,\n",
        "            temperature=0.7\n",
        "        )\n",
        "        return response.choices[0].message.content\n",
        "    except Exception as e:\n",
        "        return f\"Error generando respuesta: {e}\"\n",
        "\n",
        "def inicializar_rag():\n",
        "    \"\"\"Inicializa el sistema RAG cargando documentos y creando √≠ndices\"\"\"\n",
        "    global chunks, vectors, vectorizer\n",
        "\n",
        "    print(\"Cargando documentos...\")\n",
        "    documents = load_documents(folder_path)\n",
        "\n",
        "    if not documents.strip():\n",
        "        print(\"No se encontraron documentos en la carpeta.\")\n",
        "        return False\n",
        "\n",
        "    print(\"Creando fragmentos de texto...\")\n",
        "    chunks = create_chunks(documents)\n",
        "\n",
        "    print(\"Generando √≠ndice de b√∫squeda...\")\n",
        "    vectors = vectorizer.fit_transform(chunks)\n",
        "\n",
        "    print(f\"Sistema listo: {len(chunks)} fragmentos procesados.\")\n",
        "    return True\n",
        "\n",
        "# Inicializaci√≥n del sistema\n",
        "inicializar_rag()\n",
        "\n",
        "# BLOQUE DE INFERENCIA - Ejecutar por separado\n",
        "query = \"¬øDe qu√© tratan estos documentos?\"  # Cambia tu pregunta aqu√≠\n",
        "context = '\\n\\n'.join(search_relevant_chunks(query))\n",
        "respuesta = client.chat.completions.create(model=\"openai/gpt-oss-20b\", messages=[{\"role\": \"user\", \"content\": f\"Contexto: {context}\\nPregunta: {query}\\nResponde solo con informaci√≥n del contexto en espa√±ol.\"}], max_tokens=400).choices[0].message.content\n",
        "print(query)\n",
        "print(respuesta)"
      ],
      "metadata": {
        "id": "YhL3CdQ3vvlu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Rag Base de Datos\n"
      ],
      "metadata": {
        "id": "AuxeC36BO4y7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# RAG BASE DE DATOS CON GROQ API\n",
        "# Sistema de consultas inteligentes sobre bases de datos usando embeddings y LLM\n",
        "# Convierte preguntas en lenguaje natural a SQL ejecutable\n",
        "\n",
        "\"\"\"\n",
        "ARQUITECTURA DEL SISTEMA:\n",
        "\n",
        "1. Carga de datos: CSV ‚Üí SQLite en memoria\n",
        "2. An√°lisis de esquema: Detecta tipos y genera embeddings de columnas\n",
        "3. B√∫squeda sem√°ntica: Encuentra columnas relevantes para la consulta\n",
        "4. Generaci√≥n SQL: LLM crea consulta SQL basada en esquema y pregunta\n",
        "5. Ejecuci√≥n: Consulta todas las observaciones sin l√≠mites\n",
        "6. Respuesta: LLM genera respuesta natural con estad√≠sticas precisas\n",
        "\"\"\"\n",
        "\n",
        "# Instalaci√≥n de dependencias\n",
        "!pip install groq sentence-transformers pandas numpy scikit-learn -q\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sqlite3\n",
        "import os\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from groq import Groq\n",
        "from google.colab import userdata\n",
        "\n",
        "# Configuraci√≥n del sistema\n",
        "GROQ_API_KEY = userdata.get('GROQ_KEY')\n",
        "client = Groq(api_key=GROQ_API_KEY)\n",
        "\n",
        "# Modelo centralizado para ambos componentes del sistema\n",
        "GROQ_MODEL = \"openai/gpt-oss-20b\"\n",
        "\n",
        "# Configuraci√≥n de paths y variables globales\n",
        "folder_path = '/content/carpeta_rag'\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "conn = None\n",
        "table_name = 'data'\n",
        "column_embeddings = {}\n",
        "column_info = {}\n",
        "schema_description = \"\"\n",
        "\n",
        "def load_database(folder_path):\n",
        "    \"\"\"Carga CSV desde carpeta y crea base de datos en memoria\"\"\"\n",
        "    global conn, column_embeddings, column_info, schema_description\n",
        "\n",
        "    os.makedirs(folder_path, exist_ok=True)\n",
        "\n",
        "    csv_file = None\n",
        "    try:\n",
        "        files = os.listdir(folder_path)\n",
        "        print(f\"Archivos en carpeta: {files}\")\n",
        "\n",
        "        # Buscar primer archivo CSV\n",
        "        for filename in files:\n",
        "            if filename.endswith('.csv'):\n",
        "                csv_file = os.path.join(folder_path, filename)\n",
        "                break\n",
        "\n",
        "        if not csv_file:\n",
        "            print(\"Error: No se encontr√≥ ning√∫n archivo CSV en la carpeta.\")\n",
        "            return False\n",
        "\n",
        "        print(f\"Cargando {csv_file}...\")\n",
        "        df = pd.read_csv(csv_file)\n",
        "\n",
        "        # Crear conexi√≥n SQLite en memoria\n",
        "        if conn:\n",
        "            conn.close()\n",
        "        conn = sqlite3.connect(':memory:', check_same_thread=False)\n",
        "        df.to_sql(table_name, conn, index=False, if_exists='replace')\n",
        "\n",
        "        # Generar embeddings y descripci√≥n del esquema\n",
        "        column_embeddings.clear()\n",
        "        column_info.clear()\n",
        "        schema_parts = []\n",
        "\n",
        "        for col in df.columns:\n",
        "            # Detectar tipo de columna autom√°ticamente\n",
        "            if df[col].dtype in ['int64', 'float64']:\n",
        "                col_type = 'NUMERIC'\n",
        "                sample_values = f\"rango: {df[col].min()} - {df[col].max()}\"\n",
        "                desc = f\"{col} (num√©rica): {sample_values}\"\n",
        "            elif df[col].nunique() <= 20:\n",
        "                col_type = 'CATEGORICAL'\n",
        "                unique_vals = list(df[col].unique())\n",
        "                sample_values = f\"valores: {', '.join(map(str, unique_vals[:5]))}\"\n",
        "                desc = f\"{col} (categ√≥rica): {sample_values}\"\n",
        "            else:\n",
        "                col_type = 'TEXT'\n",
        "                sample_values = f\"texto con {df[col].nunique()} valores √∫nicos\"\n",
        "                desc = f\"{col} (texto): {sample_values}\"\n",
        "\n",
        "            column_info[col] = {\n",
        "                'type': col_type,\n",
        "                'sample_values': sample_values,\n",
        "                'description': desc\n",
        "            }\n",
        "\n",
        "            # Generar embedding para b√∫squeda sem√°ntica\n",
        "            column_embeddings[col] = model.encode([desc])[0]\n",
        "            schema_parts.append(f\"- {col} ({col_type}): {sample_values}\")\n",
        "\n",
        "        schema_description = f\"\"\"\n",
        "Tabla: {table_name}\n",
        "Total de registros: {len(df)}\n",
        "Columnas disponibles:\n",
        "\"\"\" + \"\\n\".join(schema_parts)\n",
        "\n",
        "        print(f\"Base de datos cargada: {df.shape[0]} filas, {df.shape[1]} columnas\")\n",
        "        print(\"Esquema detectado:\")\n",
        "        print(schema_description)\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error cargando datos: {e}\")\n",
        "        return False\n",
        "\n",
        "def find_relevant_columns(query, top_k=5):\n",
        "    \"\"\"Encuentra las columnas m√°s relevantes usando b√∫squeda sem√°ntica\"\"\"\n",
        "    query_emb = model.encode([query])[0]\n",
        "    scores = []\n",
        "    for col, col_emb in column_embeddings.items():\n",
        "        sim = cosine_similarity([query_emb], [col_emb])[0][0]\n",
        "        scores.append((col, sim, column_info[col]))\n",
        "    return sorted(scores, key=lambda x: x[1], reverse=True)[:top_k]\n",
        "\n",
        "def generate_sql_with_llm(query):\n",
        "    \"\"\"Genera consulta SQL usando LLM con contexto del esquema\"\"\"\n",
        "    relevant_cols = find_relevant_columns(query)\n",
        "\n",
        "    # Construir contexto de columnas relevantes\n",
        "    cols_context = \"\\nColumnas m√°s relevantes para la consulta:\\n\"\n",
        "    for col, score, info in relevant_cols:\n",
        "        cols_context += f\"- {col} ({info['type']}): {info['sample_values']}\\n\"\n",
        "\n",
        "    prompt = f\"\"\"Eres un experto en SQL. Genera una consulta SQL para responder la pregunta del usuario.\n",
        "\n",
        "ESQUEMA DE LA BASE DE DATOS:\n",
        "{schema_description}\n",
        "\n",
        "{cols_context}\n",
        "\n",
        "PREGUNTA DEL USUARIO: {query}\n",
        "\n",
        "INSTRUCCIONES:\n",
        "1. Usa el nombre de tabla: {table_name}\n",
        "2. NO uses LIMIT - necesitamos todos los datos\n",
        "3. Si necesitas agrupar, usa GROUP BY apropiadamente\n",
        "4. Si necesitas ordenar, usa ORDER BY\n",
        "5. Para consultas num√©ricas usa SUM, AVG, COUNT, etc.\n",
        "6. Responde SOLO con la consulta SQL, sin explicaciones\n",
        "\n",
        "CONSULTA SQL:\"\"\"\n",
        "\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=GROQ_MODEL,\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            max_tokens=200,\n",
        "            temperature=0.1\n",
        "        )\n",
        "        sql_generated = response.choices[0].message.content.strip()\n",
        "\n",
        "        # Limpiar la respuesta\n",
        "        sql_generated = sql_generated.replace('```sql', '').replace('```', '').strip()\n",
        "\n",
        "        return sql_generated\n",
        "    except Exception as e:\n",
        "        print(f\"Error generando SQL: {e}\")\n",
        "        # Fallback simple\n",
        "        relevant_col_names = [col for col, _, _ in relevant_cols[:3]]\n",
        "        return f\"SELECT {', '.join(relevant_col_names)} FROM {table_name}\"\n",
        "\n",
        "def execute_query(sql):\n",
        "    \"\"\"Ejecuta la consulta SQL en la base de datos\"\"\"\n",
        "    global conn\n",
        "    if conn is None:\n",
        "        return \"Error: Base de datos no inicializada\", pd.DataFrame()\n",
        "\n",
        "    try:\n",
        "        print(f\"Ejecutando SQL: {sql}\")\n",
        "        resultado = pd.read_sql_query(sql, conn)\n",
        "        return sql, resultado\n",
        "    except Exception as e:\n",
        "        print(f\"Error SQL: {e}\")\n",
        "        # Fallback: mostrar todas las columnas relevantes\n",
        "        try:\n",
        "            fallback_sql = f\"SELECT * FROM {table_name}\"\n",
        "            resultado = pd.read_sql_query(fallback_sql, conn)\n",
        "            return fallback_sql, resultado\n",
        "        except Exception as e2:\n",
        "            print(f\"Error en consulta fallback: {e2}\")\n",
        "            return \"Error\", pd.DataFrame()\n",
        "\n",
        "def generate_answer(query, sql_executed, data):\n",
        "    \"\"\"Genera respuesta natural usando LLM con an√°lisis estad√≠stico\"\"\"\n",
        "    if data.empty:\n",
        "        return \"No se encontraron datos para esta consulta.\"\n",
        "\n",
        "    # Preparar estad√≠sticas autom√°ticas\n",
        "    stats_summary = f\"Se procesaron {len(data)} registros totales.\\n\"\n",
        "\n",
        "    # Agregar estad√≠sticas de columnas num√©ricas\n",
        "    numeric_cols = data.select_dtypes(include=[np.number]).columns\n",
        "    if len(numeric_cols) > 0:\n",
        "        stats_summary += \"\\nEstad√≠sticas num√©ricas:\\n\"\n",
        "        for col in numeric_cols[:3]:\n",
        "            stats_summary += f\"- {col}: total={data[col].sum():.2f}, promedio={data[col].mean():.2f}\\n\"\n",
        "\n",
        "    # Preparar muestra de datos\n",
        "    data_sample = data.head(15).to_string(index=False) if len(data) > 15 else data.to_string(index=False)\n",
        "    if len(data) > 15:\n",
        "        data_sample += f\"\\n... (mostrando 15 de {len(data)} registros)\"\n",
        "\n",
        "    prompt = f\"\"\"Analiza estos resultados de base de datos y responde la pregunta del usuario de forma clara y profesional.\n",
        "\n",
        "PREGUNTA ORIGINAL: {query}\n",
        "SQL EJECUTADO: {sql_executed}\n",
        "\n",
        "ESTAD√çSTICAS:\n",
        "{stats_summary}\n",
        "\n",
        "DATOS OBTENIDOS:\n",
        "{data_sample}\n",
        "\n",
        "INSTRUCCIONES:\n",
        "- Responde bas√°ndote en los {len(data)} registros procesados\n",
        "- Da n√∫meros exactos y estad√≠sticas precisas\n",
        "- Responde en espa√±ol de forma profesional y directa\n",
        "- Si hay rankings, menciona los elementos m√°s importantes\n",
        "- No repitas el SQL ni la pregunta\n",
        "\n",
        "RESPUESTA:\"\"\"\n",
        "\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=GROQ_MODEL,\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            max_tokens=300,\n",
        "            temperature=0.2\n",
        "        )\n",
        "        return response.choices[0].message.content.strip()\n",
        "    except Exception as e:\n",
        "        return f\"Error generando respuesta: {e}\"\n",
        "\n",
        "def query_database(user_query):\n",
        "    \"\"\"Funci√≥n principal para procesar consultas de usuario\"\"\"\n",
        "    print(f\"\\nüîç Procesando consulta: {user_query}\")\n",
        "\n",
        "    # Pipeline completo del RAG\n",
        "    sql = generate_sql_with_llm(user_query)\n",
        "    sql_executed, data = execute_query(sql)\n",
        "    respuesta = generate_answer(user_query, sql_executed, data)\n",
        "\n",
        "    print(f\"üìä SQL ejecutado: {sql_executed}\")\n",
        "    print(f\"üí° Respuesta: {respuesta}\")\n",
        "\n",
        "    return respuesta, data\n",
        "\n",
        "def inicializar_rag():\n",
        "    \"\"\"Inicializa el sistema RAG completo\"\"\"\n",
        "    print(\"üöÄ Iniciando sistema RAG...\")\n",
        "    if load_database(folder_path):\n",
        "        print(\"‚úÖ Sistema RAG listo para consultas.\")\n",
        "        return True\n",
        "    else:\n",
        "        print(\"‚ùå Error iniciando RAG.\")\n",
        "        return False\n",
        "\n",
        "# Inicializaci√≥n del sistema\n",
        "inicializar_rag()\n",
        "\n",
        "# Ejemplo de uso\n",
        "query = \"¬øcu√°l es la ciudad con mayores ventas?\"\n",
        "sql = generate_sql_with_llm(query)\n",
        "sql_executed, data = execute_query(sql)\n",
        "respuesta = generate_answer(query, sql_executed, data)\n",
        "print(f\"Pregunta: {query}\")\n",
        "print(f\"SQL: {sql_executed}\")\n",
        "print(f\"Respuesta: {respuesta}\")"
      ],
      "metadata": {
        "id": "MMmRdGQ0NQe0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BLOQUE DE INFERENCIA - ejemplo\n",
        "query = \"¬øCu√°les son los colores que m√°s venden?\"\n",
        "sql = generate_sql_with_llm(query)\n",
        "sql_executed, data = execute_query(sql)\n",
        "respuesta = generate_answer(query, sql_executed, data)\n",
        "print(f\"Pregunta: {query}\")\n",
        "print(f\"SQL: {sql_executed}\")\n",
        "print(f\"Respuesta: {respuesta}\")"
      ],
      "metadata": {
        "id": "XAY5WtocNwyC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BLOQUE DE INFERENCIA - ejemplo\n",
        "query = \"¬øcu√°l es el producto que m√°s genera ingresos??\"\n",
        "sql = generate_sql_with_llm(query)\n",
        "sql_executed, data = execute_query(sql)\n",
        "respuesta = generate_answer(query, sql_executed, data)\n",
        "print(f\"Pregunta: {query}\")\n",
        "print(f\"SQL: {sql_executed}\")\n",
        "print(f\"Respuesta: {respuesta}\")"
      ],
      "metadata": {
        "id": "k9TJqVvzOGKk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BLOQUE DE INFERENCIA - ejemplo\n",
        "query = \"¬øcu√°l es la talla que m√°s se vende?\"\n",
        "sql = generate_sql_with_llm(query)\n",
        "sql_executed, data = execute_query(sql)\n",
        "respuesta = generate_answer(query, sql_executed, data)\n",
        "print(f\"Pregunta: {query}\")\n",
        "print(f\"SQL: {sql_executed}\")\n",
        "print(f\"Respuesta: {respuesta}\")"
      ],
      "metadata": {
        "id": "byLNsNDOiL5i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BLOQUE DE INFERENCIA - ejemplo\n",
        "query = \"¬øCu√°nto suman las transacciones de la talla M?\"\n",
        "sql = generate_sql_with_llm(query)\n",
        "sql_executed, data = execute_query(sql)\n",
        "respuesta = generate_answer(query, sql_executed, data)\n",
        "print(f\"Pregunta: {query}\")\n",
        "print(f\"SQL: {sql_executed}\")\n",
        "print(f\"Respuesta: {respuesta}\")"
      ],
      "metadata": {
        "id": "XjnhSaYeYKML"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BLOQUE DE INFERENCIA - ejemplo\n",
        "query = \"¬øcu√°l es el m√©todo de pago m√°s importante\"\n",
        "sql = generate_sql_with_llm(query)\n",
        "sql_executed, data = execute_query(sql)\n",
        "respuesta = generate_answer(query, sql_executed, data)\n",
        "print(f\"Pregunta: {query}\")\n",
        "print(f\"SQL: {sql_executed}\")\n",
        "print(f\"Respuesta: {respuesta}\")"
      ],
      "metadata": {
        "id": "ZwhUEL8BYZLa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}