{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOcEXlaGEjTVyllaCGz15yt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CamiloVga/Curso-IA-Para-Ciencia-de-Datos/blob/main/Script_Sesi%C3%B3n_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# IA para la Ciencia de Datos\n",
        "## Universidad de los Andes\n",
        "\n",
        "**Profesor:** Camilo Vega - AI/ML Engineer  \n",
        "**LinkedIn:** https://www.linkedin.com/in/camilo-vega-169084b1/\n",
        "\n",
        "---\n",
        "\n",
        "## Gu√≠a: Fine-tuning y RAG (Retrieval-Augmented Generation)\n",
        "\n",
        "Este notebook presenta **3 implementaciones pr√°cticas**:\n",
        "\n",
        "1. **Fine-tuning B√°sico** - An√°lisis de sentimientos con tweets\n",
        "2. **RAG Simple** - B√∫squeda b√°sica en documentos\n",
        "3. **RAG Base de Datos** - Sistema especializado con embeddings\n",
        "\n",
        "### Requisitos\n",
        "- **GPU:** Tesla T4 m√≠nimo (Colab gratuito)\n",
        "- **APIs:** Hugging Face token\n",
        "- **Datos:** Tweets y dataset de ventas\n",
        "\n",
        "## Configuraci√≥n APIs\n",
        "- **Hugging Face:**\n",
        "  1. [Crear token](https://huggingface.co/settings/tokens)\n",
        "  2. En Colab: üîë Secrets ‚Üí Agregar `HF_TOKEN` ‚Üí Pegar tu token\n",
        "\n",
        "Cada secci√≥n es **independiente** y puede ejecutarse por separado.\n"
      ],
      "metadata": {
        "id": "LZ2dQpaLpth1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2kkpznc1pd0A"
      },
      "outputs": [],
      "source": [
        "# 1. FINE-TUNING B√ÅSICO - AN√ÅLISIS DE SENTIMIENTOS\n",
        "\n",
        "\"\"\"\n",
        "Fine-tuning de un modelo pre-entrenado para an√°lisis de sentimientos\n",
        "usando el dataset de reviews de Amazon en espa√±ol\n",
        "\"\"\"\n",
        "\n",
        "#0 Instalaciones\n",
        "!pip install torch transformers datasets scikit-learn matplotlib -q\n",
        "\n",
        "#1 Imports\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    pipeline\n",
        ")\n",
        "from sklearn.metrics import accuracy_score\n",
        "import random\n",
        "from collections import defaultdict\n",
        "\n",
        "#2 Configuraci√≥n del dispositivo\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Dispositivo: {device}\")\n",
        "\n",
        "#3 Cargar dataset de Amazon Reviews\n",
        "print(\"Cargando dataset de Amazon Reviews...\")\n",
        "dataset = load_dataset(\"mteb/amazon_reviews_multi\", \"es\", split=\"train\")\n",
        "print(f\"Total ejemplos: {len(dataset)}\")\n",
        "\n",
        "#4 Balancear dataset por clases\n",
        "print(\"\\nüìä Analizando distribuci√≥n de clases...\")\n",
        "class_counts = defaultdict(int)\n",
        "for sample in dataset:\n",
        "    class_counts[sample['label']] += 1\n",
        "\n",
        "print(\"Distribuci√≥n original:\")\n",
        "for label, count in sorted(class_counts.items()):\n",
        "    print(f\"  Clase {label}: {count:,} ejemplos\")\n",
        "\n",
        "#5 Crear dataset balanceado\n",
        "samples_per_class = 500  # Muestras por clase para entrenamiento r√°pido\n",
        "balanced_samples = []\n",
        "\n",
        "# Organizar por clase\n",
        "class_samples = defaultdict(list)\n",
        "for sample in dataset:\n",
        "    class_samples[sample['label']].append(sample)\n",
        "\n",
        "# Balancear clases\n",
        "random.seed(42)\n",
        "for label, samples in class_samples.items():\n",
        "    if len(samples) >= samples_per_class:\n",
        "        selected = random.sample(samples, samples_per_class)\n",
        "        balanced_samples.extend(selected)\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è Clase {label}: solo {len(samples)} ejemplos disponibles\")\n",
        "        balanced_samples.extend(samples)\n",
        "\n",
        "print(f\"\\n‚úÖ Dataset balanceado: {len(balanced_samples)} ejemplos\")\n",
        "\n",
        "#6 Extraer textos y etiquetas\n",
        "texts = [sample['text'] for sample in balanced_samples]\n",
        "labels = [sample['label'] for sample in balanced_samples]\n",
        "\n",
        "# Verificar distribuci√≥n final\n",
        "final_distribution = defaultdict(int)\n",
        "for label in labels:\n",
        "    final_distribution[label] += 1\n",
        "\n",
        "print(\"Distribuci√≥n balanceada:\")\n",
        "for label, count in sorted(final_distribution.items()):\n",
        "    print(f\"  Clase {label}: {count} ejemplos\")\n",
        "\n",
        "#7 Mostrar ejemplos\n",
        "print(\"\\nüìù Ejemplos del dataset:\")\n",
        "for i in range(3):\n",
        "    print(f\"{i+1}. Texto: '{texts[i][:80]}...'\")\n",
        "    print(f\"   Etiqueta: {labels[i]} (1=muy negativa, 5=muy positiva)\")\n",
        "    print()\n",
        "\n",
        "#8 Cargar modelo y tokenizador\n",
        "model_name = \"bert-base-multilingual-cased\"\n",
        "print(f\"Cargando modelo: {model_name}\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "num_labels = len(set(labels))  # N√∫mero √∫nico de etiquetas\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=num_labels\n",
        ").to(device)\n",
        "\n",
        "print(f\"‚úÖ Modelo cargado con {num_labels} clases\")\n",
        "\n",
        "#9 Tokenizar textos\n",
        "print(\"Tokenizando textos...\")\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(\n",
        "        examples,\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=512,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "# Tokenizar todos los textos\n",
        "tokenized_inputs = tokenize_function(texts)\n",
        "\n",
        "#10 Crear dataset de PyTorch\n",
        "class ReviewsDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx] - 1, dtype=torch.long)  # Ajustar a 0-indexado\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "# Crear datasets\n",
        "train_dataset = ReviewsDataset(tokenized_inputs, labels)\n",
        "\n",
        "# Split para validaci√≥n\n",
        "split_idx = int(0.8 * len(train_dataset))\n",
        "train_subset = torch.utils.data.Subset(train_dataset, range(split_idx))\n",
        "eval_subset = torch.utils.data.Subset(train_dataset, range(split_idx, len(train_dataset)))\n",
        "\n",
        "print(f\"Train: {len(train_subset)}, Eval: {len(eval_subset)}\")\n",
        "\n",
        "#11 Configurar hiperpar√°metros de entrenamiento\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./amazon_model\",\n",
        "\n",
        "    # HIPERPAR√ÅMETROS PRINCIPALES\n",
        "    num_train_epochs=3,                    # √âpocas - cu√°ntas veces ve los datos\n",
        "    per_device_train_batch_size=16,        # Tama√±o batch - memoria GPU vs velocidad\n",
        "    learning_rate=5e-5,                    # Tasa aprendizaje - velocidad de actualizaci√≥n\n",
        "    weight_decay=0.01,                     # Regularizaci√≥n - previene overfitting\n",
        "\n",
        "    # OPTIMIZACI√ìN\n",
        "    warmup_steps=200,                      # Pasos calentamiento - estabiliza inicio\n",
        "    gradient_accumulation_steps=2,         # Acumular gradientes - simula batch m√°s grande\n",
        "\n",
        "    # EVALUACI√ìN\n",
        "    eval_strategy=\"epoch\",                 # Evaluar cada √©poca\n",
        "    save_strategy=\"epoch\",                 # Guardar cada √©poca\n",
        "    load_best_model_at_end=True,          # Cargar mejor modelo\n",
        "\n",
        "    # LOGGING\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=50,\n",
        "    report_to=None,                       # Sin wandb\n",
        "\n",
        "    # L√çMITES\n",
        "    max_steps=150,                        # M√°ximo pasos para demo r√°pida\n",
        "    save_total_limit=1,                   # Solo guardar mejor modelo\n",
        ")\n",
        "\n",
        "#12 Funci√≥n de m√©tricas\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = predictions.argmax(axis=-1)\n",
        "    accuracy = accuracy_score(labels, predictions)\n",
        "    return {'accuracy': accuracy}\n",
        "\n",
        "#13 Crear trainer con registro de p√©rdida\n",
        "class CustomTrainer(Trainer):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.training_losses = []\n",
        "\n",
        "    def log(self, logs):\n",
        "        if \"loss\" in logs:\n",
        "            self.training_losses.append(logs[\"loss\"])\n",
        "        super().log(logs)\n",
        "\n",
        "#14 Configurar entrenador\n",
        "trainer = CustomTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_subset,\n",
        "    eval_dataset=eval_subset,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "#15 Entrenar modelo\n",
        "print(\"\\nüöÄ Iniciando fine-tuning...\")\n",
        "trainer.train()\n",
        "print(\"‚úÖ Entrenamiento completado\")\n",
        "\n",
        "#16 Evaluar modelo final\n",
        "print(\"\\nüìä Evaluaci√≥n final:\")\n",
        "results = trainer.evaluate()\n",
        "for key, value in results.items():\n",
        "    if key.startswith('eval_'):\n",
        "        print(f\"  {key}: {value:.4f}\")\n",
        "\n",
        "#17 Guardar modelo\n",
        "model_path = \"./amazon_sentiment_model\"\n",
        "trainer.save_model(model_path)\n",
        "tokenizer.save_pretrained(model_path)\n",
        "print(f\"üíæ Modelo guardado en: {model_path}\")\n",
        "\n",
        "#18 Visualizar p√©rdida de entrenamiento\n",
        "if trainer.training_losses:\n",
        "    import matplotlib.pyplot as plt\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(trainer.training_losses)\n",
        "    plt.title('P√©rdida durante el entrenamiento')\n",
        "    plt.xlabel('Pasos')\n",
        "    plt.ylabel('P√©rdida')\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "    print(f\"P√©rdida inicial: {trainer.training_losses[0]:.4f}\")\n",
        "    print(f\"P√©rdida final: {trainer.training_losses[-1]:.4f}\")\n",
        "\n",
        "#19 Crear pipeline de inferencia\n",
        "print(\"\\nü§ñ Creando pipeline de inferencia...\")\n",
        "sentiment_pipeline = pipeline(\n",
        "    \"text-classification\",\n",
        "    model=model_path,\n",
        "    tokenizer=model_path,\n",
        "    device=0 if torch.cuda.is_available() else -1\n",
        ")\n",
        "\n",
        "#20 Funci√≥n de interpretaci√≥n\n",
        "def interpretar_sentimiento(resultado):\n",
        "    \"\"\"Convierte resultado num√©rico a texto legible\"\"\"\n",
        "    # Pipeline devuelve LABEL_0, LABEL_1, etc.\n",
        "    if resultado['label'].startswith('LABEL_'):\n",
        "        label_num = int(resultado['label'].split('_')[1])\n",
        "\n",
        "        # Mapear a estrellas (0-indexado a 1-5 estrellas)\n",
        "        estrellas = label_num + 1\n",
        "\n",
        "        if estrellas == 1:\n",
        "            sentimiento = \"‚≠ê Muy Negativo\"\n",
        "        elif estrellas == 2:\n",
        "            sentimiento = \"‚≠ê‚≠ê Negativo\"\n",
        "        elif estrellas == 3:\n",
        "            sentimiento = \"‚≠ê‚≠ê‚≠ê Neutral\"\n",
        "        elif estrellas == 4:\n",
        "            sentimiento = \"‚≠ê‚≠ê‚≠ê‚≠ê Positivo\"\n",
        "        elif estrellas == 5:\n",
        "            sentimiento = \"‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Muy Positivo\"\n",
        "        else:\n",
        "            sentimiento = f\"Desconocido ({estrellas} estrellas)\"\n",
        "    else:\n",
        "        sentimiento = resultado['label']\n",
        "\n",
        "    return {\n",
        "        'sentimiento': sentimiento,\n",
        "        'confianza': resultado['score']\n",
        "    }\n",
        "\n",
        "#21 Probar modelo con ejemplos\n",
        "test_reviews = [\n",
        "    \"Me encanta este producto, es excelente y funciona perfectamente\",\n",
        "    \"Terrible compra, no funciona y es de muy mala calidad\",\n",
        "    \"El producto est√° bien, nada espectacular pero cumple su funci√≥n\",\n",
        "    \"¬°Incre√≠ble! Super√≥ todas mis expectativas, lo recomiendo totalmente\",\n",
        "    \"No me gust√≥ para nada, perd√≠ mi dinero con esta compra\",\n",
        "    \"Producto decente por el precio que tiene\"\n",
        "]\n",
        "\n",
        "print(\"\\nüß™ PROBANDO MODELO:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "for i, review in enumerate(test_reviews, 1):\n",
        "    resultado = sentiment_pipeline(review)[0]\n",
        "    interpretado = interpretar_sentimiento(resultado)\n",
        "\n",
        "    print(f\"{i}. Review: \\\"{review[:60]}{'...' if len(review) > 60 else ''}\\\"\")\n",
        "    print(f\"   Sentimiento: {interpretado['sentimiento']}\")\n",
        "    print(f\"   Confianza: {interpretado['confianza']:.3f}\")\n",
        "    print()\n",
        "\n",
        "#22 Funci√≥n de inferencia personalizada (alternativa)\n",
        "def analizar_review(texto):\n",
        "    \"\"\"Funci√≥n simple para analizar sentimiento de una review\"\"\"\n",
        "    resultado = sentiment_pipeline(texto)[0]\n",
        "    interpretado = interpretar_sentimiento(resultado)\n",
        "\n",
        "    print(f\"üìù Review: \\\"{texto}\\\"\")\n",
        "    print(f\"üéØ Sentimiento: {interpretado['sentimiento']}\")\n",
        "    print(f\"üìä Confianza: {interpretado['confianza']:.1%}\")\n",
        "\n",
        "    return interpretado\n",
        "\n",
        "print(\"\\nüîç Ejemplo de uso de funci√≥n personalizada:\")\n",
        "analizar_review(\"Este dispositivo es fant√°stico, me encanta usarlo todos los d√≠as\")\n",
        "\n",
        "print(f\"\\n‚úÖ FINE-TUNING COMPLETADO EXITOSAMENTE!\")\n",
        "print(f\"üìÅ Modelo guardado en: {model_path}\")\n",
        "print(f\"üéØ Accuracy final: {results.get('eval_accuracy', 'N/A'):.4f}\")\n",
        "print(f\"‚ö° Listo para usar en producci√≥n!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# RAG SIMPLE CON GROQ API\n",
        "# Sistema de b√∫squeda en documentos usando TF-IDF y Groq para generaci√≥n\n",
        "\n",
        "# Instalaci√≥n de dependencias\n",
        "!pip install groq scikit-learn PyPDF2 python-docx pandas openpyxl -q\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from groq import Groq\n",
        "from google.colab import userdata\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from PyPDF2 import PdfReader\n",
        "from docx import Document\n",
        "\n",
        "# Configuraci√≥n del sistema\n",
        "GROQ_API_KEY = userdata.get('GROQ_KEY')\n",
        "client = Groq(api_key=GROQ_API_KEY)\n",
        "folder_path = '/content/carpeta_rag'\n",
        "\n",
        "# Variables globales del sistema\n",
        "vectorizer = TfidfVectorizer(max_features=1000, stop_words=None)\n",
        "chunks = []\n",
        "vectors = None\n",
        "\n",
        "def load_documents(folder_path):\n",
        "    \"\"\"Carga y procesa documentos de m√∫ltiples formatos\"\"\"\n",
        "    text = ''\n",
        "    os.makedirs(folder_path, exist_ok=True)\n",
        "\n",
        "    for filename in os.listdir(folder_path):\n",
        "        filepath = os.path.join(folder_path, filename)\n",
        "\n",
        "        try:\n",
        "            if filename.endswith('.txt'):\n",
        "                with open(filepath, 'r', encoding='utf-8') as f:\n",
        "                    text += f.read() + '\\n'\n",
        "\n",
        "            elif filename.endswith('.pdf'):\n",
        "                reader = PdfReader(filepath)\n",
        "                for page in reader.pages:\n",
        "                    text += page.extract_text() + '\\n'\n",
        "\n",
        "            elif filename.endswith('.docx'):\n",
        "                doc = Document(filepath)\n",
        "                for paragraph in doc.paragraphs:\n",
        "                    text += paragraph.text + '\\n'\n",
        "\n",
        "            elif filename.endswith('.csv'):\n",
        "                df = pd.read_csv(filepath)\n",
        "                text += df.to_string() + '\\n'\n",
        "\n",
        "            elif filename.endswith('.xlsx'):\n",
        "                df = pd.read_excel(filepath)\n",
        "                text += df.to_string() + '\\n'\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "    return text\n",
        "\n",
        "def create_chunks(text, chunk_size=500):\n",
        "    \"\"\"Divide el texto en fragmentos de tama√±o manejable\"\"\"\n",
        "    words = text.split()\n",
        "    chunks = []\n",
        "\n",
        "    for i in range(0, len(words), chunk_size):\n",
        "        chunk = ' '.join(words[i:i + chunk_size])\n",
        "        if len(chunk.strip()) > 100:\n",
        "            chunks.append(chunk.strip())\n",
        "\n",
        "    return chunks\n",
        "\n",
        "def search_relevant_chunks(query, top_k=3):\n",
        "    \"\"\"Encuentra fragmentos m√°s relevantes usando similitud coseno\"\"\"\n",
        "    query_vector = vectorizer.transform([query])\n",
        "    similarities = cosine_similarity(query_vector, vectors)[0]\n",
        "\n",
        "    # Obtener √≠ndices ordenados por relevancia\n",
        "    top_indices = np.argsort(similarities)[-top_k:][::-1]\n",
        "\n",
        "    relevant_chunks = []\n",
        "    for idx in top_indices:\n",
        "        if similarities[idx] > 0.1:  # Umbral m√≠nimo de relevancia\n",
        "            relevant_chunks.append(chunks[idx])\n",
        "\n",
        "    return relevant_chunks\n",
        "\n",
        "def generate_answer(query, context):\n",
        "    \"\"\"Genera respuesta usando Groq API con el contexto encontrado\"\"\"\n",
        "    if not context:\n",
        "        return \"No encontr√© informaci√≥n relevante en los documentos.\"\n",
        "\n",
        "    prompt = f\"\"\"Bas√°ndote en esta informaci√≥n:\n",
        "\n",
        "{context}\n",
        "\n",
        "Pregunta: {query}\n",
        "\n",
        "Instrucciones:\n",
        "- Responde solo con informaci√≥n del contexto\n",
        "- Si no hay informaci√≥n suficiente, di que no puedes responder\n",
        "- Responde en espa√±ol de forma clara y concisa\"\"\"\n",
        "\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"llama3-8b-8192\",\n",
        "            messages=[\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ],\n",
        "            max_tokens=400,\n",
        "            temperature=0.7\n",
        "        )\n",
        "        return response.choices[0].message.content\n",
        "    except Exception as e:\n",
        "        return f\"Error generando respuesta: {e}\"\n",
        "\n",
        "def inicializar_rag():\n",
        "    \"\"\"Inicializa el sistema RAG cargando documentos y creando √≠ndices\"\"\"\n",
        "    global chunks, vectors, vectorizer\n",
        "\n",
        "    print(\"Cargando documentos...\")\n",
        "    documents = load_documents(folder_path)\n",
        "\n",
        "    if not documents.strip():\n",
        "        print(\"No se encontraron documentos en la carpeta.\")\n",
        "        return False\n",
        "\n",
        "    print(\"Creando fragmentos de texto...\")\n",
        "    chunks = create_chunks(documents)\n",
        "\n",
        "    print(\"Generando √≠ndice de b√∫squeda...\")\n",
        "    vectors = vectorizer.fit_transform(chunks)\n",
        "\n",
        "    print(f\"Sistema listo: {len(chunks)} fragmentos procesados.\")\n",
        "    return True\n",
        "\n",
        "# Inicializaci√≥n del sistema\n",
        "inicializar_rag()\n",
        "\n",
        "# BLOQUE DE INFERENCIA - Ejecutar por separado\n",
        "query = \"¬øDe qu√© tratan estos documentos?\"  # Cambia tu pregunta aqu√≠\n",
        "context = '\\n\\n'.join(search_relevant_chunks(query))\n",
        "respuesta = client.chat.completions.create(model=\"llama3-8b-8192\", messages=[{\"role\": \"user\", \"content\": f\"Contexto: {context}\\nPregunta: {query}\\nResponde solo con informaci√≥n del contexto en espa√±ol.\"}], max_tokens=400).choices[0].message.content\n",
        "print(query)\n",
        "print(respuesta)"
      ],
      "metadata": {
        "id": "YhL3CdQ3vvlu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RAG BASE DE DATOS CON GROQ API\n",
        "\n",
        "\n",
        "# ===== INSTALACI√ìN =====\n",
        "!pip install groq sentence-transformers pandas numpy scikit-learn -q\n",
        "\n",
        "# ===== C√ìDIGO RAG MINIMALISTA =====\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sqlite3\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from groq import Groq\n",
        "from google.colab import userdata\n",
        "\n",
        "class RAGSimple:\n",
        "    def __init__(self):\n",
        "        print(\"Iniciando RAG...\")\n",
        "        self.model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "        self.conn = None\n",
        "        self.table_name = 'data'\n",
        "        self.column_embeddings = {}\n",
        "        self.column_types = {}\n",
        "\n",
        "    def cargar_datos(self, csv_url):\n",
        "        \"\"\"Cargar CSV y crear base de datos\"\"\"\n",
        "        print(\"Cargando datos...\")\n",
        "        df = pd.read_csv(csv_url)\n",
        "        self.conn = sqlite3.connect(':memory:')\n",
        "        df.to_sql(self.table_name, self.conn, index=False, if_exists='replace')\n",
        "\n",
        "        # Analizar columnas\n",
        "        for col in df.columns:\n",
        "            if df[col].dtype in ['int64', 'float64']:\n",
        "                self.column_types[col] = 'numeric'\n",
        "                desc = f\"{col} valores num√©ricos entre {df[col].min()} y {df[col].max()}\"\n",
        "            elif df[col].nunique() <= 20:\n",
        "                self.column_types[col] = 'categorical'\n",
        "                valores = ', '.join(map(str, df[col].unique()[:5]))\n",
        "                desc = f\"{col} categor√≠as como: {valores}\"\n",
        "            else:\n",
        "                self.column_types[col] = 'text'\n",
        "                desc = f\"{col} texto libre\"\n",
        "\n",
        "            self.column_embeddings[col] = self.model.encode([desc])[0]\n",
        "\n",
        "        print(f\"‚úì Datos cargados: {df.shape[0]} filas, {df.shape[1]} columnas\")\n",
        "        return df.shape\n",
        "\n",
        "    def encontrar_columnas(self, pregunta):\n",
        "        \"\"\"Encontrar columnas relevantes\"\"\"\n",
        "        pregunta_emb = self.model.encode([pregunta])[0]\n",
        "        scores = []\n",
        "\n",
        "        for col, col_emb in self.column_embeddings.items():\n",
        "            sim = cosine_similarity([pregunta_emb], [col_emb])[0][0]\n",
        "            scores.append((col, sim))\n",
        "\n",
        "        return sorted(scores, key=lambda x: x[1], reverse=True)[:3]\n",
        "\n",
        "    def generar_sql(self, pregunta):\n",
        "        \"\"\"Generar SQL inteligente\"\"\"\n",
        "        cols_relevantes = [col for col, _ in self.encontrar_columnas(pregunta)]\n",
        "        cols_numericas = [c for c in cols_relevantes if self.column_types.get(c) == 'numeric']\n",
        "        cols_categoricas = [c for c in cols_relevantes if self.column_types.get(c) == 'categorical']\n",
        "\n",
        "        pregunta_lower = pregunta.lower()\n",
        "\n",
        "        # Detecci√≥n de intenci√≥n mejorada\n",
        "        if any(palabra in pregunta_lower for palabra in ['m√°s vendido', 'm√°s popular', 'qu√© producto', 'cu√°les productos']):\n",
        "            # Productos m√°s vendidos por cantidad\n",
        "            if cols_numericas and cols_categoricas:\n",
        "                return f\"\"\"\n",
        "                SELECT {cols_categoricas[0]} as producto,\n",
        "                       SUM({cols_numericas[0]}) as total_cantidad,\n",
        "                       COUNT(*) as num_ventas\n",
        "                FROM {self.table_name}\n",
        "                GROUP BY {cols_categoricas[0]}\n",
        "                ORDER BY total_cantidad DESC\n",
        "                LIMIT 10\n",
        "                \"\"\"\n",
        "\n",
        "        if any(palabra in pregunta_lower for palabra in ['m√©todo pago', 'forma pago', 'c√≥mo pagan', 'pago m√°s com√∫n']):\n",
        "            # An√°lisis de m√©todos de pago\n",
        "            return f\"\"\"\n",
        "            SELECT M√©todo_pago,\n",
        "                   COUNT(*) as cantidad,\n",
        "                   ROUND(COUNT(*) * 100.0 / (SELECT COUNT(*) FROM {self.table_name}), 1) as porcentaje\n",
        "            FROM {self.table_name}\n",
        "            GROUP BY M√©todo_pago\n",
        "            ORDER BY cantidad DESC\n",
        "            \"\"\"\n",
        "\n",
        "        if any(palabra in pregunta_lower for palabra in ['ingresos', 'revenue', 'dinero', 'ganancias']):\n",
        "            # An√°lisis de ingresos\n",
        "            if 'Precio_unitario' in self.column_types and 'Cantidad' in self.column_types:\n",
        "                return f\"\"\"\n",
        "                SELECT {cols_categoricas[0] if cols_categoricas else 'Producto'} as categoria,\n",
        "                       SUM(Precio_unitario * Cantidad) as total_ingresos,\n",
        "                       AVG(Precio_unitario * Cantidad) as ingreso_promedio\n",
        "                FROM {self.table_name}\n",
        "                GROUP BY {cols_categoricas[0] if cols_categoricas else 'Producto'}\n",
        "                ORDER BY total_ingresos DESC\n",
        "                LIMIT 10\n",
        "                \"\"\"\n",
        "\n",
        "        if any(palabra in pregunta_lower for palabra in ['ciudad', 'sucursal', 'd√≥nde', 'ubicaci√≥n']):\n",
        "            # An√°lisis por ubicaci√≥n\n",
        "            return f\"\"\"\n",
        "            SELECT Sucursal,\n",
        "                   COUNT(*) as num_ventas,\n",
        "                   SUM(Precio_unitario * Cantidad) as total_ingresos\n",
        "            FROM {self.table_name}\n",
        "            GROUP BY Sucursal\n",
        "            ORDER BY num_ventas DESC\n",
        "            \"\"\"\n",
        "\n",
        "        if any(palabra in pregunta_lower for palabra in ['promedio', 'precio promedio', 'categor√≠a']):\n",
        "            # An√°lisis de precios por categor√≠a\n",
        "            return f\"\"\"\n",
        "            SELECT Categor√≠a,\n",
        "                   COUNT(*) as num_productos,\n",
        "                   AVG(Precio_unitario) as precio_promedio,\n",
        "                   MIN(Precio_unitario) as precio_min,\n",
        "                   MAX(Precio_unitario) as precio_max\n",
        "            FROM {self.table_name}\n",
        "            GROUP BY Categor√≠a\n",
        "            ORDER BY precio_promedio DESC\n",
        "            \"\"\"\n",
        "\n",
        "        # Query general mejorada\n",
        "        select_cols = ', '.join(cols_relevantes[:3]) if cols_relevantes else '*'\n",
        "        return f\"SELECT {select_cols} FROM {self.table_name} LIMIT 15\"\n",
        "\n",
        "    def ejecutar_consulta(self, sql):\n",
        "        \"\"\"Ejecutar SQL\"\"\"\n",
        "        try:\n",
        "            sql_limpio = ' '.join(line.strip() for line in sql.strip().split('\\n') if line.strip())\n",
        "            resultado = pd.read_sql_query(sql_limpio, self.conn)\n",
        "            return sql_limpio, resultado\n",
        "        except Exception as e:\n",
        "            print(f\"Error SQL: {e}\")\n",
        "            sql_simple = f\"SELECT * FROM {self.table_name} LIMIT 10\"\n",
        "            resultado = pd.read_sql_query(sql_simple, self.conn)\n",
        "            return sql_simple, resultado\n",
        "\n",
        "    def responder(self, pregunta, usar_groq=True):\n",
        "        \"\"\"Responder pregunta completa\"\"\"\n",
        "        try:\n",
        "            # Generar y ejecutar SQL\n",
        "            sql = self.generar_sql(pregunta)\n",
        "            sql_ejecutado, datos = self.ejecutar_consulta(sql)\n",
        "\n",
        "            print(f\"SQL ejecutado: {sql_ejecutado}\")\n",
        "            print(f\"Resultados: {len(datos)} filas\\n\")\n",
        "\n",
        "            # Crear contexto\n",
        "            contexto = f\"\"\"Pregunta: {pregunta}\n",
        "SQL ejecutado: {sql_ejecutado}\n",
        "Resultados: {len(datos)} filas encontradas\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "            if not datos.empty:\n",
        "                # Estad√≠sticas autom√°ticas\n",
        "                cols_numericas = datos.select_dtypes(include=[np.number]).columns\n",
        "                if len(cols_numericas) > 0:\n",
        "                    contexto += \"Estad√≠sticas calculadas:\\n\"\n",
        "                    for col in cols_numericas[:3]:\n",
        "                        contexto += f\"- {col}: total={datos[col].sum():.2f}, promedio={datos[col].mean():.2f}, m√°ximo={datos[col].max():.2f}\\n\"\n",
        "                    contexto += \"\\n\"\n",
        "\n",
        "                contexto += \"Datos encontrados:\\n\"\n",
        "                contexto += datos.to_string(index=False)\n",
        "\n",
        "            # Respuesta con Groq si est√° disponible\n",
        "            if usar_groq:\n",
        "                try:\n",
        "                    groq_key = userdata.get('GROQ_KEY')\n",
        "                    client = Groq(api_key=groq_key)\n",
        "\n",
        "                    response = client.chat.completions.create(\n",
        "                        model=\"llama3-8b-8192\",\n",
        "                        messages=[{\n",
        "                            \"role\": \"user\",\n",
        "                            \"content\": f\"\"\"Analiza estos datos y responde de forma clara y espec√≠fica:\n",
        "\n",
        "{contexto}\n",
        "\n",
        "Instrucciones:\n",
        "- Responde bas√°ndote √öNICAMENTE en los datos mostrados\n",
        "- Da n√∫meros exactos y estad√≠sticas precisas\n",
        "- Responde en espa√±ol de forma profesional\n",
        "- Si hay un ranking, muestra los top 3-5 elementos\n",
        "\n",
        "Pregunta: {pregunta}\"\"\"\n",
        "                        }],\n",
        "                        max_tokens=400\n",
        "                    )\n",
        "\n",
        "                    return response.choices[0].message.content\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error con Groq: {e}\")\n",
        "                    usar_groq = False\n",
        "\n",
        "            # Respuesta autom√°tica si no hay Groq\n",
        "            if not datos.empty:\n",
        "                respuesta = f\"An√°lisis para: {pregunta}\\n\\n\"\n",
        "                respuesta += f\"Encontr√© {len(datos)} resultados:\\n\\n\"\n",
        "                respuesta += datos.head(10).to_string(index=False)\n",
        "\n",
        "                if len(cols_numericas) > 0:\n",
        "                    respuesta += f\"\\n\\nEstad√≠sticas principales:\\n\"\n",
        "                    for col in cols_numericas[:2]:\n",
        "                        respuesta += f\"- {col}: Total = {datos[col].sum():.2f}, Promedio = {datos[col].mean():.2f}\\n\"\n",
        "\n",
        "                return respuesta\n",
        "            else:\n",
        "                return \"No se encontraron datos para esta consulta.\"\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"Error procesando consulta: {e}\"\n",
        "\n",
        "# ===== FUNCI√ìN DE USO SIMPLE =====\n",
        "def iniciar_rag():\n",
        "    \"\"\"Inicializar RAG con el dataset\"\"\"\n",
        "    rag = RAGSimple()\n",
        "    rag.cargar_datos('https://raw.githubusercontent.com/CamiloVga/Curso-IA-Para-Ciencia-de-Datos/main/datos_tienda_ropa.csv')\n",
        "    return rag\n",
        "\n",
        "def preguntar(rag, pregunta):\n",
        "    \"\"\"Hacer una pregunta al RAG\"\"\"\n",
        "    return rag.responder(pregunta, usar_groq=True)\n",
        "\n",
        "# ===== C√ìDIGO DE PRUEBA =====\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"üöÄ INICIANDO SISTEMA RAG\")\n",
        "    rag = iniciar_rag()\n",
        "\n",
        "    # Preguntas de prueba\n",
        "    preguntas_test = [\n",
        "        \"¬øcu√°les son los productos m√°s vendidos?\",\n",
        "        \"¬øcu√°l es el m√©todo de pago m√°s com√∫n?\",\n",
        "        \"¬øqu√© producto genera m√°s ingresos?\",\n",
        "        \"¬øcu√°l es el precio promedio por categor√≠a?\"\n",
        "    ]\n",
        "\n",
        "    for i, pregunta in enumerate(preguntas_test, 1):\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"PREGUNTA {i}: {pregunta}\")\n",
        "        print('='*60)\n",
        "\n",
        "        respuesta = preguntar(rag, pregunta)\n",
        "        print(respuesta)"
      ],
      "metadata": {
        "id": "c6QnDrLVxcJS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}