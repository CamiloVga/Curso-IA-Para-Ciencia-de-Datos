{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPr2p6CoftbuCx0HPB5tw+A",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CamiloVga/Curso-IA-Para-Ciencia-de-Datos/blob/main/Script_Sesi%C3%B3n_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# IA para la Ciencia de Datos\n",
        "## Universidad de los Andes\n",
        "\n",
        "**Profesor:** Camilo Vega - AI/ML Engineer  \n",
        "**LinkedIn:** https://www.linkedin.com/in/camilo-vega-169084b1/\n",
        "\n",
        "---\n",
        "\n",
        "## Gu√≠a: Fine-tuning y RAG (Retrieval-Augmented Generation)\n",
        "\n",
        "Este notebook presenta **3 implementaciones pr√°cticas**:\n",
        "\n",
        "1. **Fine-tuning B√°sico** - An√°lisis de sentimientos con tweets\n",
        "2. **RAG Simple** - B√∫squeda b√°sica en documentos\n",
        "3. **RAG Base de Datos** - Sistema especializado con embeddings\n",
        "\n",
        "### Requisitos\n",
        "- **GPU:** Tesla T4 m√≠nimo (Colab gratuito)\n",
        "- **APIs:** Hugging Face token\n",
        "- **Datos:** Tweets y dataset de ventas\n",
        "\n",
        "## Configuraci√≥n APIs\n",
        "- **Hugging Face:**\n",
        "  1. [Crear token](https://huggingface.co/settings/tokens)\n",
        "  2. En Colab: üîë Secrets ‚Üí Agregar `HF_TOKEN` ‚Üí Pegar tu token\n",
        "\n",
        "Cada secci√≥n es **independiente** y puede ejecutarse por separado.\n"
      ],
      "metadata": {
        "id": "LZ2dQpaLpth1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Fine Tuning"
      ],
      "metadata": {
        "id": "cWzTVw3kOxU5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2kkpznc1pd0A"
      },
      "outputs": [],
      "source": [
        "# 1. Instalaciones\n",
        "!pip install torch transformers datasets scikit-learn matplotlib seaborn pandas huggingface_hub -q\n",
        "\n",
        "# 2. Imports y configuraci√≥n\n",
        "from google.colab import userdata\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, classification_report, roc_auc_score\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from sklearn.model_selection import train_test_split\n",
        "from huggingface_hub import login\n",
        "\n",
        "# Configuraci√≥n\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_name = \"bert-base-multilingual-cased\"  # Modelo multiling√ºe para ingl√©s\n",
        "\n",
        "# Autenticaci√≥n\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "login(token=hf_token)\n",
        "print(f\"Dispositivo: {device}\")\n",
        "print(f\"Modelo: {model_name} (multiling√ºe para textos en ingl√©s)\")\n",
        "\n",
        "# 3. Cargar datos\n",
        "print(\"Cargando dataset de tweets...\")\n",
        "dataset = load_dataset(\"mteb/tweet_sentiment_extraction\", split=\"train\")\n",
        "\n",
        "textos = dataset[\"text\"]\n",
        "etiquetas = dataset[\"label\"]  # 0=negative, 1=neutral, 2=positive\n",
        "\n",
        "print(f\"Dataset cargado: {len(textos)} muestras\")\n",
        "print(f\"Distribuci√≥n: {pd.Series(etiquetas).value_counts().sort_index().to_dict()}\")\n",
        "\n",
        "# 4. Filtrar y balancear\n",
        "df = pd.DataFrame({'texto': textos, 'etiqueta': etiquetas})\n",
        "\n",
        "# Tomar 5000 ejemplos balanceados\n",
        "df_sample = df.groupby('etiqueta').apply(\n",
        "    lambda x: x.sample(min(1667, len(x)), random_state=42)\n",
        ").reset_index(drop=True).sample(frac=1, random_state=42)\n",
        "\n",
        "textos_final = df_sample['texto'].tolist()\n",
        "etiquetas_final = df_sample['etiqueta'].tolist()\n",
        "\n",
        "print(f\"Datos balanceados: {len(textos_final)} muestras\")\n",
        "print(f\"Distribuci√≥n final: {pd.Series(etiquetas_final).value_counts().sort_index().to_dict()}\")\n",
        "\n",
        "# 5. Split train/test\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    textos_final, etiquetas_final, test_size=0.2, random_state=42, stratify=etiquetas_final\n",
        ")\n",
        "\n",
        "# 6. Modelo y tokenizador\n",
        "print(\"Cargando modelo...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3).to(device)\n",
        "\n",
        "# 7. Tokenizaci√≥n\n",
        "train_encodings = tokenizer(X_train, truncation=True, padding=True, max_length=128, return_tensors=\"pt\")\n",
        "test_encodings = tokenizer(X_test, truncation=True, padding=True, max_length=128, return_tensors=\"pt\")\n",
        "\n",
        "# 8. Dataset clase\n",
        "class SentimentDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "train_dataset = SentimentDataset(train_encodings, y_train)\n",
        "test_dataset = SentimentDataset(test_encodings, y_test)\n",
        "\n",
        "# 9. M√©tricas (DEFINIR ANTES DE USAR)\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    accuracy = accuracy_score(labels, predictions)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='weighted')\n",
        "    return {'accuracy': accuracy, 'f1': f1, 'precision': precision, 'recall': recall}\n",
        "\n",
        "# 10. Configuraci√≥n de entrenamiento (DEFINIR ANTES DE USAR)\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./modelo',\n",
        "    num_train_epochs=5,  # Reducir √©pocas para evitar divergencia\n",
        "    per_device_train_batch_size=32,  # Batch m√°s grande para estabilidad\n",
        "    per_device_eval_batch_size=64,\n",
        "    learning_rate=1e-5,  # Learning rate mucho m√°s bajo\n",
        "    weight_decay=0.1,    # M√°s regularizaci√≥n\n",
        "    warmup_steps=200,\n",
        "    eval_strategy=\"epoch\",\n",
        "    logging_steps=50,\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_accuracy\",  # Optimizar por accuracy\n",
        "    greater_is_better=True,\n",
        "    report_to=[],\n",
        "    dataloader_drop_last=True,\n",
        "    gradient_checkpointing=True,  # Ahorrar memoria\n",
        "    fp16=True  # Precisi√≥n mixta para estabilidad\n",
        ")\n",
        "\n",
        "# 11. Entrenar modelo est√°ndar (AHORA CON VARIABLES DEFINIDAS)\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"ENTRENANDO...\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Entrenar y capturar historial\n",
        "history = trainer.train()\n",
        "\n",
        "# Extraer m√©tricas del historial de logs\n",
        "train_losses = []\n",
        "eval_losses = []\n",
        "eval_accuracies = []\n",
        "\n",
        "for log in trainer.state.log_history:\n",
        "    if 'train_loss' in log:\n",
        "        train_losses.append(log['train_loss'])\n",
        "    if 'eval_loss' in log:\n",
        "        eval_losses.append(log['eval_loss'])\n",
        "    if 'eval_accuracy' in log:\n",
        "        eval_accuracies.append(log['eval_accuracy'])\n",
        "\n",
        "print(f\"Entrenamiento completado. Logs capturados: {len(eval_losses)} evaluaciones\")\n",
        "\n",
        "# 12. Gr√°ficas usando historial de logs\n",
        "if eval_losses:\n",
        "    plt.figure(figsize=(15, 4))\n",
        "    epochs = range(1, len(eval_losses) + 1)\n",
        "\n",
        "    # P√©rdida de evaluaci√≥n\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.plot(epochs, eval_losses, 'r-o', linewidth=2, markersize=6)\n",
        "    plt.title('P√©rdida de Evaluaci√≥n')\n",
        "    plt.xlabel('√âpoca')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # Accuracy\n",
        "    plt.subplot(1, 3, 2)\n",
        "    if eval_accuracies and len(eval_accuracies) == len(eval_losses):\n",
        "        plt.plot(epochs, eval_accuracies, 'g-o', linewidth=2, markersize=6)\n",
        "    plt.title('Accuracy')\n",
        "    plt.xlabel('√âpoca')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # Train loss si est√° disponible\n",
        "    plt.subplot(1, 3, 3)\n",
        "    if train_losses:\n",
        "        # Si hay m√∫ltiples train losses por √©poca, promediarlos\n",
        "        if len(train_losses) > len(epochs):\n",
        "            steps_per_epoch = len(train_losses) // len(epochs)\n",
        "            train_by_epoch = []\n",
        "            for i in range(len(epochs)):\n",
        "                start = i * steps_per_epoch\n",
        "                end = min((i + 1) * steps_per_epoch, len(train_losses))\n",
        "                if start < len(train_losses):\n",
        "                    epoch_avg = sum(train_losses[start:end]) / len(train_losses[start:end])\n",
        "                    train_by_epoch.append(epoch_avg)\n",
        "\n",
        "            if len(train_by_epoch) == len(epochs):\n",
        "                plt.plot(epochs, train_by_epoch, 'b-s', linewidth=2, alpha=0.7, label='Train Loss')\n",
        "                plt.plot(epochs, eval_losses, 'r-o', linewidth=2, label='Eval Loss')\n",
        "                plt.legend()\n",
        "        else:\n",
        "            plt.plot(epochs, eval_losses, 'r-o', linewidth=2)\n",
        "    else:\n",
        "        plt.plot(epochs, eval_losses, 'r-o', linewidth=2)\n",
        "\n",
        "    plt.title('Curvas de P√©rdida')\n",
        "    plt.xlabel('√âpoca')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"\\nRESUMEN ENTRENAMIENTO:\")\n",
        "    if len(eval_losses) > 1:\n",
        "        print(f\"Loss inicial: {eval_losses[0]:.4f}\")\n",
        "        print(f\"Loss final: {eval_losses[-1]:.4f}\")\n",
        "        print(f\"Mejora: {eval_losses[0] - eval_losses[-1]:.4f}\")\n",
        "\n",
        "    if eval_accuracies and len(eval_accuracies) > 1:\n",
        "        print(f\"Accuracy inicial: {eval_accuracies[0]:.4f}\")\n",
        "        print(f\"Accuracy final: {eval_accuracies[-1]:.4f}\")\n",
        "        print(f\"Mejora: {eval_accuracies[-1] - eval_accuracies[0]:.4f}\")\n",
        "\n",
        "else:\n",
        "    print(\"No se encontraron m√©tricas de evaluaci√≥n en el historial\")\n",
        "\n",
        "# 13. Evaluaci√≥n final\n",
        "predictions = trainer.predict(test_dataset)\n",
        "y_pred = np.argmax(predictions.predictions, axis=1)\n",
        "y_pred_proba = torch.softmax(torch.tensor(predictions.predictions), dim=1).numpy()\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"RESULTADOS\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Verificar que las dimensiones coincidan\n",
        "print(f\"Muestras de prueba: {len(y_test)}\")\n",
        "print(f\"Predicciones: {len(y_pred)}\")\n",
        "\n",
        "# Labels names\n",
        "labels_names = ['Negativo', 'Neutral', 'Positivo']\n",
        "\n",
        "# Solo evaluar si las dimensiones coinciden\n",
        "if len(y_test) == len(y_pred):\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"ACCURACY: {accuracy:.4f}\")\n",
        "\n",
        "    # Matriz de confusi√≥n\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=labels_names, yticklabels=labels_names)\n",
        "    plt.title('Matriz de Confusi√≥n')\n",
        "    plt.show()\n",
        "\n",
        "    # ROC-AUC\n",
        "    y_true_bin = label_binarize(y_test, classes=[0, 1, 2])\n",
        "    roc_auc = roc_auc_score(y_true_bin, y_pred_proba, multi_class='ovr', average='weighted')\n",
        "    print(f\"ROC-AUC: {roc_auc:.4f}\")\n",
        "\n",
        "    # Classification report\n",
        "    print(\"\\nREPORTE:\")\n",
        "    print(classification_report(y_test, y_pred, target_names=labels_names))\n",
        "else:\n",
        "    print(f\"Error: Inconsistencia en dimensiones - y_test: {len(y_test)}, y_pred: {len(y_pred)}\")\n",
        "    print(\"Evaluando solo con las primeras muestras que coincidan...\")\n",
        "\n",
        "    min_len = min(len(y_test), len(y_pred))\n",
        "    y_test_truncated = y_test[:min_len]\n",
        "    y_pred_truncated = y_pred[:min_len]\n",
        "\n",
        "    accuracy = accuracy_score(y_test_truncated, y_pred_truncated)\n",
        "    print(f\"ACCURACY (truncado): {accuracy:.4f}\")\n",
        "\n",
        "    # Matriz de confusi√≥n truncada\n",
        "    cm = confusion_matrix(y_test_truncated, y_pred_truncated)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=labels_names, yticklabels=labels_names)\n",
        "    plt.title('Matriz de Confusi√≥n')\n",
        "    plt.show()\n",
        "\n",
        "# 14. Guardar modelo\n",
        "trainer.save_model(\"./sentiment_model\")\n",
        "tokenizer.save_pretrained(\"./sentiment_model\")\n",
        "print(\"Modelo guardado en ./sentiment_model\")\n",
        "\n",
        "# 15. Inferencia individual simple\n",
        "from transformers import pipeline\n",
        "\n",
        "# Cargar el modelo entrenado\n",
        "sentiment_pipeline = pipeline(\"text-classification\", model=\"./sentiment_model\", tokenizer=\"./sentiment_model\")\n",
        "\n",
        "# Ejemplo de uso: cambiar el texto aqu√≠ para probar\n",
        "texto_prueba = \"I love this amazing product!\"\n",
        "\n",
        "resultado = sentiment_pipeline(texto_prueba)[0]\n",
        "label_idx = int(resultado['label'].split('_')[1])\n",
        "sentimientos = ['Negativo', 'Neutral', 'Positivo']\n",
        "\n",
        "print(\"\\n\" + \"=\"*40)\n",
        "print(\"INFERENCIA INDIVIDUAL\")\n",
        "print(\"=\"*40)\n",
        "print(f\"Texto: '{texto_prueba}'\")\n",
        "print(f\"Sentimiento: {sentimientos[label_idx]}\")\n",
        "print(f\"Confianza: {resultado['score']:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#RAG con documentos"
      ],
      "metadata": {
        "id": "RuKAxobgOz-p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# RAG SIMPLE CON GROQ API\n",
        "# Sistema de b√∫squeda en documentos usando TF-IDF y Groq para generaci√≥n\n",
        "\n",
        "# Instalaci√≥n de dependencias\n",
        "!pip install groq scikit-learn PyPDF2 python-docx pandas openpyxl -q\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from groq import Groq\n",
        "from google.colab import userdata\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from PyPDF2 import PdfReader\n",
        "from docx import Document\n",
        "\n",
        "# Configuraci√≥n del sistema\n",
        "GROQ_API_KEY = userdata.get('GROQ_KEY')\n",
        "client = Groq(api_key=GROQ_API_KEY)\n",
        "folder_path = '/content/carpeta_rag'\n",
        "\n",
        "# Variables globales del sistema\n",
        "vectorizer = TfidfVectorizer(max_features=1000, stop_words=None)\n",
        "chunks = []\n",
        "vectors = None\n",
        "\n",
        "def load_documents(folder_path):\n",
        "    \"\"\"Carga y procesa documentos de m√∫ltiples formatos\"\"\"\n",
        "    text = ''\n",
        "    os.makedirs(folder_path, exist_ok=True)\n",
        "\n",
        "    for filename in os.listdir(folder_path):\n",
        "        filepath = os.path.join(folder_path, filename)\n",
        "\n",
        "        try:\n",
        "            if filename.endswith('.txt'):\n",
        "                with open(filepath, 'r', encoding='utf-8') as f:\n",
        "                    text += f.read() + '\\n'\n",
        "\n",
        "            elif filename.endswith('.pdf'):\n",
        "                reader = PdfReader(filepath)\n",
        "                for page in reader.pages:\n",
        "                    text += page.extract_text() + '\\n'\n",
        "\n",
        "            elif filename.endswith('.docx'):\n",
        "                doc = Document(filepath)\n",
        "                for paragraph in doc.paragraphs:\n",
        "                    text += paragraph.text + '\\n'\n",
        "\n",
        "            elif filename.endswith('.csv'):\n",
        "                df = pd.read_csv(filepath)\n",
        "                text += df.to_string() + '\\n'\n",
        "\n",
        "            elif filename.endswith('.xlsx'):\n",
        "                df = pd.read_excel(filepath)\n",
        "                text += df.to_string() + '\\n'\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "    return text\n",
        "\n",
        "def create_chunks(text, chunk_size=500):\n",
        "    \"\"\"Divide el texto en fragmentos de tama√±o manejable\"\"\"\n",
        "    words = text.split()\n",
        "    chunks = []\n",
        "\n",
        "    for i in range(0, len(words), chunk_size):\n",
        "        chunk = ' '.join(words[i:i + chunk_size])\n",
        "        if len(chunk.strip()) > 100:\n",
        "            chunks.append(chunk.strip())\n",
        "\n",
        "    return chunks\n",
        "\n",
        "def search_relevant_chunks(query, top_k=3):\n",
        "    \"\"\"Encuentra fragmentos m√°s relevantes usando similitud coseno\"\"\"\n",
        "    query_vector = vectorizer.transform([query])\n",
        "    similarities = cosine_similarity(query_vector, vectors)[0]\n",
        "\n",
        "    # Obtener √≠ndices ordenados por relevancia\n",
        "    top_indices = np.argsort(similarities)[-top_k:][::-1]\n",
        "\n",
        "    relevant_chunks = []\n",
        "    for idx in top_indices:\n",
        "        if similarities[idx] > 0.1:  # Umbral m√≠nimo de relevancia\n",
        "            relevant_chunks.append(chunks[idx])\n",
        "\n",
        "    return relevant_chunks\n",
        "\n",
        "def generate_answer(query, context):\n",
        "    \"\"\"Genera respuesta usando Groq API con el contexto encontrado\"\"\"\n",
        "    if not context:\n",
        "        return \"No encontr√© informaci√≥n relevante en los documentos.\"\n",
        "\n",
        "    prompt = f\"\"\"Bas√°ndote en esta informaci√≥n:\n",
        "\n",
        "{context}\n",
        "\n",
        "Pregunta: {query}\n",
        "\n",
        "Instrucciones:\n",
        "- Responde solo con informaci√≥n del contexto\n",
        "- Si no hay informaci√≥n suficiente, di que no puedes responder\n",
        "- Responde en espa√±ol de forma clara y concisa\"\"\"\n",
        "\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"llama3-8b-8192\",\n",
        "            messages=[\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ],\n",
        "            max_tokens=400,\n",
        "            temperature=0.7\n",
        "        )\n",
        "        return response.choices[0].message.content\n",
        "    except Exception as e:\n",
        "        return f\"Error generando respuesta: {e}\"\n",
        "\n",
        "def inicializar_rag():\n",
        "    \"\"\"Inicializa el sistema RAG cargando documentos y creando √≠ndices\"\"\"\n",
        "    global chunks, vectors, vectorizer\n",
        "\n",
        "    print(\"Cargando documentos...\")\n",
        "    documents = load_documents(folder_path)\n",
        "\n",
        "    if not documents.strip():\n",
        "        print(\"No se encontraron documentos en la carpeta.\")\n",
        "        return False\n",
        "\n",
        "    print(\"Creando fragmentos de texto...\")\n",
        "    chunks = create_chunks(documents)\n",
        "\n",
        "    print(\"Generando √≠ndice de b√∫squeda...\")\n",
        "    vectors = vectorizer.fit_transform(chunks)\n",
        "\n",
        "    print(f\"Sistema listo: {len(chunks)} fragmentos procesados.\")\n",
        "    return True\n",
        "\n",
        "# Inicializaci√≥n del sistema\n",
        "inicializar_rag()\n",
        "\n",
        "# BLOQUE DE INFERENCIA - Ejecutar por separado\n",
        "query = \"¬øDe qu√© tratan estos documentos?\"  # Cambia tu pregunta aqu√≠\n",
        "context = '\\n\\n'.join(search_relevant_chunks(query))\n",
        "respuesta = client.chat.completions.create(model=\"llama3-8b-8192\", messages=[{\"role\": \"user\", \"content\": f\"Contexto: {context}\\nPregunta: {query}\\nResponde solo con informaci√≥n del contexto en espa√±ol.\"}], max_tokens=400).choices[0].message.content\n",
        "print(query)\n",
        "print(respuesta)"
      ],
      "metadata": {
        "id": "YhL3CdQ3vvlu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Rag Base de Datos\n"
      ],
      "metadata": {
        "id": "AuxeC36BO4y7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# RAG BASE DE DATOS CON GROQ API\n",
        "# Sistema de consultas en bases de datos usando embeddings y Groq para generaci√≥n\n",
        "# DIFERENCIAS vs RAG normal: embeddings de metadatos + generaci√≥n autom√°tica de SQL\n",
        "\n",
        "\"\"\"\n",
        "L√ìGICA DEL RAG BASE DE DATOS:\n",
        "\n",
        "1. Embedding busca: columnas similares (no datos raw)\n",
        "2. Patr√≥n detecta: tipo de consulta en lenguaje natural\n",
        "3. SQL generado: c√≥digo ejecutable basado en patrones + columnas\n",
        "4. Resultado: datos reales de la base de datos\n",
        "5. LLM: respuesta natural con estad√≠sticas precisas\n",
        "\n",
        "Ejemplo flujo:\n",
        "Input: \"¬øqu√© m√©todo de pago prefiere la gente?\"\n",
        "1. Embedding busca: \"M√©todo_pago\" (alta similitud)\n",
        "2. Patr√≥n detecta: \"preferir\" ‚Üí GROUP BY + COUNT\n",
        "3. SQL generado: SELECT M√©todo_pago, COUNT(*) FROM data GROUP BY M√©todo_pago\n",
        "4. Resultado: Tarjeta: 150, Efectivo: 89\n",
        "5. LLM: \"La gente prefiere tarjeta (150 transacciones vs 89 en efectivo)\"\n",
        "\"\"\"\n",
        "\n",
        "# Instalaci√≥n de dependencias\n",
        "!pip install groq sentence-transformers pandas numpy scikit-learn -q\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sqlite3\n",
        "import os\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from groq import Groq\n",
        "from google.colab import userdata\n",
        "\n",
        "# Configuraci√≥n del sistema\n",
        "GROQ_API_KEY = userdata.get('GROQ_KEY')\n",
        "client = Groq(api_key=GROQ_API_KEY)\n",
        "folder_path = '/content/carpeta_rag'\n",
        "\n",
        "# Variables globales del sistema\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "conn = None\n",
        "table_name = 'data'\n",
        "column_embeddings = {}  # Embeddings de metadatos de columnas (no datos raw)\n",
        "column_types = {}       # Tipos de datos para generar SQL inteligente\n",
        "\n",
        "def load_database(folder_path):\n",
        "    \"\"\"Carga CSV desde carpeta y crea base de datos en memoria\"\"\"\n",
        "    global conn, column_embeddings, column_types\n",
        "\n",
        "    os.makedirs(folder_path, exist_ok=True)\n",
        "\n",
        "    # Buscar archivo CSV en la carpeta\n",
        "    csv_file = None\n",
        "    try:\n",
        "        files = os.listdir(folder_path)\n",
        "        print(f\"Archivos en carpeta: {files}\")\n",
        "\n",
        "        for filename in files:\n",
        "            if filename.endswith('.csv'):\n",
        "                csv_file = os.path.join(folder_path, filename)\n",
        "                break\n",
        "\n",
        "        if not csv_file:\n",
        "            # Cargar datos de ejemplo si no hay CSV\n",
        "            print(\"No se encontr√≥ CSV. Creando datos de ejemplo...\")\n",
        "            data = {\n",
        "                'Producto': ['Camiseta', 'Pantal√≥n', 'Zapatos', 'Chaqueta'],\n",
        "                'Categor√≠a': ['Ropa', 'Ropa', 'Calzado', 'Ropa'],\n",
        "                'Precio_unitario': [25.99, 45.50, 89.99, 120.00],\n",
        "                'Cantidad': [15, 8, 12, 5],\n",
        "                'M√©todo_pago': ['Efectivo', 'Tarjeta', 'Efectivo', 'Tarjeta'],\n",
        "                'Sucursal': ['Norte', 'Sur', 'Norte', 'Centro']\n",
        "            }\n",
        "            df = pd.DataFrame(data)\n",
        "        else:\n",
        "            print(f\"Cargando {csv_file}...\")\n",
        "            df = pd.read_csv(csv_file)\n",
        "\n",
        "        # Crear conexi√≥n SQLite\n",
        "        if conn:\n",
        "            conn.close()\n",
        "        conn = sqlite3.connect(':memory:', check_same_thread=False)\n",
        "        df.to_sql(table_name, conn, index=False, if_exists='replace')\n",
        "\n",
        "        # Limpiar variables anteriores\n",
        "        column_embeddings.clear()\n",
        "        column_types.clear()\n",
        "\n",
        "        # Analizar columnas y crear embeddings\n",
        "        # CLAVE: No embebemos los datos, sino descripciones de las columnas\n",
        "        # Esto permite mapear preguntas a columnas relevantes de la DB\n",
        "        for col in df.columns:\n",
        "            if df[col].dtype in ['int64', 'float64']:\n",
        "                column_types[col] = 'numeric'\n",
        "                desc = f\"{col} valores num√©ricos entre {df[col].min()} y {df[col].max()}\"\n",
        "            elif df[col].nunique() <= 20:\n",
        "                column_types[col] = 'categorical'\n",
        "                valores = ', '.join(map(str, df[col].unique()[:5]))\n",
        "                desc = f\"{col} categor√≠as como: {valores}\"\n",
        "            else:\n",
        "                column_types[col] = 'text'\n",
        "                desc = f\"{col} texto libre\"\n",
        "\n",
        "            # Embedding de la descripci√≥n de la columna, no del contenido\n",
        "            column_embeddings[col] = model.encode([desc])[0]\n",
        "\n",
        "        print(f\"Base de datos cargada: {df.shape[0]} filas, {df.shape[1]} columnas\")\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error cargando datos: {e}\")\n",
        "        return False\n",
        "\n",
        "def find_relevant_columns(query, top_k=3):\n",
        "    \"\"\"Encuentra columnas m√°s relevantes usando similitud coseno\n",
        "    Diferencia clave vs RAG normal: buscamos columnas, no fragmentos de texto\"\"\"\n",
        "    query_emb = model.encode([query])[0]\n",
        "    scores = []\n",
        "\n",
        "    for col, col_emb in column_embeddings.items():\n",
        "        sim = cosine_similarity([query_emb], [col_emb])[0][0]\n",
        "        scores.append((col, sim))\n",
        "\n",
        "    return sorted(scores, key=lambda x: x[1], reverse=True)[:top_k]\n",
        "\n",
        "def generate_sql(query):\n",
        "    \"\"\"Genera consulta SQL inteligente basada en la pregunta\n",
        "    INNOVACI√ìN: Combina embeddings + detecci√≥n de patrones para SQL autom√°tico\n",
        "    RAG tradicional solo busca texto, aqu√≠ generamos c√≥digo ejecutable\"\"\"\n",
        "    cols_relevantes = [col for col, _ in find_relevant_columns(query)]\n",
        "    cols_numericas = [c for c in cols_relevantes if column_types.get(c) == 'numeric']\n",
        "    cols_categoricas = [c for c in cols_relevantes if column_types.get(c) == 'categorical']\n",
        "\n",
        "    query_lower = query.lower()\n",
        "\n",
        "    # Detecci√≥n de patrones en la pregunta\n",
        "    if any(palabra in query_lower for palabra in ['m√°s vendido', 'm√°s popular', 'qu√© producto', 'cu√°les productos']):\n",
        "        if cols_numericas and cols_categoricas:\n",
        "            return f\"\"\"\n",
        "            SELECT {cols_categoricas[0]} as producto,\n",
        "                   SUM({cols_numericas[0]}) as total_cantidad,\n",
        "                   COUNT(*) as num_ventas\n",
        "            FROM {table_name}\n",
        "            GROUP BY {cols_categoricas[0]}\n",
        "            ORDER BY total_cantidad DESC\n",
        "            LIMIT 10\n",
        "            \"\"\"\n",
        "\n",
        "    if any(palabra in query_lower for palabra in ['m√©todo pago', 'forma pago', 'c√≥mo pagan', 'pago m√°s com√∫n']):\n",
        "        return f\"\"\"\n",
        "        SELECT M√©todo_pago,\n",
        "               COUNT(*) as cantidad,\n",
        "               ROUND(COUNT(*) * 100.0 / (SELECT COUNT(*) FROM {table_name}), 1) as porcentaje\n",
        "        FROM {table_name}\n",
        "        GROUP BY M√©todo_pago\n",
        "        ORDER BY cantidad DESC\n",
        "        \"\"\"\n",
        "\n",
        "    if any(palabra in query_lower for palabra in ['ingresos', 'revenue', 'dinero', 'ganancias']):\n",
        "        if 'Precio_unitario' in column_types and 'Cantidad' in column_types:\n",
        "            return f\"\"\"\n",
        "            SELECT {cols_categoricas[0] if cols_categoricas else 'Producto'} as categoria,\n",
        "                   SUM(Precio_unitario * Cantidad) as total_ingresos,\n",
        "                   AVG(Precio_unitario * Cantidad) as ingreso_promedio\n",
        "            FROM {table_name}\n",
        "            GROUP BY {cols_categoricas[0] if cols_categoricas else 'Producto'}\n",
        "            ORDER BY total_ingresos DESC\n",
        "            LIMIT 10\n",
        "            \"\"\"\n",
        "\n",
        "    if any(palabra in query_lower for palabra in ['ciudad', 'sucursal', 'd√≥nde', 'ubicaci√≥n']):\n",
        "        return f\"\"\"\n",
        "        SELECT Sucursal,\n",
        "               COUNT(*) as num_ventas,\n",
        "               SUM(Precio_unitario * Cantidad) as total_ingresos\n",
        "        FROM {table_name}\n",
        "        GROUP BY Sucursal\n",
        "        ORDER BY num_ventas DESC\n",
        "        \"\"\"\n",
        "\n",
        "    if any(palabra in query_lower for palabra in ['promedio', 'precio promedio', 'categor√≠a']):\n",
        "        return f\"\"\"\n",
        "        SELECT Categor√≠a,\n",
        "               COUNT(*) as num_productos,\n",
        "               AVG(Precio_unitario) as precio_promedio,\n",
        "               MIN(Precio_unitario) as precio_min,\n",
        "               MAX(Precio_unitario) as precio_max\n",
        "        FROM {table_name}\n",
        "        GROUP BY Categor√≠a\n",
        "        ORDER BY precio_promedio DESC\n",
        "        \"\"\"\n",
        "\n",
        "    # Query general\n",
        "    select_cols = ', '.join(cols_relevantes[:3]) if cols_relevantes else '*'\n",
        "    return f\"SELECT {select_cols} FROM {table_name} LIMIT 15\"\n",
        "\n",
        "def execute_query(sql):\n",
        "    \"\"\"Ejecuta consulta SQL y retorna resultados\n",
        "    Diferencia vs RAG normal: ejecutamos c√≥digo contra DB real, no solo texto\"\"\"\n",
        "    global conn\n",
        "\n",
        "    if conn is None:\n",
        "        return \"Error: Base de datos no inicializada\", pd.DataFrame()\n",
        "\n",
        "    try:\n",
        "        sql_clean = ' '.join(line.strip() for line in sql.strip().split('\\n') if line.strip())\n",
        "        print(f\"Ejecutando SQL: {sql_clean}\")\n",
        "        resultado = pd.read_sql_query(sql_clean, conn)\n",
        "        return sql_clean, resultado\n",
        "    except Exception as e:\n",
        "        print(f\"Error SQL: {e}\")\n",
        "        try:\n",
        "            sql_simple = f\"SELECT * FROM {table_name} LIMIT 10\"\n",
        "            resultado = pd.read_sql_query(sql_simple, conn)\n",
        "            return sql_simple, resultado\n",
        "        except Exception as e2:\n",
        "            print(f\"Error en consulta simple: {e2}\")\n",
        "            return \"Error\", pd.DataFrame()\n",
        "\n",
        "def generate_answer(query, sql_executed, data):\n",
        "    \"\"\"Genera respuesta usando Groq API con los datos encontrados\n",
        "    Contexto especial: incluye SQL ejecutado + estad√≠sticas calculadas autom√°ticamente\"\"\"\n",
        "    if data.empty:\n",
        "        return \"No se encontraron datos para esta consulta.\"\n",
        "\n",
        "    # Crear contexto con estad√≠sticas autom√°ticas\n",
        "    context = f\"\"\"Pregunta: {query}\n",
        "SQL ejecutado: {sql_executed}\n",
        "Resultados: {len(data)} filas encontradas\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "    # Agregar estad√≠sticas de columnas num√©ricas\n",
        "    cols_numericas = data.select_dtypes(include=[np.number]).columns\n",
        "    if len(cols_numericas) > 0:\n",
        "        context += \"Estad√≠sticas calculadas:\\n\"\n",
        "        for col in cols_numericas[:3]:\n",
        "            context += f\"- {col}: total={data[col].sum():.2f}, promedio={data[col].mean():.2f}, m√°ximo={data[col].max():.2f}\\n\"\n",
        "        context += \"\\n\"\n",
        "\n",
        "    context += \"Datos encontrados:\\n\"\n",
        "    context += data.to_string(index=False)\n",
        "\n",
        "    # Generar respuesta con Groq\n",
        "    prompt = f\"\"\"Analiza estos datos y responde de forma clara y espec√≠fica:\n",
        "\n",
        "{context}\n",
        "\n",
        "Instrucciones:\n",
        "- Responde bas√°ndote √öNICAMENTE en los datos mostrados\n",
        "- Da n√∫meros exactos y estad√≠sticas precisas\n",
        "- Responde en espa√±ol de forma profesional\n",
        "- Si hay un ranking, muestra los top 3-5 elementos\n",
        "\n",
        "Pregunta: {query}\"\"\"\n",
        "\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"llama3-8b-8192\",\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            max_tokens=400\n",
        "        )\n",
        "        return response.choices[0].message.content\n",
        "    except Exception as e:\n",
        "        return f\"Error generando respuesta: {e}\"\n",
        "\n",
        "def inicializar_rag():\n",
        "    \"\"\"Inicializa el sistema RAG cargando base de datos\"\"\"\n",
        "    print(\"Iniciando sistema RAG...\")\n",
        "\n",
        "    # Crear carpeta si no existe\n",
        "    os.makedirs(folder_path, exist_ok=True)\n",
        "\n",
        "    if load_database(folder_path):\n",
        "        print(\"Sistema RAG listo para consultas.\")\n",
        "        return True\n",
        "    else:\n",
        "        print(\"Error iniciando RAG.\")\n",
        "        return False\n",
        "\n",
        "# Inicializaci√≥n del sistema\n",
        "inicializar_rag()\n",
        "\n",
        "# BLOQUE DE INFERENCIA - Ejecutar por separado\n",
        "query = \"¬øcu√°les son los productos m√°s vendidos?\"  # Cambia tu pregunta aqu√≠\n",
        "sql = generate_sql(query)\n",
        "sql_executed, data = execute_query(sql)\n",
        "respuesta = generate_answer(query, sql_executed, data)\n",
        "print(f\"Pregunta: {query}\")\n",
        "print(f\"SQL: {sql_executed}\")\n",
        "print(f\"Respuesta: {respuesta}\")"
      ],
      "metadata": {
        "id": "MMmRdGQ0NQe0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BLOQUE DE INFERENCIA - Ejecutar por separado\n",
        "query = \"¬øcu√°l es el m√©todo de pago m√°s com√∫n?\"\n",
        "sql = generate_sql(query)\n",
        "sql_executed, data = execute_query(sql)\n",
        "respuesta = generate_answer(query, sql_executed, data)\n",
        "print(f\"Pregunta: {query}\")\n",
        "print(f\"SQL: {sql_executed}\")\n",
        "print(f\"Respuesta: {respuesta}\")"
      ],
      "metadata": {
        "id": "XAY5WtocNwyC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BLOQUE DE INFERENCIA - Ejecutar por separado\n",
        "query = \"¬øcu√°l producto genera m√°s ingresos?\"\n",
        "sql = generate_sql(query)\n",
        "sql_executed, data = execute_query(sql)\n",
        "respuesta = generate_answer(query, sql_executed, data)\n",
        "print(f\"Pregunta: {query}\")\n",
        "print(f\"SQL: {sql_executed}\")\n",
        "print(f\"Respuesta: {respuesta}\")"
      ],
      "metadata": {
        "id": "k9TJqVvzOGKk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}